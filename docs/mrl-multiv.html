<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.3 Modelo de Regresión Lineal Multivariado (RL-Multivariado) | Chapter 10</title>
  <meta name="description" content="Este libro contine ……" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="5.3 Modelo de Regresión Lineal Multivariado (RL-Multivariado) | Chapter 10" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/imagenes/imagen1.png" />
  <meta property="og:description" content="Este libro contine ……" />
  <meta name="github-repo" content="raperezaga/iam" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.3 Modelo de Regresión Lineal Multivariado (RL-Multivariado) | Chapter 10" />
  
  <meta name="twitter:description" content="Este libro contine ……" />
  <meta name="twitter:image" content="/imagenes/imagen1.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rlm.html"/>
<link rel="next" href="ACP.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preámbulo</a>
<ul>
<li class="chapter" data-level="" data-path="descripción.html"><a href="descripción.html"><i class="fa fa-check"></i>Descripción</a></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="" data-path="dedicación.html"><a href="dedicación.html"><i class="fa fa-check"></i>Dedicación</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="paquetes-usados-en-el-libro.html"><a href="paquetes-usados-en-el-libro.html"><i class="fa fa-check"></i>Paquetes usados en el libro</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="rep-al.html"><a href="rep-al.html"><i class="fa fa-check"></i><b>1</b> Repaso de álgebra lineal</a>
<ul>
<li class="chapter" data-level="1.1" data-path="acb-al.html"><a href="acb-al.html"><i class="fa fa-check"></i><b>1.1</b> Algunos conceptos básicos</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="acb-al.html"><a href="acb-al.html#matrices"><i class="fa fa-check"></i><b>1.1.1</b> Matrices</a></li>
<li class="chapter" data-level="1.1.2" data-path="acb-al.html"><a href="acb-al.html#vectores"><i class="fa fa-check"></i><b>1.1.2</b> Vectores</a></li>
<li class="chapter" data-level="1.1.3" data-path="acb-al.html"><a href="acb-al.html#operaciones-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Operaciones Matriciales</a></li>
<li class="chapter" data-level="1.1.4" data-path="acb-al.html"><a href="acb-al.html#matrices-especiales"><i class="fa fa-check"></i><b>1.1.4</b> Matrices Especiales</a></li>
<li class="chapter" data-level="1.1.5" data-path="acb-al.html"><a href="acb-al.html#descomp-espectral"><i class="fa fa-check"></i><b>1.1.5</b> Descomposición Espectral de una Matriz (eigen-descomposición)</a></li>
<li class="chapter" data-level="1.1.6" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-una-matriz"><i class="fa fa-check"></i><b>1.1.6</b> Diagonalización de una Matriz</a></li>
<li class="chapter" data-level="1.1.7" data-path="acb-al.html"><a href="acb-al.html#diagonalización-ortogonal"><i class="fa fa-check"></i><b>1.1.7</b> Diagonalización Ortogonal</a></li>
<li class="chapter" data-level="1.1.8" data-path="acb-al.html"><a href="acb-al.html#diagonalizacion-inversa"><i class="fa fa-check"></i><b>1.1.8</b> Diagonalización de la Inversa de una Matriz</a></li>
<li class="chapter" data-level="1.1.9" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-la-matriz-raíz-cuadrada"><i class="fa fa-check"></i><b>1.1.9</b> Diagonalización de la Matriz Raíz Cuadrada</a></li>
<li class="chapter" data-level="1.1.10" data-path="acb-al.html"><a href="acb-al.html#traza-determinante-y-rango-de-una-matriz"><i class="fa fa-check"></i><b>1.1.10</b> Traza, Determinante y Rango de una Matriz</a></li>
<li class="chapter" data-level="1.1.11" data-path="acb-al.html"><a href="acb-al.html#formas-cuadraticas"><i class="fa fa-check"></i><b>1.1.11</b> Formas Cuadráticas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><a href="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><i class="fa fa-check"></i><b>1.2</b> Algunas Propiedades Estadísticas de la Descomposición de una Matriz en Valores y Vectores Propios</a></li>
<li class="chapter" data-level="1.3" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html"><i class="fa fa-check"></i><b>1.3</b> Diferenciación con Vectores y Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html#vector-gradiente-para-diferentes-definiciones-de-funderlinemathbfx"><i class="fa fa-check"></i><b>1.3.1</b> Vector-Gradiente para diferentes definiciones de <span class="math inline">\(f(\underline{\mathbf{x}})\)</span>:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="org_pres_dat.html"><a href="org_pres_dat.html"><i class="fa fa-check"></i><b>2</b> Organización y Presentación de Datos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html"><i class="fa fa-check"></i><b>2.1</b> Resumenes Descriptivos</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#matriz-de-datos"><i class="fa fa-check"></i><b>2.1.1</b> Matriz de Datos</a></li>
<li class="chapter" data-level="2.1.2" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#estadísticos-descriptivos"><i class="fa fa-check"></i><b>2.1.2</b> Estadísticos descriptivos</a></li>
<li class="chapter" data-level="2.1.3" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#algunas-notaciones"><i class="fa fa-check"></i><b>2.1.3</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="2.1.4" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#representación-gráfica-de-observaciones-multivariadas"><i class="fa fa-check"></i><b>2.1.4</b> Representación Gráfica de Observaciones multivariadas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vectores-y-matrices-aleatorias.html"><a href="vectores-y-matrices-aleatorias.html"><i class="fa fa-check"></i><b>2.2</b> Vectores y Matrices Aleatorias</a></li>
<li class="chapter" data-level="2.3" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html"><i class="fa fa-check"></i><b>2.3</b> Vectores y Matrices Poblacionales Particionados</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-del-vector-de-medias-poblacionales"><i class="fa fa-check"></i><b>2.3.1</b> Particionamiento del Vector de Medias-Poblacionales</a></li>
<li class="chapter" data-level="2.3.2" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-de-la-matriz-de-var-cov-poblacional-mathbfsigma"><i class="fa fa-check"></i><b>2.3.2</b> Particionamiento de la Matriz de Var-Cov Poblacional <span class="math inline">\(\mathbf{\Sigma}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.4</b> Propiedades Sobre la Media y Varianza de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.5" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html"><i class="fa fa-check"></i><b>2.5</b> Vectores y Matrices Muestrales Particionadas</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-del-vector-de-medias-muestrales"><i class="fa fa-check"></i><b>2.5.1</b> Particionamiento del Vector de Medias Muestrales</a></li>
<li class="chapter" data-level="2.5.2" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-de-la-matriz-de-var-cov-muestrales"><i class="fa fa-check"></i><b>2.5.2</b> Particionamiento de la Matriz de Var-Cov Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="algunas-formas-matriciales-eficientes.html"><a href="algunas-formas-matriciales-eficientes.html"><i class="fa fa-check"></i><b>2.6</b> Algunas formas matriciales Eficientes</a></li>
<li class="chapter" data-level="2.7" data-path="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.7</b> Propiedades Sobre la Media y Varianza Muestral de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.8" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><i class="fa fa-check"></i><b>2.8</b> Varianza Generalizada Muestral y su Interpretación Geométrica</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-1-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.1</b> Interpretación Geométrica-1 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.2" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-2-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.2</b> Interpretación Geométrica-2 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.3" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#varianza-generalizada-determinada-por-la-matriz-de-correlación-muestral-mathbfr"><i class="fa fa-check"></i><b>2.8.3</b> Varianza Generalizada Determinada por la Matriz de Correlación Muestral <span class="math inline">\(\mathbf{R}\)</span></a></li>
<li class="chapter" data-level="2.8.4" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#relación-entre-mathbfs-y-mathbfr"><i class="fa fa-check"></i><b>2.8.4</b> Relación entre <span class="math inline">\(|\mathbf{S}|\)</span> y <span class="math inline">\(|\mathbf{R}|\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="varianza-total-muestral.html"><a href="varianza-total-muestral.html"><i class="fa fa-check"></i><b>2.9</b> Varianza Total Muestral</a></li>
<li class="chapter" data-level="2.10" data-path="muestra-aleatoria-de-distribuciones-p-variadas.html"><a href="muestra-aleatoria-de-distribuciones-p-variadas.html"><i class="fa fa-check"></i><b>2.10</b> Muestra Aleatoria de Distribuciones <span class="math inline">\(p\)</span>-Variadas</a></li>
<li class="chapter" data-level="2.11" data-path="distancias.html"><a href="distancias.html"><i class="fa fa-check"></i><b>2.11</b> Distancias</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="distancias.html"><a href="distancias.html#dist_euclidea"><i class="fa fa-check"></i><b>2.11.1</b> Definición de Algunas Distancias</a></li>
<li class="chapter" data-level="2.11.2" data-path="distancias.html"><a href="distancias.html#relación-de-la-distancia-de-mahalanobis-con-la-distribución-chi-cuadrado"><i class="fa fa-check"></i><b>2.11.2</b> Relación de la Distancia de Mahalanobis con la Distribución chi-Cuadrado</a></li>
<li class="chapter" data-level="2.11.3" data-path="distancias.html"><a href="distancias.html#ejemplo"><i class="fa fa-check"></i><b>2.11.3</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Normal-Multiv.html"><a href="Normal-Multiv.html"><i class="fa fa-check"></i><b>3</b> Distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="geometria-NM.html"><a href="geometria-NM.html"><i class="fa fa-check"></i><b>3.1</b> Geometría y propiedades de la NM</a></li>
<li class="chapter" data-level="3.2" data-path="normal-univariada.html"><a href="normal-univariada.html"><i class="fa fa-check"></i><b>3.2</b> Normal Univariada</a></li>
<li class="chapter" data-level="3.3" data-path="normal-multivariada.html"><a href="normal-multivariada.html"><i class="fa fa-check"></i><b>3.3</b> Normal Multivariada</a></li>
<li class="chapter" data-level="3.4" data-path="algunos-aspectos-geométricos-de-la-nm.html"><a href="algunos-aspectos-geométricos-de-la-nm.html"><i class="fa fa-check"></i><b>3.4</b> Algunos Aspectos Geométricos de la NM</a></li>
<li class="chapter" data-level="3.5" data-path="prop-nm.html"><a href="prop-nm.html"><i class="fa fa-check"></i><b>3.5</b> Propiedades de la distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="prop-nm.html"><a href="prop-nm.html#prop1"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Propiedad-1:</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="prop-nm.html"><a href="prop-nm.html#prop2"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Propiedad-2:</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="prop-nm.html"><a href="prop-nm.html#prop3"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Propiedad-3:</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="prop-nm.html"><a href="prop-nm.html#prop4"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Propiedad-4:</strong></a></li>
<li class="chapter" data-level="3.5.5" data-path="prop-nm.html"><a href="prop-nm.html#prop5"><i class="fa fa-check"></i><b>3.5.5</b> <strong>Propiedad-5:</strong></a></li>
<li class="chapter" data-level="3.5.6" data-path="prop-nm.html"><a href="prop-nm.html#prop6"><i class="fa fa-check"></i><b>3.5.6</b> <strong>Propiedad-6:</strong></a></li>
<li class="chapter" data-level="3.5.7" data-path="prop-nm.html"><a href="prop-nm.html#prop7"><i class="fa fa-check"></i><b>3.5.7</b> <strong>Propiedad-7:</strong></a></li>
<li class="chapter" data-level="3.5.8" data-path="prop-nm.html"><a href="prop-nm.html#prop8"><i class="fa fa-check"></i><b>3.5.8</b> <strong>Propiedad-8:</strong></a></li>
<li class="chapter" data-level="3.5.9" data-path="prop-nm.html"><a href="prop-nm.html#prop9"><i class="fa fa-check"></i><b>3.5.9</b> <strong>Propiedad-9:</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.6</b> Evaluación del Supuesto de Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-a-nivel-marginal-ie.-normalidad-univariada"><i class="fa fa-check"></i><b>3.6.1</b> Evaluación a nivel marginal (ie. Normalidad Univariada)</a></li>
<li class="chapter" data-level="3.6.2" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-de-la-normalidad-bi-variada"><i class="fa fa-check"></i><b>3.6.2</b> Evaluación de la Normalidad Bi-variada</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="detección-de-observaciones-atípicas.html"><a href="detección-de-observaciones-atípicas.html"><i class="fa fa-check"></i><b>3.7</b> Detección de Observaciones Atípicas</a></li>
<li class="chapter" data-level="3.8" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.8</b> Transformaciones para Acercar a la Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#familia-de-transformaciones-de-potencia-de-box-y-cox-1964"><i class="fa fa-check"></i><b>3.8.1</b> Familia de Transformaciones de Potencia de Box y Cox (1964)</a></li>
<li class="chapter" data-level="3.8.2" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#transformaciones-para-el-caso-multivariado"><i class="fa fa-check"></i><b>3.8.2</b> Transformaciones para el Caso Multivariado:</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html"><i class="fa fa-check"></i><b>3.9</b> Muestra Aleatoria Normal <span class="math inline">\(p\)</span>-Variada</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#estimadores-de-máx-ver-de-una-normal-multivariada"><i class="fa fa-check"></i><b>3.9.1</b> Estimadores de Máx-Ver de una Normal-Multivariada</a></li>
<li class="chapter" data-level="3.9.2" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#distribución-muestral-de-overlineunderlinemathbfx-y-mathbfs_n"><i class="fa fa-check"></i><b>3.9.2</b> Distribución Muestral de <span class="math inline">\(\overline{\underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\mathbf{S}_n\)</span></a></li>
<li class="chapter" data-level="3.9.3" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#comportamiento-de-underlineoverlinemathbfx_p-y-mathbfs-para-tamaños-muestrales-grandes"><i class="fa fa-check"></i><b>3.9.3</b> Comportamiento de <span class="math inline">\(\underline{\overline{\mathbf{x}}}_p\)</span> y <span class="math inline">\(\mathbf{S}\)</span> para Tamaños Muestrales Grandes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferen-estad.html"><a href="inferen-estad.html"><i class="fa fa-check"></i><b>4</b> Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i><b>4.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2" data-path="inferencia-estadística-para-la-media-mu-caso-univariado.html"><a href="inferencia-estadística-para-la-media-mu-caso-univariado.html"><i class="fa fa-check"></i><b>4.2</b> Inferencia Estadística para la Media (<span class="math inline">\(\mu\)</span>)-caso univariado</a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><i class="fa fa-check"></i><b>4.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-desconocida-normal"><i class="fa fa-check"></i><b>4.3.1</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Desconocida (Normal)</a></li>
<li class="chapter" data-level="4.3.2" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-conocida-población-normal"><i class="fa fa-check"></i><b>4.3.2</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Conocida (Población: Normal)</a></li>
<li class="chapter" data-level="4.3.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-en-muestra-grande"><i class="fa fa-check"></i><b>4.3.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> en Muestra Grande</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><i class="fa fa-check"></i><b>4.4</b> PH Acerca de Contrastes del Vector de Medias Poblacional <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span>, de una <span class="math inline">\(N_p(\underline{\boldsymbol{\mu}} \ , \ \mathbf{\Sigma} )\)</span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-para-el-caso-de-pnm"><i class="fa fa-check"></i><b>4.4.1</b> PH de Contrastes para el Caso de PNM</a></li>
<li class="chapter" data-level="4.4.2" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-en-en-caso-de-n-grande"><i class="fa fa-check"></i><b>4.4.2</b> PH de Contrastes en en caso de <span class="math inline">\(n\)</span>-Grande</a></li>
<li class="chapter" data-level="4.4.3" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#prueba-de-hipótesis-para-igualdad-de-medias"><i class="fa fa-check"></i><b>4.4.3</b> Prueba de hipótesis para igualdad de medias</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><i class="fa fa-check"></i><b>4.5</b> PH para Igualdad de Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}=\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida"><i class="fa fa-check"></i><b>4.5.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.5.2" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida"><i class="fa fa-check"></i><b>4.5.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.5.3" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-3.-mathbfsigma_-underlinemathbfxneq-mathbfsigma_-underlinemathbfy-desconocida"><i class="fa fa-check"></i><b>4.5.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}\neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocida</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><i class="fa fa-check"></i><b>4.6</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,   para muestras grandes</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida-1"><i class="fa fa-check"></i><b>4.6.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.6.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida-1"><i class="fa fa-check"></i><b>4.6.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.6.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-3.-mathbfsigma_-underlinemathbfx-neq-mathbfsigma_-underlinemathbfy-desconocidas"><i class="fa fa-check"></i><b>4.6.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}} \neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocidas</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><i class="fa fa-check"></i><b>4.7</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,      Observaciones Pareadas</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-univariado"><i class="fa fa-check"></i><b>4.7.1</b> Caso Univariado</a></li>
<li class="chapter" data-level="4.7.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-multivariado"><i class="fa fa-check"></i><b>4.7.2</b> Caso Multivariado</a></li>
<li class="chapter" data-level="4.7.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#uso-de-contrastes-para-la-comparación-de-medias-en-muestras-pareadas"><i class="fa fa-check"></i><b>4.7.3</b> Uso de Contrastes para la Comparación de Medias en Muestras Pareadas</a></li>
<li class="chapter" data-level="4.7.4" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#un-diseño-de-medidas-repetidas-para-comparar-tratamientos"><i class="fa fa-check"></i><b>4.7.4</b> Un Diseño de Medidas Repetidas para Comparar Tratamientos</a></li>
<li class="chapter" data-level="4.7.5" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#análisis-de-perfiles"><i class="fa fa-check"></i><b>4.7.5</b> Análisis de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html"><i class="fa fa-check"></i><b>4.8</b> Inferencia para la Matriz de Varianzas Covarianza</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueva-de-rv-sigma"><i class="fa fa-check"></i><b>4.8.1</b> Pruba de Razón de Verosimilitud</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueba-de-bartlet"><i class="fa fa-check"></i><b>4.8.2</b> Prueba de Bartlet</a></li>
<li class="chapter" data-level="4.8.3" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#dos-o-más-matrices-de-varianzas-covarianzas"><i class="fa fa-check"></i><b>4.8.3</b> Dos o más Matrices de Varianzas Covarianzas</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><i class="fa fa-check"></i><b>4.9</b> Regiones de Confianza y Comparaciones Simultáneas entre las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#construcción-de-una-región-de-confianza-para-underlineboldsymbol-mu-cuando-la-población-tiene-distribución-n_punderlineboldsymbol-mumathbfsigma-con-underlineboldsymbol-mu-y-mathbfsigma-desconocidos"><i class="fa fa-check"></i><b>4.9.1</b> Construcción de una Región de Confianza para <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> Cuando la Población tiene Distribución <span class="math inline">\(N_p(\underline{\boldsymbol \mu},\mathbf{\Sigma})\)</span>, con <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span> desconocidos</a></li>
<li class="chapter" data-level="4.9.2" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#región-de-confianza-para-el-caso-de-p2-elipse-de-confianza"><i class="fa fa-check"></i><b>4.9.2</b> Región de Confianza para el Caso de <span class="math inline">\(p=2\)</span> (Elipse de Confianza)</a></li>
<li class="chapter" data-level="4.9.3" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-para-las-componentes-del-vector-de-medias-underlineboldsymbol-mu"><i class="fa fa-check"></i><b>4.9.3</b> Intervalos de Confianza Simultáneos para las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a></li>
<li class="chapter" data-level="4.9.4" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-t2-simultáneos-para-diferencias-de-medias"><i class="fa fa-check"></i><b>4.9.4</b> IC <span class="math inline">\(T^2\)</span> Simultáneos para Diferencias de Medias</a></li>
<li class="chapter" data-level="4.9.5" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#IC-Bonferroni"><i class="fa fa-check"></i><b>4.9.5</b> Método de Bonferroni para Comparaciones Múltiples</a></li>
<li class="chapter" data-level="4.9.6" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-chi2-caso-n-grande"><i class="fa fa-check"></i><b>4.9.6</b> Intervalos de Confianza Simultáneos <span class="math inline">\(\chi^2\)</span> (caso n-Grande)</a></li>
<li class="chapter" data-level="4.9.7" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-simultáneos-para-n-grande-usando-la-normal-estándar-z_alpha"><i class="fa fa-check"></i><b>4.9.7</b> IC Simultáneos para n-Grande Usando la Normal Estándar <span class="math inline">\(Z_\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mrlm.html"><a href="mrlm.html"><i class="fa fa-check"></i><b>5</b> Modelos de Regresión Lineal Multivariados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introducción-2.html"><a href="introducción-2.html"><i class="fa fa-check"></i><b>5.1</b> Introducción</a></li>
<li class="chapter" data-level="5.2" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>5.2</b> Modelo de Regresión Lineal Múltiple (RLM)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="rlm.html"><a href="rlm.html#estimación-de-mínimos-cuadrados"><i class="fa fa-check"></i><b>5.2.1</b> Estimación de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="5.2.2" data-path="rlm.html"><a href="rlm.html#inferencias-acerca-del-modelo-de-regresión-lineal-múltiple"><i class="fa fa-check"></i><b>5.2.2</b> Inferencias Acerca del Modelo de Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="5.2.3" data-path="rlm.html"><a href="rlm.html#inferencias-a-partir-de-la-función-de-regresión-estimada"><i class="fa fa-check"></i><b>5.2.3</b> Inferencias A partir de la Función de Regresión Estimada</a></li>
<li class="chapter" data-level="5.2.4" data-path="rlm.html"><a href="rlm.html#validación-de-los-supuestos-del-modelo-y-otros-aspectos-del-la-regresión"><i class="fa fa-check"></i><b>5.2.4</b> Validación de los Supuestos del Modelo y Otros Aspectos del la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html"><i class="fa fa-check"></i><b>5.3</b> Modelo de Regresión Lineal Multivariado (RL-Multivariado)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mrl-multiv.html"><a href="mrl-multiv.html#algunas-notaciones-1"><i class="fa fa-check"></i><b>5.3.1</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="5.3.2" data-path="mrl-multiv.html"><a href="mrl-multiv.html#mrl-multivariado-en-forma-matricial"><i class="fa fa-check"></i><b>5.3.2</b> MRL-Multivariado en forma Matricial</a></li>
<li class="chapter" data-level="5.3.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html#m-mrl-múltiples"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(m\)</span>-MRL-Múltiples</a></li>
<li class="chapter" data-level="5.3.4" data-path="mrl-multiv.html"><a href="mrl-multiv.html#estimador-de-mínimos-cuadrados-de-los-parámetros"><i class="fa fa-check"></i><b>5.3.4</b> Estimador de Mínimos Cuadrados de los Parámetros</a></li>
<li class="chapter" data-level="5.3.5" data-path="mrl-multiv.html"><a href="mrl-multiv.html#propiedades-de-estimadores-de-mínimos-cuadrados-del-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.5</b> Propiedades de Estimadores de Mínimos Cuadrados del MRL-Multivariado</a></li>
<li class="chapter" data-level="5.3.6" data-path="mrl-multiv.html"><a href="mrl-multiv.html#prv-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.6</b> Prueba de Razón de Verosimilitud para Parámetros de Regresión en MRL-Multivariados</a></li>
<li class="chapter" data-level="5.3.7" data-path="mrl-multiv.html"><a href="mrl-multiv.html#otras-pruebas-estadísticas-multivariadas"><i class="fa fa-check"></i><b>5.3.7</b> Otras Pruebas Estadísticas Multivariadas</a></li>
<li class="chapter" data-level="5.3.8" data-path="mrl-multiv.html"><a href="mrl-multiv.html#predicciones-a-partir-de-un-modelo-de-regresión-múltiple-multivariado"><i class="fa fa-check"></i><b>5.3.8</b> Predicciones A Partir de un Modelo de Regresión Múltiple Multivariado</a></li>
<li class="chapter" data-level="5.3.9" data-path="mrl-multiv.html"><a href="mrl-multiv.html#el-concepto-de-regresión-lineal"><i class="fa fa-check"></i><b>5.3.9</b> El Concepto de Regresión Lineal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ACP.html"><a href="ACP.html"><i class="fa fa-check"></i><b>6</b> Análisis de Componentes Principales (ACP)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducción-3.html"><a href="introducción-3.html"><i class="fa fa-check"></i><b>6.1</b> Introducción</a></li>
<li class="chapter" data-level="6.2" data-path="interpretaciones-geométricas-y-algebraícas-del-acp.html"><a href="interpretaciones-geométricas-y-algebraícas-del-acp.html"><i class="fa fa-check"></i><b>6.2</b> Interpretaciones Geométricas y Algebraícas del ACP</a></li>
<li class="chapter" data-level="6.3" data-path="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><a href="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><i class="fa fa-check"></i><b>6.3</b> Interpretación Geométrica del ACP Mediante un Ejemplo Simple <span class="math inline">\(p=2\)</span></a></li>
<li class="chapter" data-level="6.4" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html"><i class="fa fa-check"></i><b>6.4</b> Mas de dos Ejes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#min-cuadrados"><i class="fa fa-check"></i><b>6.4.1</b> Ajuste de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="6.4.2" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#relación-entre-los-sub-espacios-de-mathbbrp-y-de-mathbbrn"><i class="fa fa-check"></i><b>6.4.2</b> Relación entre los Sub-espacios de <span class="math inline">\(\mathbb{R}^p\)</span> y de <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="componentes-principales-en-análisis-de-regresión.html"><a href="componentes-principales-en-análisis-de-regresión.html"><i class="fa fa-check"></i><b>6.5</b> Componentes Principales en Análisis de Regresión</a></li>
<li class="chapter" data-level="6.6" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html"><i class="fa fa-check"></i><b>6.6</b> Componentes Principales Poblacionales</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#definicón-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.1</b> Definicón de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.2" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#determinación-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.2</b> Determinación de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.3" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-derivadas-de-una-normal-multivariada"><i class="fa fa-check"></i><b>6.6.3</b> Componentes Principales Derivadas de una Normal Multivariada</a></li>
<li class="chapter" data-level="6.6.4" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-usando-variables-estandarizadas"><i class="fa fa-check"></i><b>6.6.4</b> Componentes Principales usando Variables Estandarizadas</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><i class="fa fa-check"></i><b>6.7</b> Componentes Principales para Matrices de Var-Cov <span class="math inline">\(\mathbf{\Sigma}\)</span> con Estructuras Especiales</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#variables-no-correlacionadas"><i class="fa fa-check"></i><b>6.7.1</b> Variables No-Correlacionadas</a></li>
<li class="chapter" data-level="6.7.2" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#var-correlacionadas"><i class="fa fa-check"></i><b>6.7.2</b> Variables Altamente Correlacionadas</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="componentes-principales-muestrales.html"><a href="componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.8</b> Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.9" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html"><i class="fa fa-check"></i><b>6.9</b> Algunos Ejemplos de ACP</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo1"><i class="fa fa-check"></i><b>6.9.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.9.2" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-2"><i class="fa fa-check"></i><b>6.9.2</b> Ejemplo-2</a></li>
<li class="chapter" data-level="6.9.3" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-3"><i class="fa fa-check"></i><b>6.9.3</b> Ejemplo-3</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="cómo-elegir-el-número-de-componentes-principales.html"><a href="cómo-elegir-el-número-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.10</b> Cómo elegir el número de Componentes Principales?</a></li>
<li class="chapter" data-level="6.11" data-path="interpretación-de-las-componentes-principales-muestrales.html"><a href="interpretación-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.11</b> Interpretación de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.12" data-path="estandarización-de-las-componentes-principales-muestrales.html"><a href="estandarización-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.12</b> Estandarización de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.13" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.13</b> Gráficas en un Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="6.13.1" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#graficos-de-las-cps"><i class="fa fa-check"></i><b>6.13.1</b> Graficos de las CPS</a></li>
<li class="chapter" data-level="6.13.2" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#el-gráfico-biplot"><i class="fa fa-check"></i><b>6.13.2</b> El gráfico biplot:</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><a href="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><i class="fa fa-check"></i><b>6.14</b> Propiedades de <span class="math inline">\(\hat{\lambda}_i\)</span> y <span class="math inline">\(\underline{\hat{\mathbf{e}}}_i\)</span> en Muestras Grandes</a></li>
<li class="chapter" data-level="6.15" data-path="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><a href="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><i class="fa fa-check"></i><b>6.15</b> Prueba de Hipótesis para Estructura de Correlación Igual</a></li>
<li class="chapter" data-level="6.16" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html"><i class="fa fa-check"></i><b>6.16</b> Ejemplos Finales</a>
<ul>
<li class="chapter" data-level="6.16.1" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-1"><i class="fa fa-check"></i><b>6.16.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.16.2" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-acp-con-individuos-y-variables-suplementarias"><i class="fa fa-check"></i><b>6.16.2</b> Ejemplo ACP con Individuos y Variables Suplementarias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="affc.html"><a href="affc.html"><i class="fa fa-check"></i><b>7</b> Análisis Factorial de Factores Comunes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introducción-4.html"><a href="introducción-4.html"><i class="fa fa-check"></i><b>7.1</b> Introducción</a></li>
<li class="chapter" data-level="7.2" data-path="tipos-de-factores.html"><a href="tipos-de-factores.html"><i class="fa fa-check"></i><b>7.2</b> Tipos de Factores</a></li>
<li class="chapter" data-level="7.3" data-path="motivación-del-análisis-factorial.html"><a href="motivación-del-análisis-factorial.html"><i class="fa fa-check"></i><b>7.3</b> Motivación del Análisis Factorial</a></li>
<li class="chapter" data-level="7.4" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html"><i class="fa fa-check"></i><b>7.4</b> Enfoques del AF</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#métodos-para-realizar-la-extracción-de-los-factores"><i class="fa fa-check"></i><b>7.4.1</b> Métodos para realizar la extracción de los factores</a></li>
<li class="chapter" data-level="7.4.2" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#rotación-de-ejes"><i class="fa fa-check"></i><b>7.4.2</b> Rotación de Ejes</a></li>
<li class="chapter" data-level="7.4.3" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#puntuaciones-o-scores-de-los-sujetos-en-los-factores-extraídos"><i class="fa fa-check"></i><b>7.4.3</b> Puntuaciones o Scores de los Sujetos en los Factores Extraídos</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html"><i class="fa fa-check"></i><b>7.5</b> El Modelo de Factor Ortogonal</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#estructura-de-covarianzas-del-modelo-de-factor-ortogonal"><i class="fa fa-check"></i><b>7.5.1</b> Estructura de Covarianzas del Modelo de Factor Ortogonal</a></li>
<li class="chapter" data-level="7.5.2" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#ejemplos"><i class="fa fa-check"></i><b>7.5.2</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html"><i class="fa fa-check"></i><b>7.6</b> Métodos de Estimación</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-cp"><i class="fa fa-check"></i><b>7.6.1</b> El Método de la Componente Principal</a></li>
<li class="chapter" data-level="7.6.2" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-pa"><i class="fa fa-check"></i><b>7.6.2</b> Una Aproximación Modificada (La Solución del Factor Principal o de las CP-Iteradas o de Ejes Principales-PA)</a></li>
<li class="chapter" data-level="7.6.3" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-mle"><i class="fa fa-check"></i><b>7.6.3</b> El Método de Máxima Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="prueba-para-el-número-de-factores-muestra-grande..html"><a href="prueba-para-el-número-de-factores-muestra-grande..html"><i class="fa fa-check"></i><b>7.7</b> Prueba para el Número de Factores (Muestra Grande).</a></li>
<li class="chapter" data-level="7.8" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html"><i class="fa fa-check"></i><b>7.8</b> Rotación de Factores</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-cuando-m2-factores"><i class="fa fa-check"></i><b>7.8.1</b> Rotaciones cuando <span class="math inline">\(m=2\)</span>-Factores</a></li>
<li class="chapter" data-level="7.8.2" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#criterio-varimáx"><i class="fa fa-check"></i><b>7.8.2</b> Criterio Varimáx:</a></li>
<li class="chapter" data-level="7.8.3" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>7.8.3</b> Rotaciones Oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html"><i class="fa fa-check"></i><b>7.9</b> Factores-Scores (o Puntuaciones) de los Factores</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#método-de-los-mínimos-cuadrados-ponderados"><i class="fa fa-check"></i><b>7.9.1</b> Método de los Mínimos Cuadrados Ponderados</a></li>
<li class="chapter" data-level="7.9.2" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#el-método-de-la-regresión"><i class="fa fa-check"></i><b>7.9.2</b> El Método de la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><a href="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><i class="fa fa-check"></i><b>7.10</b> Perspectivas y Estrategías para el Análisis de Factor</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="avm.html"><a href="avm.html"><i class="fa fa-check"></i><b>8</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introducción-5.html"><a href="introducción-5.html"><i class="fa fa-check"></i><b>8.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introducción-5.html"><a href="introducción-5.html#suposiciones-del-manova"><i class="fa fa-check"></i><b>8.1.1</b> Suposiciones del MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>8.2</b> Análisis de Varianza Univariado (ANOVA)</a></li>
<li class="chapter" data-level="8.3" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>8.3</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="manova.html"><a href="manova.html#modelo-manova-para-comparar-g-vectores-de-medias-poblacionales"><i class="fa fa-check"></i><b>8.3.1</b> Modelo MANOVA para comparar <span class="math inline">\(g\)</span>-Vectores de Medias Poblacionales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adiscriminante.html"><a href="adiscriminante.html"><i class="fa fa-check"></i><b>9</b> Análisis Discriminante y Clasificación</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introducción-6.html"><a href="introducción-6.html"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.2</b> Separación y Clasificación para el caso de dos poblaciones</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html#clasificación-de-dos-poblaciones"><i class="fa fa-check"></i><b>9.2.1</b> Clasificación de dos Poblaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.3</b> Regla de Discriminación para dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#costo-esperado-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.1</b> Costo Esperado de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.2" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#probabilidad-total-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.2</b> Probabilidad Total de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#regla-de-clasificación-de-bayes"><i class="fa fa-check"></i><b>9.3.3</b> Regla de Clasificación de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html"><i class="fa fa-check"></i><b>9.4</b> Clasificación con Dos Poblaciones Normales Multivariadas</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1mathbfsigma_2mathbfsigma"><i class="fa fa-check"></i><b>9.4.1</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\mathbf{\Sigma}\)</span></a></li>
<li class="chapter" data-level="9.4.2" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1neq-mathbfsigma_2"><i class="fa fa-check"></i><b>9.4.2</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1\neq \mathbf{\Sigma}_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html"><i class="fa fa-check"></i><b>9.5</b> Evaluación de las Funciones de Clasificación</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-óptimo-teo"><i class="fa fa-check"></i><b>9.5.1</b> Tasa de Error Óptimo (TEO)</a></li>
<li class="chapter" data-level="9.5.2" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual"><i class="fa fa-check"></i><b>9.5.2</b> Tasa de Error Actual</a></li>
<li class="chapter" data-level="9.5.3" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-aparente-teap"><i class="fa fa-check"></i><b>9.5.3</b> Tasa de Error Aparente (TEAP)</a></li>
<li class="chapter" data-level="9.5.4" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual-esperada"><i class="fa fa-check"></i><b>9.5.4</b> Tasa de Error Actual Esperada</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.6</b> Clasificación con Varias Poblaciones (Es decir Más de dos Poblaciones)</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html#costo-esperado-de-mal-clasificación-ecm"><i class="fa fa-check"></i><b>9.6.1</b> Costo Esperado de Mal Clasificación (ECM)</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.7</b> Clasificación con Poblaciones Normales y más de dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#un-clasificador-equivalente-para-el-caso-de-matrices-de-var-cov-iguales"><i class="fa fa-check"></i><b>9.7.1</b> Un Clasificador Equivalente para el Caso de Matrices de Var-Cov Iguales</a></li>
<li class="chapter" data-level="9.7.2" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#comparación-dos-a-dos-de-los-scores-de-la-función-de-clasificación-lineal"><i class="fa fa-check"></i><b>9.7.2</b> Comparación Dos a Dos de los Scores de la Función de Clasificación Lineal</a></li>
<li class="chapter" data-level="9.7.3" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#tasa-de-error-actual-esperada-aer-estimada"><i class="fa fa-check"></i><b>9.7.3</b> Tasa de Error Actual Esperada (AER) Estimada</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><i class="fa fa-check"></i><b>9.8</b> Método de Fisher Para Discriminar Entre Varias Poblaciones</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#uso-de-la-función-de-discriminación-de-fisher-para-clasificación"><i class="fa fa-check"></i><b>9.8.1</b> Uso de la Función de Discriminación de Fisher Para Clasificación</a></li>
<li class="chapter" data-level="9.8.2" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#relación-entre-la-regla-de-clasificación-de-fisher-y-las-funciones-de-discriminación-de-la-teoría-normal"><i class="fa fa-check"></i><b>9.8.2</b> Relación entre la Regla de Clasificación de Fisher y las Funciones de Discriminación de la Teoría Normal</a></li>
<li class="chapter" data-level="9.8.3" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#regla-de-clasificación-de-fisher-basado-en-discriminantes-muestrales"><i class="fa fa-check"></i><b>9.8.3</b> Regla de Clasificación de Fisher Basado en Discriminantes Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html"><i class="fa fa-check"></i><b>9.9</b> Regresión Logística y Clasificación</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#el-modelo-logit"><i class="fa fa-check"></i><b>9.9.1</b> El Modelo Logit</a></li>
<li class="chapter" data-level="9.9.2" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#análisis-de-regresión-logística"><i class="fa fa-check"></i><b>9.9.2</b> Análisis de Regresión Logística</a></li>
<li class="chapter" data-level="9.9.3" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#regresión-logística-con-respuestas-binomiales"><i class="fa fa-check"></i><b>9.9.3</b> Regresión Logística con Respuestas Binomiales</a></li>
<li class="chapter" data-level="9.9.4" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#chequeo-del-modelo"><i class="fa fa-check"></i><b>9.9.4</b> Chequeo del Modelo</a></li>
<li class="chapter" data-level="9.9.5" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#prueba-de-residuales-y-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>9.9.5</b> Prueba de Residuales y de Bondad de Ajuste</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="acluster.html"><a href="acluster.html"><i class="fa fa-check"></i><b>10</b> Análisis de Agrupamiento o Cluster</a>
<ul>
<li class="chapter" data-level="10.1" data-path="introducción-7.html"><a href="introducción-7.html"><i class="fa fa-check"></i><b>10.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2" data-path="consideraciones-iniciales.html"><a href="consideraciones-iniciales.html"><i class="fa fa-check"></i><b>10.2</b> Consideraciones Iniciales</a></li>
<li class="chapter" data-level="10.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html"><i class="fa fa-check"></i><b>10.3</b> Medidas de Similaridad entre Pares de Observaciones</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#medidas-de-distancia"><i class="fa fa-check"></i><b>10.3.1</b> Medidas de Distancia</a></li>
<li class="chapter" data-level="10.3.2" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-de-correlación-entre-casos"><i class="fa fa-check"></i><b>10.3.2</b> Coeficientes de Correlación (entre casos)</a></li>
<li class="chapter" data-level="10.3.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-binarios-de-asociación-o-similaridad-entre-observaciones"><i class="fa fa-check"></i><b>10.3.3</b> Coeficientes Binarios de Asociación o Similaridad Entre Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><a href="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><i class="fa fa-check"></i><b>10.4</b> Medidas de Similaridad o Asociación para Pares de Variables</a></li>
<li class="chapter" data-level="10.5" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html"><i class="fa fa-check"></i><b>10.5</b> Métodos Jerárquicos de Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-enlaces-para-agupamientos-aglomerativos"><i class="fa fa-check"></i><b>10.5.1</b> Métodos Jerarquicos de Enlaces para Agupamientos Aglomerativos</a></li>
<li class="chapter" data-level="10.5.2" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-agrupamientos-desaglomerativos"><i class="fa fa-check"></i><b>10.5.2</b> Métodos Jerarquicos de Agrupamientos Desaglomerativos</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><i class="fa fa-check"></i><b>10.6</b> Métodos NO-Jerárquicos de Partición o Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#pasos-o-etapas-de-un-método-de-clasificación-no-jararquico"><i class="fa fa-check"></i><b>10.6.1</b> Pasos o etapas de un Método de Clasificación No-Jararquico</a></li>
<li class="chapter" data-level="10.6.2" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#método-de-las-k-medias-o-k-means"><i class="fa fa-check"></i><b>10.6.2</b> Método de las <span class="math inline">\(k\)</span>-Medias o <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="cómo-determinar-el-número-apropiado-de-conglomerados.html"><a href="cómo-determinar-el-número-apropiado-de-conglomerados.html"><i class="fa fa-check"></i><b>10.7</b> Cómo determinar el número apropiado de conglomerados?</a></li>
<li class="chapter" data-level="10.8" data-path="agrupamientos-basados-en-modelos-estadísticos.html"><a href="agrupamientos-basados-en-modelos-estadísticos.html"><i class="fa fa-check"></i><b>10.8</b> Agrupamientos Basados en Modelos Estadísticos</a></li>
<li class="chapter" data-level="10.9" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html"><i class="fa fa-check"></i><b>10.9</b> Escalamiento Multidimensional</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html#el-algoritmo-básico"><i class="fa fa-check"></i><b>10.9.1</b> El Algoritmo Básico</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html"><i class="fa fa-check"></i><b>10.10</b> Análisis de Correspondencia</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#desarrollo-algebraíco-del-análsisi-de-correspondencia"><i class="fa fa-check"></i><b>10.10.1</b> Desarrollo Algebraíco del Análsisi de Correspondencia</a></li>
<li class="chapter" data-level="10.10.2" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-como-un-problema-de-mínimos-cuadrados-ponderado"><i class="fa fa-check"></i><b>10.10.2</b> Análisis de Correspondencia como un Problema de Mínimos Cuadrados Ponderado</a></li>
<li class="chapter" data-level="10.10.3" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-medinate-el-método-de-aproximación-de-perfiles"><i class="fa fa-check"></i><b>10.10.3</b> Análisis de Correspondencia Medinate el Método de Aproximación de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><i class="fa fa-check"></i><b>10.11</b> Biplots para la Visualización de Unidades Muestrales y Variables</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html#construcción-de-un-biplot"><i class="fa fa-check"></i><b>10.11.1</b> Construcción de un Biplot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Chapter 10</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mrl-multiv" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Modelo de Regresión Lineal Multivariado (RL-Multivariado)<a href="mrl-multiv.html#mrl-multiv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ahora se considera el problema de modelar la relación entre <span class="math inline">\(m\)</span>-variables respuestas <span class="math inline">\(Y_1,Y_2,\ldots,Y_m\)</span> y un solo conjunto de variables regresoras o predictoras <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span>. Cada una de las variables respuestas se asume que siguen su propio Modelo de Regresión Lineal Múltiple, dado por:
<span class="math display" id="eq:mrl-multivariado1">\[
\begin{equation}
\underset{\text{Respuesta-k}}{Y_k} =\underset{\text{Media que depende de las}\  Z´s}{ \underbrace{ \beta_{0k} + \beta_{1k}\ Z_1 + \beta_{2k}\ Z_2 + \cdots + \beta_{rk}\ Z_r} } +  \underset{Error}{\varepsilon_k } \ \ , \ \ k=1,2,\ldots,m
\end{equation}
\tag{5.38}
\]</span></p>
<p>es decir, se tienen <span class="math inline">\(m\)</span>-MRL-Múltiples dados por:
<span class="math display">\[
Y_1 = \beta_{01} + \beta_{11}\ Z_1 + \beta_{21}\ Z_2 + \cdots + \beta_{r1}\ Z_r  +  \varepsilon_1 \\
Y_2 = \beta_{02} + \beta_{12}\ Z_1 + \beta_{22}\ Z_2 + \cdots + \beta_{r2}\ Z_r  +  \varepsilon_2 \\
Y_3 = \beta_{03} + \beta_{13}\ Z_1 + \beta_{23}\ Z_2 + \cdots + \beta_{r3}\ Z_r  +  \varepsilon_1 \\
\vdots \\
Y_m = \beta_{0m} + \beta_{1m}\ Z_1 + \beta_{2m}\ Z_2 + \cdots + \beta_{rm}\ Z_r  +  \varepsilon_m
\]</span></p>
<p>En este caso se tiene que el término de error:
<span class="math display">\[
\underline{\boldsymbol \varepsilon}=\begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_m \end{bmatrix}, \ \ \ \text{tiene:} \ \ E[\ \underline{\boldsymbol \varepsilon}\ ]= \underline{\mathbf{0}}_{\ m} \ \ \ , \ \ \ Var[\ \underline{\boldsymbol \varepsilon}\ ]=\mathbf{\Sigma}_{m\times m},
\]</span></p>
<p>es decir que, los términos de errores asociados con variables respuestas diferentes pueden estar correlacionados.</p>
<div id="algunas-notaciones-1" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Algunas Notaciones<a href="mrl-multiv.html#algunas-notaciones-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para establecer la notación conforme al modelo de regresión lineal múltiple clásico, sea,
<span class="math display">\[
\underline{\mathbf{z}}_{\ (j)}=\begin{bmatrix} z_{j0} &amp; z_{j1} &amp; \cdots &amp; z_{jr} \end{bmatrix}\ \ \ , \ \ \ j=1,2,\ldots,n
\]</span></p>
<p>el vector fila que denota los valores de las <span class="math inline">\(r\)</span>-variables predictoras para la <span class="math inline">\(j\)</span>-ésima observación o individuo o ensayo, con <span class="math inline">\(j=1,2,\ldots,n\)</span>, y sea
<span class="math display">\[
\underline{\mathbf y}_{\ (j)}^{\ t}=\begin{bmatrix} y_{j1} &amp; y_{j2} &amp; \cdots &amp; y_{jm} \end{bmatrix}_{1\times m}\ \ \ , \ \ \ j=1,2,\ldots,n
\]</span></p>
<p>el vector fila que denota los valores de las <span class="math inline">\(m\)</span>-variables respuestas sobre el <span class="math inline">\(j\)</span>-ésimo individuos u observación, y sea
<span class="math display">\[
\underline{\boldsymbol \varepsilon}_{\ (j)}^{\ t}=\begin{bmatrix} \varepsilon_{j1} &amp;  \varepsilon_{j2} &amp; \cdots &amp; \varepsilon_{jm} \end{bmatrix}_{1\times m}\ \ \ , \ \ \ j=1,2,\ldots,n
\]</span></p>
<p>el vector fila de errores de las <span class="math inline">\(m\)</span>-mediciones realizadas o tomadas sobre el <span class="math inline">\(j\)</span>-ésimo individuo.</p>
<p><strong>En Notación Matricial se tiene la Matriz Diseño</strong> dada por:
<span class="math display">\[
\underset{n\times (r+1)}{\mathbf{Z}}= \begin{bmatrix} z_{10} &amp; z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1r} \\
z_{20} &amp; z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2r} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
z_{n0} &amp; z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{nr}
\end{bmatrix}=\begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n\times 1}{ \underline{\mathbf{z}}_{\ (0)} } &amp;  
\underset{n \times 1}{\underline{\mathbf{z}}_{\ (1)} } &amp; \cdots &amp; \underset{n\times 1}{\underline{\mathbf{z}}_{\ (r)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix} \ \ \ \ ; \ \ \ \ \ \ \
\underset{n \times 1}{ \underline{\mathbf{z}}_{\ (l)} }=\begin{bmatrix} z_{1l} \\ z_{2l} \\ y_{zl}\\ \vdots \\ z_{nl} \end{bmatrix}, \ \\ \ l=0,1,2,\ldots,r \\
= \begin{bmatrix} \longleftarrow \underset{1\times (r+1)}{ \underline{\mathbf{z}}^t_{\ (1)} } \longrightarrow  \\   \longleftarrow \underset{1\times (r+1)}{ \underline{\mathbf{z}}^t_{\ (2)} } \longrightarrow \\ \vdots  \\  \longleftarrow \underset{1\times (r+1)}{ \underline{\mathbf{z}}^t_{\ (n)} } \longrightarrow \end{bmatrix}_{n \times (r+1)}, \ \ \ \ \ \ \underset{(r+1)\times 1}{ \underline{\mathbf{z}}_{\ (j)} }=\begin{bmatrix} z_{j0} \\ z_{j1} \\ z_{j2}\\ \vdots \\ z_{jr} \end{bmatrix}, \ \ \ j=1,2,\ldots,n
\]</span></p>
<p>que corresponde a la misma matriz diseño del MRL múltiple.</p>
<p><strong>Las otras cantidades matriciales involucradas son</strong>:</p>
<p><em>La Matriz de datos para las <span class="math inline">\(m\)</span>-variables respuestas:</em>
<span class="math display">\[
\underset{n\times m}{\mathbf{Y}}= \begin{bmatrix} Y_{11} &amp; Y_{12} &amp; Y_{13} &amp; \cdots &amp; Y_{1m} \\
Y_{21} &amp; Y_{22} &amp; Y_{23} &amp; \cdots &amp; Y_{2m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Y_{n1} &amp; Y_{n2} &amp; Y_{n3} &amp; \cdots &amp; Y_{nm}
\end{bmatrix} = \begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n\times 1}{ \underline{\mathbf{y}}_{\ (1)} } &amp;  
\underset{n \times 1}{\underline{\mathbf{y}}_{\ (2)} } &amp; \cdots &amp; \underset{n\times 1}{\underline{\mathbf{y}}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix} \ \ \ \ ; \ \ \ \ \ \ \
\underset{n \times 1}{ \underline{\mathbf{y}}_{\ (k)} }=\begin{bmatrix} y_{1k} \\ y_{2k} \\ y_{3k}\\ \vdots \\ y_{nk} \end{bmatrix}, \ \\ \ k=1,2,\ldots,m \\
\begin{bmatrix} \longleftarrow \underset{1\times m}{ \underline{\mathbf{y}}^t_{\ (1)} } \longrightarrow  \\   \longleftarrow \underset{1\times m}{ \underline{\mathbf{y}}^t_{\ (2)} } \longrightarrow \\ \vdots  \\  \longleftarrow \underset{1\times m}{ \underline{\mathbf{y}}^t_{\ (n)} } \longrightarrow \end{bmatrix}_{(r+1) \times m}, \ \ \ \ \ \ \underset{m\times 1}{ \underline{\mathbf{y}}_{\ (j)} }=\begin{bmatrix} y_{j1} \\ y_{j2} \\ y_{j3}\\ \vdots \\ y_{jm} \end{bmatrix}, \ \ \ j=1,2,\ldots,n
\]</span></p>
<p><em>La Matriz de Coeficientes o Parámetros para los <span class="math inline">\(m\)</span>-MRL-Múltiples o para el MRL-Multivariado:</em></p>
<p><span class="math display">\[
\underset{(r+1)\times m}{\boldsymbol \beta }= \begin{bmatrix} \beta_{01} &amp; \beta_{02} &amp; \beta_{03} &amp; \cdots &amp; \beta_{0m} \\
\beta_{11} &amp; \beta_{12} &amp; \beta_{13} &amp; \cdots &amp; \beta_{1m} \\
\beta_{21} &amp; \beta_{22} &amp; \beta_{23} &amp; \cdots &amp; \beta_{2m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\beta_{r1} &amp; \beta_{r2} &amp; \beta_{r3} &amp; \cdots &amp; \beta_{rm}
\end{bmatrix} =\begin{bmatrix}  \uparrow  &amp; \uparrow &amp;  &amp; \uparrow \\  \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (1)} } &amp;  
\underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (2)} } &amp; \cdots &amp; \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp;  &amp; \downarrow\end{bmatrix}  \ \  ; \ \ \
\underset{(r+1) \times 1}{ \underline{\boldsymbol{\beta}}_{\ (k)} }=\begin{bmatrix} \beta_{0k} \\ \beta_{1k} \\ \beta_{2k}\\ \vdots \\ \beta_{rk} \end{bmatrix}, \ \\
\ k=1,2,\ldots,m  \\
= \begin{bmatrix} \longleftarrow \underset{1 \times m}{ \underline{\boldsymbol \beta}_{\ 0}^{\ t} } \longrightarrow  \\   \longleftarrow \underset{1 \times m}{\underline{\boldsymbol \beta}_{\ 1}^{\ t} } \longrightarrow \\ \vdots  \\  \longleftarrow\underset{1 \times m}{\underline{\boldsymbol \beta}_{\ r}^{\ t} } \longrightarrow \end{bmatrix}_{(r+1) \times m}, \ \ \ \ \ \ \underset{m \times 1}{ \underline{\boldsymbol \beta}_{\ l} }=\begin{bmatrix} \beta_{l1} \\ \beta_{l2} \\ \beta_{l3}\\ \vdots \\ \beta_{lm} \end{bmatrix}, \ \ \ l=0,1,2,\ldots,r
\]</span></p>
<p><em>La Matriz de Errores para los <span class="math inline">\(m\)</span>-MRL-Múltiples o para el MRL-Multivariado:</em>
<span class="math display">\[
\underset{n\times m}{\boldsymbol \varepsilon }= \begin{bmatrix} \varepsilon_{11} &amp; \varepsilon_{12} &amp; \varepsilon_{13} &amp; \cdots &amp; \varepsilon_{1m} \\
\varepsilon_{21} &amp; \varepsilon_{22} &amp; \varepsilon_{23} &amp; \cdots &amp; \varepsilon_{2m} \\
\varepsilon_{31} &amp; \varepsilon_{32} &amp; \varepsilon_{33} &amp; \cdots &amp; \varepsilon_{3m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\varepsilon_{n1} &amp; \varepsilon_{n2} &amp; \varepsilon_{n3} &amp; \cdots &amp; \varepsilon_{nm} \\
\end{bmatrix} = \begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (1)} } &amp;  
\underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (2)} } &amp; \cdots &amp; \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix}  \ \ \ \ ; \ \ \ \ \ \ \
\underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (k)} }=\begin{bmatrix} \varepsilon_{1k} \\ \varepsilon_{2k} \\ \varepsilon_{3k}\\ \vdots \\ \varepsilon_{nk} \end{bmatrix}, \ \\ \ k=1,2,\ldots,m   \\
= \begin{bmatrix} \longleftarrow \underset{1 \times m}{ \underline{\boldsymbol \varepsilon}_{\ 1}^t } \longrightarrow  \\ \longleftarrow \underset{1 \times m}{\underline{\boldsymbol \varepsilon}_{\ 2}^t } \longrightarrow \\ \vdots \\  \longleftarrow\underset{1 \times m}{\underline{\boldsymbol \varepsilon}_{\ n}^t } \longrightarrow \end{bmatrix}_{n \times m}, \ \ \ \ \ \ \underset{m \times 1}{ \underline{\boldsymbol \varepsilon}_{\ j} }=\begin{bmatrix} \varepsilon_{j1} \\ \varepsilon_{j2} \\ \varepsilon_{j3}\\ \vdots \\ \varepsilon_{jm} \end{bmatrix}, \ \ \ j=1,2,\ldots,n
\]</span></p>
</div>
<div id="mrl-multivariado-en-forma-matricial" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> MRL-Multivariado en forma Matricial<a href="mrl-multiv.html#mrl-multivariado-en-forma-matricial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display" id="eq:mrl-multivariado-matricial">\[
\begin{equation}
\underset{n\times m}{\mathbf{Y}}=\underset{n\times (r+1)}{\mathbf{Z}}\  \underset{(r+1) \times m}{\Large \boldsymbol \beta} + \underset{n\times m}{\Large \boldsymbol \varepsilon}
\end{equation}
\tag{5.39}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n\times 1}{ \underline{\mathbf{y}}_{\ (1)} } &amp;  
\underset{n \times 1}{\underline{\mathbf{y}}_{\ (2)} } &amp; \cdots &amp; \underset{n\times 1}{\underline{\mathbf{y}}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix}=\begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n\times 1}{ \underline{\mathbf{z}}_{\ (0)} } &amp;  
\underset{n \times 1}{\underline{\mathbf{z}}_{\ (1)} } &amp; \cdots &amp; \underset{n\times 1}{\underline{\mathbf{z}}_{\ (r)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix} \begin{bmatrix}  \uparrow  &amp; \uparrow &amp;  &amp; \uparrow \\  \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (1)} } &amp;  
\underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (2)} } &amp; \cdots &amp; \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp;  &amp; \downarrow\end{bmatrix} + \begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (1)} } &amp;  
\underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (2)} } &amp; \cdots &amp; \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix}
\]</span></p>
<p>con:
<span class="math display">\[
E\bigl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ \bigr] = \underline{\mathbf{0}}_{\ n} \ \ \ ; \ \ \ \ Cov\bigl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ ; \ \underline{\boldsymbol \varepsilon}_{\ (k)}  \ \bigr]=\sigma_{ik}\ \mathbf{I}_n \ \ \ \ \ ; \ \ \ \ \ \ Var\bigl[\ \underline{\boldsymbol \varepsilon}_{\ (i)}  \ \bigr]=\sigma_{ii}\ \mathbf{I}_n \ \ \ \ ; \ \ \ \ i,k=1,2,\ldots,m
\]</span></p>
<p>Las <span class="math inline">\(m\)</span>-observaciones sobre el <span class="math inline">\(j\)</span>-ésimo ensayo o individuo
<span class="math display">\[
\underset{1\times m}{\underline{\mathbf{y}}_{\ (j)}^{\ t}}=\bigl (\ Y_{j1} \ ,\ Y_{j2} \ ,\ Y_{j3} \ ,\ \cdots \ ,\ Y_{jm}\ \bigr) \ \ \ , \ \ \ j=1,2,\ldots,n
\]</span>
es decir:
<span class="math display">\[
\underset{m\times 1}{\underline{\mathbf{y}}_{\ (j)}}=\begin{bmatrix} Y_{j1} \\  Y_{j2} \\ Y_{j3} \\ \vdots \\ Y_{jm}\ \end{bmatrix}_{m\times 1} \ \ \ , \ \ \ j=1,2,\ldots,n
\]</span></p>
<p>tiene matriz de varianzas-covarianzas dadas por:
<span class="math display">\[
Var\bigl[\ \underline{\mathbf{y}}_{\ (j)} \ \bigr] = \underset{m\times m}{\mathbf{\Sigma} }= \begin{bmatrix} \sigma_{11} &amp;  \sigma_{12} &amp; \cdots &amp; \sigma_{1m} \\
\sigma_{21} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{m1} &amp; \sigma_{m2} &amp; \cdots &amp; \sigma_{mm}
\end{bmatrix}_{m\times m}
\]</span></p>
<p>pero las observaciones de diferentes ensayos, <span class="math inline">\(j,l\)</span>-por ejemplo son no-correlacionados, es decir:</p>
<p><span class="math display">\[
Cov\bigl[\ \underline{\mathbf{y}}_{\ (j)} \ , \ \underline{\mathbf{y}}_{\ (l)} \ \bigr] = \sigma^2\ \mathbf{I}_{\ m} = \begin{bmatrix} \sigma^2 &amp;  0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{bmatrix}_{m\times m} \ \ \ ; \ \ \ \text{para:}\ \ \ j,l=1,2,\ldots,n
\]</span></p>
<p>En este modelo se tiene que la matriz: <span class="math inline">\(\underset{(r+1)\times m}{\Large \boldsymbol \beta}\)</span> y los <span class="math inline">\(\sigma_{ik}\)</span>-son los parámetros desconocidos del modelo. La matriz diseño <span class="math inline">\(\mathbf{Z}\)</span> tiene como <span class="math inline">\(j\)</span>-ésima fila a:
<span class="math display">\[
\underline{\mathbf{z}}_{\ (j)}= \begin{bmatrix} z_{jo} &amp;  z_{j1} &amp;  z_{j2} &amp; \cdots &amp;  z_{jr}  \end{bmatrix} \ , \ \ \ \ j=1,2,\ldots,n.
\]</span></p>
</div>
<div id="m-mrl-múltiples" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> <span class="math inline">\(m\)</span>-MRL-Múltiples<a href="mrl-multiv.html#m-mrl-múltiples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <span class="math inline">\(i\)</span>-ésima variable respuesta <span class="math inline">\(\underline{\mathbf{y}}_{\ (i)}\)</span>, para <span class="math inline">\(i=1,2,\ldots,m\)</span>, sigue el sigueinte MRL-múltiple:
<span class="math display" id="eq:mrlm-i-esima-respuesta">\[
\begin{equation}
\underset{n\times 1}{\underline{\mathbf{y}}_{\ (i)} }=  \underset{n\times (r+1)}{\mathbf{Z}}\ \ \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (i)} } + \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (i)} } \ \ , \ \ \ i=1,2,\ldots,m
\end{equation}
\tag{5.40}
\]</span></p>
<p>con:
<span class="math display">\[
E\bigl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ \bigr] = \underline{\mathbf{0}}_{\ n} \ \ \ , \ \ \ \ Var\bigl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ \bigr]=\sigma_{ii}\ \mathbf{I}_n \ \ , \ \ i=1,2,\ldots,m
\]</span></p>
<p>sin embargo, los errores para diferentes respuestas sobre el mismo ensayo o individuos pueden estar correlacionados.</p>
<p>Por ejemplo para la primera variable respuesta <span class="math inline">\(\underline{\mathbf{y}}_{\ (1)}\)</span>, se tendría el MRL-Múltiple::
<span class="math display">\[
\begin{bmatrix}  \uparrow   \\  \underset{n\times 1}{ \underline{\mathbf{y}}_{\ (1)} }  \\ \downarrow  \end{bmatrix}= \begin{bmatrix}  \uparrow  &amp; \uparrow &amp; \cdots &amp; \uparrow \\  \underset{n\times 1}{ \underline{\mathbf{z}}_{\ (0)} } &amp;  
\underset{n \times 1}{\underline{\mathbf{z}}_{\ (1)} } &amp; \cdots &amp; \underset{n\times 1}{\underline{\mathbf{z}}_{\ (r)} } \\ \downarrow  &amp; \downarrow &amp; \cdots &amp; \downarrow\end{bmatrix}\begin{bmatrix}  \uparrow   \\  \underset{(r+1)\times 1}{ \underline{\boldsymbol \beta}_{\ (1)} }  \\  \downarrow\end{bmatrix}+ \begin{bmatrix}  \uparrow   \\  \underset{n \times 1}{ \underline{\boldsymbol \varepsilon}_{\ (1)} }  \\ \downarrow \end{bmatrix}\\
\begin{bmatrix}  y_{11} \\ y_{21} \\ \vdots \\ y_{n1}  \end{bmatrix}= \begin{bmatrix} z_{10} &amp; z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1r} \\
z_{20} &amp; z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2r} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
z_{n0} &amp; z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{nr}
\end{bmatrix} \begin{bmatrix} \beta_{01} \\ \beta_{11} \\ \vdots \\ \beta_{r1}   \end{bmatrix} + \begin{bmatrix} \varepsilon_{11} \\ \varepsilon_{21} \\ \vdots \\\varepsilon_{n1}  \end{bmatrix}
\]</span></p>
</div>
<div id="estimador-de-mínimos-cuadrados-de-los-parámetros" class="section level3 hasAnchor" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Estimador de Mínimos Cuadrados de los Parámetros<a href="mrl-multiv.html#estimador-de-mínimos-cuadrados-de-los-parámetros" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dados los resultados <span class="math inline">\(\mathbf{Y}\)</span> y los valores de las variables regresoras <span class="math inline">\(\mathbf{Z}\)</span> con rango-completo, se determina el <em>Estimador de Mínimos Cuadrados</em> <span class="math inline">\(\widehat{ \underline{\boldsymbol \beta}}_{\ (i)}\)</span>-exclusivamente para las observaciones <span class="math inline">\(\underline{\mathbf{y}}_{\ (k) }\)</span>-de la <span class="math inline">\(k\)</span>-ésima variable respuesta. En este caso se tiene que:
<span class="math display" id="eq:beta-estimado-mrlm-i-respuesta">\[
\begin{equation}
\underset{(r+1)\times 1}{\widehat{ \underline{\boldsymbol \beta}}_{\ (k)} }=\underset{(r+1)\times (r+1)}{(\mathbf{Z}^{t}\mathbf{Z})^{-1}}\  \underset{(r+1)\times n\ \ n\times 1}{ \underset{}{\mathbf{Z}}^t\ \underline{\mathbf{y}}_{\ (k) } }=\begin{bmatrix} \widehat{\ \beta\ }_{0k} \\ \widehat{\ \beta\ }_{1k} \\ \widehat{\ \beta\ }_{2k} \\ \vdots \\ \widehat{\ \beta\ }_{rk} \end{bmatrix} \ \ , \ \ k=1,2,\ldots,m
\end{equation}
\tag{5.41}
\]</span></p>
<p>Recolectando las <span class="math inline">\(m\)</span>-estimaciones univariadas <span class="math inline">\(\widehat{\ \underline{\boldsymbol \beta}\ }_{\ (1)},\widehat{ \ \underline{\boldsymbol \beta}\ }_{\ (2)},\cdots,\widehat{ \ \underline{\boldsymbol \beta}\ }_{\ (m)}\)</span> de mínimos cuadrados se obtiene que:
<span class="math display">\[
\underset{(r+1)\times m}{\Large \widehat{\boldsymbol \beta}}= \begin{bmatrix}  \uparrow  &amp; \uparrow &amp;  &amp; \uparrow \\  \underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (1)} } &amp;  
\underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (2)} } &amp; \cdots &amp; \underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp;  &amp; \downarrow\end{bmatrix} \\
\text{con:}  \ \ \ \
\underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (k)} }=\begin{bmatrix} \widehat{\ \beta\ }_{0k} \\ \widehat{\ \beta\ }_{1k} \\ \widehat{\ \beta\ }_{2k} \\ \vdots \\ \widehat{\ \beta\ }_{rk} \end{bmatrix}= (\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\ \underline{\mathbf{y}}_{\ (k) }
\]</span></p>
<p>o equivalentemente
<span class="math display" id="eq:beta-mrl-multivariado-minimos-cuadrados">\[
\begin{equation}
\underset{(r+1)\times m}{\Large \widehat{\boldsymbol \beta}}=  \underset{(r+1)\times (r+1)}{(\mathbf{Z}^t\ \mathbf{Z})^{-1}}\underset{(r+1)\times m}{\mathbf{Z}^t \ \mathbf{Y} } = \begin{bmatrix}  \uparrow  &amp; \uparrow &amp;  &amp; \uparrow \\  \underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (1)} } &amp;  
\underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (2)} } &amp; \cdots &amp; \underset{(r+1)\times 1}{ \widehat{\ \underline{\boldsymbol \beta}\ }_{\ (m)} } \\ \downarrow  &amp; \downarrow &amp;  &amp; \downarrow\end{bmatrix}
\end{equation}
\tag{5.42}
\]</span></p>
<p><span class="math display">\[
\underset{(r+1)\times m}{\Large \widehat{\boldsymbol \beta}}= \begin{bmatrix} \widehat{\beta}_{01} &amp; \widehat{\beta}_{02} &amp; \widehat{\beta}_{03} &amp; \cdots &amp; \widehat{\beta}_{0m} \\
\widehat{\beta}_{11} &amp; \widehat{\beta}_{12} &amp; \widehat{\beta}_{13} &amp; \cdots &amp; \widehat{\beta}_{1m} \\
\widehat{\beta}_{21} &amp; \widehat{\beta}_{22} &amp; \widehat{\beta}_{23} &amp; \cdots &amp; \widehat{\beta}_{2m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\widehat{\beta}_{r1} &amp; \widehat{\beta}_{r2} &amp; \widehat{\beta}_{r3} &amp; \cdots &amp; \widehat{\beta}_{rm}
\end{bmatrix}
\]</span></p>
<p>Usando la matriz de Estimadores de Mínimos Cuadrados <span class="math inline">\(\Large \widehat{\boldsymbol \beta}\)</span>, se pueden formar las matrices:
<span class="math display" id="eq:matriz-estimada-mrlm-multivariado">\[
\begin{equation}
\underset{n\times m}{\widehat{\mathbf{Y}} }=\underset{n\times (r+1) \ \ (r+1)\times m}{\mathbf{Z}\  \widehat{\Large \boldsymbol \beta}} = \underset{n \times n}{ \underbrace{ \mathbf{Z}\ (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t } } \mathbf{Y} = \mathbf{H}\ \mathbf{Y}   
\end{equation}
\tag{5.43}
\]</span></p>
<p><span class="math display" id="eq:matriz-residuales-mrlm-multivariado">\[
\begin{equation}
\underset{n\times m}{ \widehat{\huge \boldsymbol \varepsilon} } =\mathbf{Y}-\widehat{\mathbf{Y}}=\mathbf{Z}\  \widehat{ \Large \boldsymbol \beta} = \mathbf{Y}-\underbrace{\mathbf{Z}\ (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t } \mathbf{Y}= \mathbf{Y}-\mathbf{H}\ \mathbf{Y} =\biggl[\mathbf{I}-\mathbf{H}\biggr]\ \mathbf{Y}   
\end{equation}
\tag{5.44}
\]</span></p>
<p>con,
<span class="math display">\[
\underset{n\times n}{\mathbf{H}}= \underset{n\times (r+1) \ \ (r+1)\times (r+1)\ \ (r+1)\times n}{ \mathbf{Z}\ (\mathbf{Z}^t\ \mathbf{Z})^{-1}\ \mathbf{Z}^t}
\]</span></p>
<p>Las condiciones de ortogonalidad entre los residuales, los valores predichos y las columnas de <span class="math inline">\(\mathbf{Z}\)</span> que se cumplen en Regresión Lineal Clásica (o RL-Múltiple), también se cumplen en Regresión Lineal Multivariada (RL-Multivariada).</p>
<p>A partir de:
<span class="math display">\[
\mathbf{Z}^t \ \biggl[\mathbf{I}_n-\mathbf{H}\biggr] = \mathbf{Z}^t \ \biggl[\ \mathbf{I}- \mathbf{Z}\ (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t\ \biggr] = \mathbf{Z}^t - \mathbf{Z}^t = \underset{(r+1)\times n}{ \mathbf{O} }
\]</span></p>
<p>se tiene que:
<span class="math display">\[
\mathbf{Z}^t\ \widehat{ \huge \boldsymbol \varepsilon} = \mathbf{Z}^t\ \biggl[\mathbf{I}-\mathbf{H}\biggr]\ \mathbf{Y} = \mathbf{O}\ \mathbf{Y} = \underset{(r+1)\times m}{ \mathbf{O} }
\]</span></p>
<p>es decir, los vectores de residuales <span class="math inline">\(\widehat{\underline{ \boldsymbol \varepsilon}}_{\ (k)}\)</span>-son perpendiculares a las columnas de <span class="math inline">\(\mathbf{Z}\)</span>. También se tiene que:
<span class="math display">\[
\underset{m\times n \ \ n\times m}{\widehat{\mathbf{Y}}^t\ \widehat{ \huge \boldsymbol \varepsilon} }= \left(\mathbf{Z}\ \widehat{\Large \boldsymbol \beta} \right)^t\ \widehat{ \huge \boldsymbol \varepsilon}= \widehat{\Large \boldsymbol \beta}^t \mathbf{Z}^t\ \widehat{ \huge \boldsymbol \varepsilon} = \underset{m\times (r+1)\ \ (r+1)\times m}{\widehat{\Large \boldsymbol \beta}^t\ \mathbf{O} }= \underset{m\times m}{ \mathbf{O} }
\]</span></p>
<p>es decir, que los valores predichos <span class="math inline">\(\widehat{\underline{\mathbf{y}}}_{\ (i)}\)</span>-son perpendiculares a los vectores residuales <span class="math inline">\(\widehat{\underline{ \boldsymbol \varepsilon}}_{\ (k)}\)</span>.</p>
<p>Ahora, a partir de:
<span class="math display">\[
\mathbf{Y} = \widehat{\mathbf{Y}} + \widehat{\huge \varepsilon}
\]</span></p>
<p>se tiene que:
<span class="math display">\[
\mathbf{Y}^t\mathbf{Y}=(\widehat{\mathbf{Y}} + \widehat{\huge \varepsilon})^t(\widehat{\mathbf{Y}} + \widehat{\huge \varepsilon})\\
=\widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}}+\widehat{\huge \varepsilon}^t\widehat{\huge \varepsilon}+\widehat{\mathbf{Y}}^t\widehat{\huge \varepsilon}+\widehat{\huge \varepsilon}^t\widehat{\mathbf{Y}} \\
=\widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}}+\widehat{\huge \varepsilon}^t\widehat{\huge \varepsilon}+\mathbf{O}+\mathbf{O}^t
\]</span></p>
<p>es decir:
<span class="math display">\[
\mathbf{Y}^t\mathbf{Y}=\widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}}+\widehat{\huge \varepsilon}^t\widehat{\huge \varepsilon}
\]</span>
o equivalentemente:
<span class="math display">\[
\biggl( \underset{\Large{\text{ y Productos Cruzados}}}{\text{Suma Total de Cuadrados}} \biggr)= \biggl( \underset{\Large{\text{Cruzados Predicha }}}{\text{Suma de Cuadrados y Productos}} \biggr) \\ + \biggl( \underset{\Large{\text{Cruzados de Errores o Residuales}}}{\text{Suma de Cuadrados y Productos }} \biggr)
\]</span></p>
<p>La suma de cuadrados y productos cruzados de residuales <span class="math inline">\(\widehat{\huge \varepsilon}^t\widehat{\huge \varepsilon}\)</span>-también se puede escribir como sigue:
<span class="math display">\[
\widehat{\huge \varepsilon}^t\widehat{\huge \varepsilon} = \mathbf{Y}^t\mathbf{Y} - \widehat{\mathbf{Y}}^t\widehat{\mathbf{Y}} = \mathbf{Y}^t\mathbf{Y} - \widehat{\Large \boldsymbol \beta}^t \mathbf{Z}^t\  \mathbf{Z}\widehat{\Large \boldsymbol \beta}
\]</span></p>
<div class="example">
<p><span id="exm:ejemplo1-mrl-multivariado" class="example"><strong>Ejemplo 5.8  (MRL-Multivariado) </strong></span>Consideremos las siguientes matrices de datos para ajustar un MRL-Multivariado.</p>
</div>
<p><em>Matriz de datos para <span class="math inline">\(m=2\)</span>-variables respuestas:</em>
<span class="math display">\[
\underset{n\times m}{\mathbf{Y}}= \begin{bmatrix} Y_{11} &amp; Y_{12}  \\
Y_{21} &amp; Y_{22}  \\
Y_{31} &amp; Y_{32}  \\
Y_{41} &amp; Y_{42} \\
Y_{51} &amp; Y_{52}  
\end{bmatrix}_{5\times 2} = \begin{bmatrix} 1 &amp; -1\\
4 &amp; -1  \\
3 &amp; 2  \\
8 &amp; 3 \\
9 &amp; 2
\end{bmatrix}_{5\times 2}= \biggl[\ \underset{5\times 1}{ \underline{\mathbf{y}}_{\ (1)} } \ \ | \ \ \underset{5 \times 1}{\underline{\mathbf{y}}_{\ (2)} }  \ \biggr]
\]</span></p>
<p><strong>Matriz de datos de <span class="math inline">\(r=1\)</span> variable Regresora</strong></p>
<p><span class="math display">\[
\underset{n\times (r+1)}{\mathbf{Z}}= \begin{bmatrix} z_{10} &amp; z_{11} \\
z_{20} &amp; z_{21}  \\
z_{30} &amp; z_{31}  \\
z_{40} &amp; z_{41} \\
z_{50} &amp; z_{51}
\end{bmatrix}_{5\times 2} = \begin{bmatrix} 1 &amp; 0 \\
1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ 1 &amp; 4\end{bmatrix}_{5\times 2}
\]</span></p>
<p>es decir:
<span class="math display">\[
\underset{(r+1)\times n}{\mathbf{Z}^t}= \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \end{bmatrix}_{2\times 5} \ \ \ ; \ \ \ (\mathbf{Z}^t\mathbf{Z})^{-1}=\begin{bmatrix} 0.6 &amp; -0.2 \\ -0.20.1 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{Z}^t\ \underline{\mathbf{y}}_{\ (1)} = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \\ 3 \\ 8 \\ 9  \end{bmatrix}= \begin{bmatrix} 25 \\ 70  \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{Z}^t\ \underline{\mathbf{y}}_{\ (2)} = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \end{bmatrix} \begin{bmatrix} -1 \\ -1 \\ 2 \\ 3 \\ 2  \end{bmatrix}= \begin{bmatrix} 5 \\ 20  \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\widehat{\underline{\boldsymbol \beta}}_{\ (1)} =(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \underline{\mathbf{y}}_{\ (1)}=\begin{bmatrix} 0.6 &amp; -0.2 \\ -0.20.1 \end{bmatrix} \begin{bmatrix} 25 \\ 70  \end{bmatrix}=\begin{bmatrix} 1 \\ 2  \end{bmatrix}\\
\widehat{\underline{\boldsymbol \beta}}_{\ (2)} =(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \underline{\mathbf{y}}_{\ (2)}=\begin{bmatrix} 0.6 &amp; -0.2 \\ -0.20.1 \end{bmatrix} \begin{bmatrix} 5 \\ 20  \end{bmatrix}=\begin{bmatrix} -1 \\ 1  \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\underset{(r+1)\times m}{ \widehat{\boldsymbol \beta} }= \begin{bmatrix} \widehat{\beta}_{01} &amp; \widehat{\beta}_{02} \\
\widehat{\beta}_{11} &amp; \widehat{\beta}_{12}
\end{bmatrix}_{2\times 2} = \begin{bmatrix} \uparrow &amp; \uparrow \\   \underset{2\times 1}{ \widehat{\underline{\boldsymbol \beta}}_{\ (1)} } &amp; \underset{2 \times 1}{\widehat{\underline{\boldsymbol \beta}}_{\ (2)} } \\
\downarrow &amp; \downarrow\end{bmatrix}=\begin{bmatrix} 1 &amp; -1 \\ 2 &amp; 1    \end{bmatrix}=(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \ \mathbf{Y} = (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \begin{bmatrix} \uparrow &amp; \uparrow \\   \underline{\mathbf{y}}_{\ (1)} &amp; \underline{\mathbf{y}}_{\ (2)} \\
\downarrow &amp; \downarrow\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\underset{n\times m}{\widehat{\mathbf{Y}} }=\mathbf{Z}\  \widehat{\Large \boldsymbol \beta} = \begin{bmatrix} 1 &amp; 0 \\
1 &amp; 1 \\ 1 &amp; 2 \\ 1 &amp; 3 \\ 1 &amp; 4\end{bmatrix}_{5\times 2}\begin{bmatrix} 1 &amp; -1 \\ 2 &amp; 1    \end{bmatrix}_{2\times 2} =  \begin{bmatrix} 1 &amp; -1 \\
3 &amp; 0 \\ 5 &amp; 1 \\ 7 &amp; 2 \\ 9 &amp; 3\end{bmatrix}_{5\times 2}
\]</span></p>
<p><span class="math display">\[
\underset{n\times m}{\widehat{\boldsymbol \varepsilon } }=\mathbf{Y}-  \widehat{\mathbf{Y}} = \begin{bmatrix} 1 &amp; -1\\
4 &amp; -1  \\
3 &amp; 2  \\
8 &amp; 3 \\
9 &amp; 2
\end{bmatrix} -  \begin{bmatrix} 1 &amp; -1 \\
3 &amp; 0 \\ 5 &amp; 1 \\ 7 &amp; 2 \\ 9 &amp; 3\end{bmatrix}_{5\times 2}=\begin{bmatrix} 0 &amp; 0\\
1 &amp; -1  \\
-2 &amp; 1  \\
1 &amp; 1 \\
0 &amp; -1
\end{bmatrix}_{5\times 2}
\]</span></p>
<p>Observe que:
<span class="math display">\[
\underset{m\times n}{\widehat{\boldsymbol \varepsilon }^{\ \ t}}\ \underset{n\times m}{\widehat{\mathbf{Y}} }=  \begin{bmatrix} 0 &amp; 1 &amp; -2 &amp; 1 &amp; 0 \\
0 &amp; -1&amp; 1 &amp; 1 &amp; -1
\end{bmatrix}_{2\times 5} \begin{bmatrix} 1 &amp; -1 \\
3 &amp; 0 \\ 5 &amp; 1 \\ 7 &amp; 2 \\ 9 &amp; 3\end{bmatrix}_{5\times 2} =\begin{bmatrix} 0 &amp; 0\\
0 &amp; 0
\end{bmatrix}_{2\times 2}
\]</span></p>
<p><span class="math display">\[
\underset{m\times n}{\widehat{\boldsymbol \varepsilon }^{\ \ t}}\ \underset{n\times m}{\widehat{\boldsymbol \varepsilon }}=  \begin{bmatrix} 0 &amp; 1 &amp; -2 &amp; 1 &amp; 0 \\
0 &amp; -1&amp; 1 &amp; 1 &amp; -1
\end{bmatrix}_{2\times 5} \begin{bmatrix} 0 &amp; 0 \\
1 &amp; -1 \\ -2 &amp; 1 \\ 1 &amp; 1 \\ 0 &amp; -1\end{bmatrix}_{5\times 2} =\begin{bmatrix} 6 &amp; -2\\
-2 &amp; 4
\end{bmatrix}_{2\times 2}
\]</span></p>
<p>Además,
<span class="math display">\[
\underset{m\times n}{\mathbf{Y}^{\ t} }\underset{n\times m}{\mathbf{Y} }=\begin{bmatrix} 1 &amp; 4 &amp; 3 &amp; 8 &amp; 9  \\
-1 &amp; -1 &amp; 2 &amp; 3 &amp; 2 \end{bmatrix}_{2\times 5} \begin{bmatrix} 1 &amp; -1 \\
4 &amp; -1 \\ 3 &amp; 2 \\ 8 &amp; 3 \\ 9 &amp; 2\end{bmatrix}_{5\times 2}= \begin{bmatrix} 171 &amp; 43 \\ 43 &amp; 19  \end{bmatrix}_{2\times 2}
\]</span></p>
<p><span class="math display">\[
\underset{m\times n}{\widehat{\mathbf{Y}}^{\ t} }\underset{n\times m}{\widehat{\mathbf{Y}} }=\begin{bmatrix} 1 &amp; 3 &amp; 5 &amp; 7 &amp; 9  \\
-1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \end{bmatrix}_{2\times 5} \begin{bmatrix} 1 &amp; -1 \\
3 &amp; 0 \\ 5 &amp; 1 \\ 7 &amp; 2 \\ 9 &amp; 3\end{bmatrix}_{5\times 2}= \begin{bmatrix} 165 &amp; 45 \\ 45 &amp; 15  \end{bmatrix}_{2\times 2}
\]</span></p>
<p>de lo anterior se puede verificar la descomposición en sumas de cuadrados y productos cruzados, dada por:
<span class="math display">\[
\underset{m\times n}{\mathbf{Y}^{\ t} }\underset{n\times m}{\mathbf{Y} }= \underset{m\times n}{\widehat{\mathbf{Y}}^{\ t} }\underset{n\times m}{\widehat{\mathbf{Y}} } + \underset{m\times n}{\widehat{\boldsymbol \varepsilon }^{\ \ t}}\underset{n\times m}{\widehat{\boldsymbol \varepsilon }} \\
\begin{bmatrix} 171 &amp; 43 \\ 43 &amp; 19  \end{bmatrix}= \begin{bmatrix} 165 &amp; 45 \\ 45 &amp; 15  \end{bmatrix} + \begin{bmatrix} 6 &amp; -2\\
-2 &amp; 4
\end{bmatrix}
\]</span></p>
</div>
<div id="propiedades-de-estimadores-de-mínimos-cuadrados-del-mrl-multivariado" class="section level3 hasAnchor" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> Propiedades de Estimadores de Mínimos Cuadrados del MRL-Multivariado<a href="mrl-multiv.html#propiedades-de-estimadores-de-mínimos-cuadrados-del-mrl-multivariado" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:estimadores-ms-mrl-multivariado" class="theorem"><strong>Teorema 5.9  (Teorema Estimadores de Mínimos Cuadrados en el MRL-Multivariado) </strong></span>El Estimador de Mínimos Cuadrados obtenido de <a href="mrl-multiv.html#eq:beta-mrl-multivariado-minimos-cuadrados">(5.42)</a></p>
</div>
<p><span class="math display">\[
\underset{(r+1)\times m}{\Large \widehat{\boldsymbol \beta}}=  \underset{(r+1)\times (r+1)}{(\mathbf{Z}^t\mathbf{Z})^{-1}}\ \underset{(r+1)\times m}{\mathbf{Z}^t \mathbf{Y} } = \biggl[\  \widehat{\underline{\boldsymbol \beta}}_{\ (1)}  \ \ | \ \ \widehat{\underline{\boldsymbol \beta}}_{\ (2)}  \ \ | \ \ \cdots \ \ | \ \ \widehat{\underline{\boldsymbol \beta}}_{\ (m)}  \ \biggr]
\]</span></p>
<p>con <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\((r+1) &lt; n\)</span>, cumple las siguientes propiedades.
<span class="math display">\[
E\biggl[\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\ \biggr]=\underline{\boldsymbol \beta}_{\ (i)} \ \ \ \text{es decir:} \ \ \ \ E\biggl[ \ {\Large \widehat{\boldsymbol \beta} }\ \biggr]=\boldsymbol {\Large \beta }
\]</span></p>
<p>y
<span class="math display">\[
Cov\biggl[\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\ , \ \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \biggr]= \sigma_{ik}\ (\mathbf{Z}^t\mathbf{Z})^{-1} \ \ \ \ i,k=1,2,\ldots,m
\]</span></p>
<p>Además, la matriz de residuales
<span class="math display">\[
\underset{n\times m}{ \widehat{\huge \boldsymbol \varepsilon} } =\mathbf{Y}-\mathbf{Z}\  \widehat{ \Large \boldsymbol \beta} = \biggl[\  \widehat{\underline{\boldsymbol \varepsilon}}_{\ (1)}  \ \ | \ \ \widehat{\underline{\boldsymbol \varepsilon}}_{\ (2)}  \ \ | \ \ \cdots \ \ | \ \ \widehat{\underline{\boldsymbol \varepsilon}}_{\ (m)}  \ \biggr]
\]</span></p>
<p>cumple que:
<span class="math display">\[
E\biggl[\ \widehat{\underline{\boldsymbol \varepsilon}}_{\ (i)}\ \biggr]=\underline{\boldsymbol 0}_{\ n\times 1} \ \ \ \text{es decir:} \ \ \ \ E\biggl[ \ {\Large \widehat{\boldsymbol \varepsilon} }\ \biggr]=\boldsymbol {\Large O }_{n\times m}
\]</span></p>
<p>y
<span class="math display">\[
E\biggl[\ \widehat{\underline{\boldsymbol \varepsilon}}_{\ (i)}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}_{\ (k)} \biggr]= (n-r-1)\ \sigma_{ik} \ \ \ \ i,k=1,2,\ldots,m
\]</span>
es decir,
<span class="math display">\[
E\biggl[\ \left(\frac{1}{n-r-1} \right) \widehat{ {\Large \boldsymbol \varepsilon}}^{\ t} \widehat{{\Large \boldsymbol \varepsilon}} \biggr]= \mathbf{\Sigma}
\]</span></p>
<p>además, las matrices <span class="math inline">\(\widehat{ \huge{ \boldsymbol \varepsilon}}\)</span> y <span class="math inline">\(\widehat{{\Large \boldsymbol \beta}}\)</span> son no-correlacionadas, es decir que:
<span class="math display">\[
Cov\biggl[\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\ ,\  \widehat{\underline{\boldsymbol \varepsilon}}_{\ (k)} \biggr]= \underset{(r+1)\times n}{\mathbf{O}}
\]</span>
de donde,
<span class="math display">\[
Cor\biggl[\ \widehat{ \beta}_{\ (ki)}\ ,\  \widehat{\boldsymbol \varepsilon}_{\ (ji)} \biggr]=0 \ \ \ , k=0,1,\cdots,r\ \ ,\ \ i=1,2,\cdots,m \ \ , \ \ j=1,2,\cdots,n
\]</span></p>
<p><strong>Mas Propiedades:</strong></p>
<p>Los vectores de medias y matrices de varianzas-covarianzas obtenidos en el teorema <a href="mrl-multiv.html#thm:estimadores-ms-mrl-multivariado">5.9</a>, permiten obtener las propiedades muestrales de los predictores de mínimos cuadrados.</p>
<p>Primero se considera el problema de estimar el vector de medias cuando las variables predictoras toman los valores dados en
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}=\begin{bmatrix}1 \\
z_{01} \\ z_{02} \\ \vdots \\ z_{0r}\end{bmatrix}_{(r+1)\times 1}
\]</span></p>
<p>La <strong>media</strong> para la <strong>i-ésima variable respuesta</strong> evaluada en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span> está dada por: <span class="math inline">\(\mu _{y_{\ i0}}=E[\ y_{\ i0}\ ]=y_{\ i0}=\underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (i)}\)</span> <strong>y es estimada por</strong>: <span class="math inline">\(\widehat{\mu }_{y_{\ i0}}=\widehat{E[\ y_{\ i0}\ ]}=\widehat{y}_{\ i0}=\underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span>, es decir, por la i-ésima componente de las relaciones de regresión ajustadas.</p>
<p>Colectivamente, los estimadores para todas las <span class="math inline">\(m\)</span>-variables respuestas evaluadas en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span> están dados por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^t\ {\Large \widehat{ \boldsymbol \beta} }=\begin{bmatrix} \widehat{y}_{\ 10} &amp; | &amp; \widehat{y}_{\ 20} &amp; | &amp;    \cdots  &amp; | &amp; \widehat{y}_{\ m0} \end{bmatrix}
= \begin{bmatrix} \underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (1)} &amp; | &amp; \underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (2)} &amp; | &amp;    \cdots  &amp; | &amp; \underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (m)} \end{bmatrix}
\]</span></p>
<p>el cual es un estimador insesgado de: <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}^t\ {\Large \boldsymbol \beta}\)</span>, ya que:</p>
<p><span class="math display">\[
E\biggl[ \underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\biggr]=\underline{\mathbf{z}}_{\ 0}^t\ E\biggl[ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\biggr] =\underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (i)} \ \ \ , \ \  \text{para cada componente}\ \ i=1,2,\cdots,m
\]</span></p>
<p>A partir de la matriz de varianzas-covarianzas para <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span> y <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (k)}\)</span> dada por:
<span class="math display">\[
Cov\biggl[\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\ , \ \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \biggr]= \sigma_{ik}\ (\mathbf{Z}^t\mathbf{Z})^{-1} \ \ \ \ i,k=1,2,\ldots,m
\]</span></p>
<p>los errores de estimación dados por:
<span class="math display">\[
\widehat{\varepsilon}_{\ i0}=y_{\ i0}-\widehat{y}_{\ i0}=\underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (i)}-\underline{\mathbf{z}}_{\ 0}^t\  \widehat{\underline{\boldsymbol \beta}}_{\ (i)}=\underline{\mathbf{z}}_{\ 0}^t(\  \underline{\boldsymbol \beta}_{\ i} - \widehat{\underline{\boldsymbol \beta}}_{\ i}\ )
\]</span></p>
<p>tienen covarianzas dadas por:
<span class="math display">\[
\begin{align*}
Cov\biggl[ \widehat{\varepsilon}_{\ i0} \ , \ \widehat{ \varepsilon}_{\ k0}  \biggr]= &amp; Cov\biggl[ \underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (i)}-\underline{\mathbf{z}}_{\ 0}^t\  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \ , \ \underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (k)}-\underline{\mathbf{z}}_{\ 0}^t\  \widehat{\underline{\boldsymbol \beta}}_{\ (k)}\biggr]\\  &amp;=Cov\biggl[ \underline{\mathbf{z}}_{\ 0}^t\ \left( \underline{\boldsymbol \beta}_{\ (i)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \right)\ , \ \underline{\mathbf{z}}_{\ 0}^t\ \left(\underline{\boldsymbol \beta}_{\ (k)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \right)\biggr] \\
&amp;= E\left\{ \biggl[ \underline{\mathbf{z}}_{\ 0}^t\ \left( \underline{\boldsymbol \beta}_{\ (i)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \right) \biggl] \biggl[ \underline{\mathbf{z}}_{\ 0}^t\ \left(\underline{\boldsymbol \beta}_{\ (k)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \right)\biggr]^t  \right\} \\
&amp;= E\left[ \underline{\mathbf{z}}_{\ 0}^t\ \left( \underline{\boldsymbol \beta}_{\ (i)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \right)   \left(\underline{\boldsymbol \beta}_{\ (k)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \right)^t \ \underline{\mathbf{z}}_{\ 0}  \right] \\
&amp;= \underline{\mathbf{z}}_{\ 0}^t\ E\left[ \left( \underline{\boldsymbol \beta}_{\ (i)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \right)   \left(\underline{\boldsymbol \beta}_{\ (k)}-  \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \right)^t \right] \underline{\mathbf{z}}_{\ 0}   \\
&amp;= \underline{\mathbf{z}}_{\ 0}^t\ Cov\left[  \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \ , \    \widehat{\underline{\boldsymbol \beta}}_{\ (k)} \right] \underline{\mathbf{z}}_{\ 0}   \\
&amp;= \underline{\mathbf{z}}_{\ 0}^t\ \sigma_{ik}\ (\mathbf{Z}^t\mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} \\
Cov\biggl[ \widehat{ \varepsilon}_{\ i0} \ , \ \widehat{ \varepsilon}_{\ k0}  \biggr]&amp;=\sigma_{ik}\  \underline{\mathbf{z}}_{\ 0}^t\  (\mathbf{Z}^t\mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0}=\sigma_{ik}\ h_{00} \ \ \ \ i,k=1,2,\ldots,m
\end{align*}
\]</span></p>
<p>Ahora, veamos el problema relacionado de pronosticar un nuevo vector de observaciones
<span class="math display">\[
\underline{\mathbf{y}}_{\ 0}=\begin{bmatrix} y_{01} \\
y_{02} \\ \vdots \\ y_{0m}\end{bmatrix}_{m\times 1}
\]</span>
en los valores de la variables regresoras dados por: <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>.</p>
<p>De acuerdo al modelo de regresión asociado a la <span class="math inline">\(i\)</span>-ésima variable respuesta dado por:
<span class="math display">\[
y_{\ 0i}= \underline{\mathbf{z}}_{\ 0}^t \ \underline{\boldsymbol \beta}_{\ i} +\boldsymbol \varepsilon_{\ 0i} , \ \ i=1,2,\ldots,m
\]</span></p>
<p>donde el “nuevo” vector de errores:
<span class="math display">\[
\underline{\boldsymbol \varepsilon}_{\ 0}=\begin{bmatrix} \varepsilon_{01} \\ \varepsilon_{02} \\ \vdots \\ \varepsilon_{0m} \end{bmatrix}_{m\times 1}
\]</span></p>
<p>es independiente de los errores <span class="math inline">\(\boldsymbol {\huge{\varepsilon}}\)</span> y cumplen que:
<span class="math display">\[
E[\varepsilon_{0i}]=0 \ \ \ \ \text{y} \ \ \ Cov\biggl[\varepsilon_{0i}\ , \ \varepsilon_{0k}\biggr]=E\biggl[\varepsilon_{0i}\varepsilon_{0k}\biggr]=\sigma_{ik}\ \ , \ \ \ i,k=1,2,\ldots,m
\]</span></p>
<p>El error de pronóstico para la <span class="math inline">\(i\)</span>-ésima componente de <span class="math inline">\(\underline{\mathbf{y}}_{\ 0}\)</span> es:
<span class="math display">\[
y_{\ 0i}-\widehat{y}_{\ 0i}=y_{\ 0i} - \underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ i}= y_{\ 0i} - \underline{\mathbf{z}}_{\ 0}^t \ \underline{\boldsymbol \beta}_{\ i} + \underline{\mathbf{z}}_{\ 0}^t \ \underline{\boldsymbol \beta}_{\ i} - \underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ i}\\ =\varepsilon_{\ 0i} - \underline{\mathbf{z}}_{\ 0}^t(\widehat{\underline{\boldsymbol \beta}}_{\ i}-  \ \underline{\boldsymbol \beta}_{\ i})=\varepsilon_{\ 0i} - \widehat{\varepsilon}_{\ 0i}
\]</span></p>
<p>de donde:
<span class="math display">\[
E\biggl[ y_{\ 0i}-\widehat{y}_{\ 0i}\biggr] = E\biggl[\varepsilon_{\ 0i} - \underline{\mathbf{z}}_{\ 0}^t(\widehat{\underline{\boldsymbol \beta}}_{\ i}-  \ \underline{\boldsymbol \beta}_{\ i})\biggr]\\
=E\bigl[\varepsilon_{\ 0i}\bigr] - \underline{\mathbf{z}}_{\ 0}^t\  E\biggr[\widehat{\underline{\boldsymbol \beta}}_{\ i} -  \ \underline{\boldsymbol \beta}_{\ i}\biggr]=0 - \underline{\mathbf{z}}_{\ 0}^t\ \underline{\mathbf{0}}=0-0\\
E\biggl[ y_{\ 0i}-\widehat{y}_{\ 0i}\biggr] = 0
\]</span></p>
<p>lo que indica que: <span class="math inline">\(\widehat{y}_{\ 0i}=\underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span>-es un estimador insesgado de: <span class="math inline">\(y_{\ 0i}\)</span>.</p>
<p>Los errores de pronóstico tienen covarianzas dadas por:
<span class="math display">\[
\begin{align}
Cov\biggl[ y_{\ 0i}-\widehat{y}_{\ 0i} \ , \ y_{\ 0k}-\widehat{y}_{\ 0k}\biggr] &amp;=Cov\biggl[ y_{\ 0i}-\underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ (i)} \ , \ y_{\ 0k}-\underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ (k)}\biggr] \\
&amp;=E\biggl[ \biggl(y_{\ 0i}-\underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}\biggr) \biggl(y_{\ 0k}-\underline{\mathbf{z}}_{\ 0}^t \ \widehat{\underline{\boldsymbol \beta}}_{\ (k)}\biggr)\biggr] \\
&amp;=E\biggl[ \biggl(\varepsilon_{\ 0i} - \underline{\mathbf{z}}_{\ 0}^t(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-  \ \underline{\boldsymbol \beta}_{\ (i)})\biggr) \biggl(\varepsilon_{\ 0k} - \underline{\mathbf{z}}_{\ 0}^t(\widehat{\underline{\boldsymbol \beta}}_{\ (k)}-  \ \underline{\boldsymbol \beta}_{\ (k)})\biggr)\biggr] \\
&amp;=E[\ \varepsilon_{\ 0i}\ \varepsilon_{\ 0k} \ ] + \underline{\mathbf{z}}_{\ 0}^t\ E\biggl[(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-  \ \underline{\boldsymbol \beta}_{\ (i)})(\widehat{\underline{\boldsymbol \beta}}_{\ (k)}-  \ \underline{\boldsymbol \beta}_{\ (k)})^t\ \biggr]\underline{\mathbf{z}}_{0} \\
&amp; - \underline{\mathbf{z}}_{\ 0}^t\ E\biggl[(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-  \ \underline{\boldsymbol \beta}_{\ (i)})\varepsilon_{\ ok}  \biggr] - E\biggl[\varepsilon_{\ oi}(\widehat{\underline{\boldsymbol \beta}}_{\ (k)} -  \ \underline{\boldsymbol \beta}_{\ (k)})^t  \biggr]\ \underline{\mathbf{z}}_{\ 0}\\
&amp;=\sigma_{ik}  + \sigma_{ik}\ \underline{\mathbf{z}}_{\ 0}^t \ (\mathbf{Z}^t\mathbf{Z})^{-1} \ \underline{\mathbf{z}}_{\ 0}  \\
&amp;= \sigma_{ik} \biggl(1 + \underline{\mathbf{z}}_{\ 0}^t \ (\mathbf{Z}^t\mathbf{Z})^{-1} \ \underline{\mathbf{z}}_{\ 0}  \biggr)\\
&amp;= \sigma_{ik}\ (1+h_{00})
\end{align}
\]</span></p>
<p>donde,
<span class="math display">\[
E\biggl[(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-  \ \underline{\boldsymbol \beta}_{\ (i)})\varepsilon_{\ ok}  \biggr]= \underline{\mathbf{0}} \ \ \ \text{y} \ \ \ \ \  E\biggl[\varepsilon_{\ oi}(\widehat{\underline{\boldsymbol \beta}}_{\ (k)} -  \ \underline{\boldsymbol \beta}_{\ (k)})^t  \biggr] = \underline{\mathbf{0}}^t
\]</span></p>
<p>ya que:
<span class="math display">\[
\begin{align}
\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-\underline{\boldsymbol \beta}_{\ (i)} &amp; =(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t\underline{\mathbf{y}}_i - \underline{\boldsymbol \beta}_{\ (i)}\\
&amp; = (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t ( \mathbf{Z}\ \underline{\boldsymbol \beta}_{\ (i)} + \underline{\boldsymbol \varepsilon}_{\ (i)} ) - \underline{\boldsymbol \beta}_{\ (i)} \\
&amp; = \underline{\boldsymbol \beta}_{\ (i)} + (\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \ \underline{\boldsymbol \varepsilon}_{\ (i)}- \underline{\boldsymbol \beta}_{\ (i)}\\
\widehat{\underline{\boldsymbol \beta}}_{\ (i)}-\underline{\boldsymbol \beta}_{\ (i)}&amp;=(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \ \underline{\boldsymbol \varepsilon}_{\ (i)}
\end{align}
\]</span></p>
<p>es decir,
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}}_{\ (i)}=(\mathbf{Z}^t\mathbf{Z})^{-1}\ \mathbf{Z}^t \ \underline{\boldsymbol \varepsilon}_{\ (i)}+\underline{\boldsymbol \beta}_{\ (i)}
\]</span></p>
<p>y los <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span> son independientes de <span class="math inline">\(\underline{\boldsymbol \varepsilon}_{\ 0}\)</span>. De manera similar se ve que: <span class="math inline">\(E\biggl[\varepsilon_{\ oi}(\widehat{\underline{\boldsymbol \beta}}_{\ (k)} - \ \underline{\boldsymbol \beta}_{\ (k)})^t \biggr] = \underline{\mathbf{0}}^t\)</span>.</p>
<div class="theorem">
<p><span id="thm:teorema-estimadores-mle-mrl-multivariada" class="theorem"><strong>Teorema 5.10  (Teorema Estimadores de Máxima Verosimilitud en el MRL-Multivariado) </strong></span>Sea el MRL-Multivariao dado en <a href="mrl-multiv.html#eq:mrl-multivariado-matricial">(5.39)</a>, definido como:</p>
</div>
<p><span class="math display">\[
\underset{n\times m}{\mathbf{Y}}=\underset{n\times (r+1)}{\mathbf{Z}}\  \underset{(r+1) \times m}{\Large \boldsymbol \beta} + \underset{n\times m}{\Large \boldsymbol \varepsilon}
\]</span></p>
<p>con,<br />
<span class="math display">\[
E\biggl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ \biggr] = \underline{\mathbf{0}}_{\ n} \ \ \ , \ \ \ \ Cov\biggl[\ \underline{\boldsymbol \varepsilon}_{\ (i)} \ , \ \underline{\boldsymbol \varepsilon}_{\ (k)}  \ \biggr]=\sigma_{ik}\ \mathbf{I}_n \ \ , \ \ i,k=1,2,\ldots,m
\]</span></p>
<p>Además, <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\((r+1) \leq n\)</span>, y <span class="math inline">\(n\geq (r+1)+m\)</span> y los errores <span class="math inline">\(\underset{n\times m}{\Large \boldsymbol \varepsilon}\)</span>-con Distribución Normal entonces, El Estimador de Máxima Verosimilitud de <span class="math inline">\(\underline{ \Large{ \boldsymbol \beta}}\)</span> está dado por:
<span class="math display">\[
\widehat{\underset{(r+1) \times m}{\Large \boldsymbol \beta}} = \underset{(r+1)\times (r+1)\ (r+1)\times n \ n\times m}{(\mathbf{Z}^t \mathbf{Z})^{-1}\ \mathbf{Z}^t\ \mathbf{Y} }
\]</span></p>
<p>Además, <span class="math inline">\(\widehat{\Large{\boldsymbol \beta}}\)</span>-tiene distribución Normal con:
<span class="math display">\[
E\biggl[\widehat{{\Large \boldsymbol \beta}}\biggr]={\Large \boldsymbol \beta} \ \ \ \ \text{y} \ \ \ \ \ Cov\biggl[\underline{\widehat{ {\Large \boldsymbol \beta}}}_{\ (i)}\ , \ \underline{\widehat{{\Large \boldsymbol \beta}}}_{\ (k)}\biggr]=\sigma_{ik}\ (\mathbf{Z}^t \mathbf{Z})^{-1}
\]</span></p>
<p>Además, <span class="math inline">\(\widehat{\Large{\boldsymbol \beta}}\)</span>-es independiente del Estimador de Máxima Verosimilitud de la matriz definida positiva <span class="math inline">\(\mathbf{\Sigma}\)</span> dado por:
<span class="math display">\[
\underset{m\times m}{\widehat{\mathbf{\Sigma}}}= \frac{1}{n}\ \underset{m\times n}{\Large \boldsymbol \varepsilon}^t\underset{n\times m}{\Large \boldsymbol \varepsilon}=\frac{1}{n}\ \biggl(\mathbf{Y}-\mathbf{Z}\widehat{\Large{\boldsymbol \beta}}^t\biggr)^t\biggl(\mathbf{Y}-\mathbf{Z}\widehat{\Large{\boldsymbol \beta}}^t\biggr)
\]</span>
y <span class="math inline">\(n\widehat{\mathbf{\Sigma}}\)</span>-tiene una distribución Wishar, <span class="math inline">\(W_{p\ ,\ n-(r+1)}(\mathbf{\Sigma})\)</span>, es decir:
<span class="math display">\[
n\widehat{\mathbf{\Sigma}} \sim W_{p\ ,\ n-(r+1)}(\mathbf{\Sigma}).
\]</span></p>
<p>La Verosimilitud Maximizada esta dada por:
<span class="math display">\[
L\biggr( \widehat{\underline{\boldsymbol \mu}} \ , \ \widehat{\mathbf{\Sigma}}\biggl)= (2\pi)^{-mn/2}|\widehat{\mathbf{\Sigma}}|^{-n/2}e^{-mn/2}.
\]</span></p>
</div>
<div id="prv-mrl-multivariado" class="section level3 hasAnchor" number="5.3.6">
<h3><span class="header-section-number">5.3.6</span> Prueba de Razón de Verosimilitud para Parámetros de Regresión en MRL-Multivariados<a href="mrl-multiv.html#prv-mrl-multivariado" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="prueba-de-subconjunto-de-parámetros" class="section level4 hasAnchor" number="5.3.6.1">
<h4><span class="header-section-number">5.3.6.1</span> Prueba de SubConjunto de Parámetros<a href="mrl-multiv.html#prueba-de-subconjunto-de-parámetros" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El Análogo a la afirmación planteada en <a href="rlm.html#eq:hipotesis-suma-extra-de-cuadrados">(5.17)</a> de que las variables <span class="math inline">\(Z_{q+1},Z_{q+2},\ldots, Z_r\)</span> no influyen en las respuesta <span class="math inline">\(Y_1,Y_2,\ldots,Y_m\)</span> se traducen en la siguiente hipótesis estadística:
<span class="math display" id="eq:hipotesis-suma-extra-de-cuadrados-multivariada">\[
\begin{equation}
\begin{cases}
H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \mathbf{O} \\ \\
H_a \ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} \neq \mathbf{O}
\end{cases}
\end{equation}
\tag{5.45}
\]</span></p>
<p>donde,
<span class="math display">\[
\underset{(r+1)\times m}{\huge{\underline{\boldsymbol \beta}}}=\begin{bmatrix} \underset{(q+1)\times m}{\Large{\boldsymbol \beta}_{\ (1)} }\\ --- \\
\underset{(r-q)\times m}{\Large{\boldsymbol \beta}_{\ (2)} } \end{bmatrix}.
\]</span></p>
<p>Haciendo,
<span class="math display">\[
\underset{n\times (r+1)}{\mathbf{Z} }= \begin{bmatrix} \underset{n\times (q+1)}{\mathbf{Z}_{\ (1)} } &amp; | &amp;
\underset{n\times (r-q)}{\mathbf{Z}_{\ (2)} } \end{bmatrix}
\]</span></p>
<p>el modelo general de RL-multivariado se puede escribir como sigue:
<span class="math display">\[
E[\ \mathbf{Y} \ ]= \mathbf{Z} \ \Large{\boldsymbol \beta}=\begin{bmatrix} \underset{n\times (q+1)}{\mathbf{Z}_{\ (1)} } &amp; | &amp;
\underset{n\times (r-q)}{\mathbf{Z}_{\ (2)} } \end{bmatrix}\begin{bmatrix} \underset{(q+1)\times m}{\Large{\boldsymbol \beta}_{\ (1)} }\\ --- \\
\underset{(r-q)\times m}{\Large{\boldsymbol \beta}_{\ (2)} } \end{bmatrix}\\
E[\ \mathbf{Y} \ ]= \mathbf{Z}_{\ (1)}\ \Large{\boldsymbol \beta}_{\ (1)} + \mathbf{Z}_{\ (2)}\ \Large{\boldsymbol \beta}_{\ (2)}
\]</span></p>
<p>Bajo <span class="math inline">\(H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \mathbf{O}\)</span>-cierto, se tiene que:
<span class="math display">\[
\mathbf{Y}= \mathbf{Z}_{\ (1)}\ \Large{\boldsymbol \beta}_{\ (1)}+ \huge{\boldsymbol \varepsilon}
\]</span></p>
<p>y la <em>Prueba de Razón de Verosimilitud de</em> <span class="math inline">\(H_0:\)</span>-se basa en las cantidades involucradas en <em>La Suma Extra de Cuadrados y Productos Cruzados</em> dada por:
<span class="math display">\[
\begin{align}
SSR\biggl( \Large{\boldsymbol \beta}_{\ (2)} \ \biggl|\ \Large{\boldsymbol \beta}_{\ (1)} \biggr)&amp;=SS_{res}(red)-SS_{res}(full)\\
&amp;=\widehat{\Large{\boldsymbol \varepsilon}}_1^{\ t}\widehat{\Large{\boldsymbol \varepsilon}}_1 - \widehat{\Large{\boldsymbol \varepsilon}}^{\ t}\widehat{\Large{\boldsymbol \varepsilon}} \\
&amp;= \biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr)^t\biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr)-\biggl( \mathbf{Y}-\mathbf{Z}\ \widehat{\Large{\boldsymbol \beta}} \biggr)^t \biggl( \mathbf{Y}-\mathbf{Z}\ \widehat{\Large{\boldsymbol \beta}} \biggr) \\
&amp; = n\biggl( \widehat{\mathbf{\Sigma}}_1 - \widehat{\mathbf{\Sigma}} \biggr)
\end{align}
\]</span></p>
<p>donde,
<span class="math display">\[
\widehat{\Large{\boldsymbol \beta}}_{\ (1)}=(\mathbf{Z}_1^t\mathbf{Z}_1)^{-1}\ \mathbf{Z}_1^t\mathbf{Y} \ \ \ \ \text{y} \ \ \  \ \ \widehat{\mathbf{\Sigma}}_1=\frac{1}{n} \widehat{\Large{\boldsymbol \varepsilon}}_1^{\ t}\widehat{\Large{\boldsymbol \varepsilon}}_1 = \frac{1}{n} \biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr)^t\biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr).
\]</span></p>
<p>Del resultado del teorema <a href="mrl-multiv.html#thm:teorema-estimadores-mle-mrl-multivariada">5.10</a>, se tiene que, la <em>Estadísitica de Razón de Verosimilitud</em> para <span class="math inline">\(H_0:\)</span>se puede expresar en términos de <em>Varianzas Generalziadas</em>, como sigue:
<span class="math display" id="eq:estad-de-razon-verosimilitud-mrl-multivariado">\[
\begin{equation}
\Lambda = \frac{\underset{\large{\boldsymbol \beta}_{\ (1)}\ ,\ \mathbf{\Sigma}}{\max}{L\biggl(\large{\boldsymbol \beta}_{\ (1)}\ ,\ \mathbf{\Sigma}\biggr)}}{\underset{\large{\boldsymbol \beta}\ ,\ \mathbf{\Sigma}}{\max}{L\biggl(\large{\boldsymbol \beta}\ ,\ \mathbf{\Sigma}\biggr)}} =  \frac{L\biggl( \widehat{\large{\boldsymbol \beta}}_{\ (1)}\ ,\ \widehat{\mathbf{\Sigma}}\biggr)}{L\biggl(\widehat{\large{\boldsymbol \beta}}\ ,\ \widehat{\mathbf{\Sigma}}\biggr)}=\frac{(2\pi)^{-mn/2}|\widehat{\mathbf{\Sigma}}_1|^{-n/2}e^{-mn/2}}{(2\pi)^{-mn/2}|\widehat{\mathbf{\Sigma}}|^{-n/2}e^{-mn/2}}=\left(\frac{|\widehat{\mathbf{\Sigma}}|}{|\widehat{\mathbf{\Sigma}}_1|} \right)^{n/2}
\end{equation}
\tag{5.46}
\]</span></p>
<p>equivalentemente, se puede usar la <em>Estadística Lambda de Willks</em> dada por:
<span class="math display">\[
\Lambda^{2/n}=\frac{|\widehat{\mathbf{\Sigma}}|}{|\widehat{\mathbf{\Sigma}}_1|}
\]</span></p>
<div class="theorem">
<p><span id="thm:teorema-distribucion-lambda-wilks-mrl-mul" class="theorem"><strong>Teorema 5.11  (Distribución de la Estadística Lambda de Wilks para Prueba de Sub-Conjuntos de Variables en un MRL-Multivariado) </strong></span>Sea el MRL-Multivariao dado en <a href="mrl-multiv.html#eq:mrl-multivariado-matricial">(5.39)</a>, definido como:</p>
</div>
<p><span class="math display">\[
\underset{n\times m}{\mathbf{Y}}=\underset{n\times (r+1)}{\mathbf{Z}}\  \underset{(r+1) \times m}{\Large \boldsymbol \beta} + \underset{n\times m}{\Large \boldsymbol \varepsilon}
\]</span>
Además, <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\((r+1) \leq n\)</span>, y <span class="math inline">\(n\geq (r+1)+m\)</span> y los errores <span class="math inline">\(\underset{n\times m}{\Large \boldsymbol \varepsilon}\)</span>-con Distribución Normal.</p>
<p>Bajo <span class="math inline">\(H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \mathbf{O}\)</span>-cierto, se tiene que: <span class="math inline">\(n\widehat{\mathbf{\Sigma}}\sim W_{p\ ,\ n-(r+1)}(\mathbf{\Sigma})\)</span>, independientemente de:
<span class="math inline">\(n\biggl( \widehat{\mathbf{\Sigma}}_1 - \widehat{\mathbf{\Sigma}} \biggr)\sim W_{p\ , \ r-q}(\mathbf{\Sigma})\)</span>. La <em>Prueba de Razón de Verosimilitud</em> de <span class="math inline">\(H_0:\)</span>es equivalente a rechazar <span class="math inline">\(H_0\)</span>-para valores grandes de:
<span class="math display">\[
-2\ \text{Ln}\ \Lambda = -n \ \text{Ln} \left(\frac{|\widehat{\mathbf{\Sigma}}|}{|\widehat{\mathbf{\Sigma}}_1|} \right)= -n\ \text{Ln} \left(\frac{|n\widehat{\mathbf{\Sigma}}|}{\biggl|n\widehat{\mathbf{\Sigma}}+n(\widehat{\mathbf{\Sigma}}_1-\widehat{\mathbf{\Sigma}})\biggr|} \right)
\]</span></p>
<p>Para <span class="math inline">\(n\)</span>-grande, <em>La Estadística Modificada</em> dada por:
<span class="math display">\[
\chi^2 = -\biggl[n-(r+1)-\frac{1}{2}(m-(r-1)+q) \biggr] \ \text{Ln} \left(\frac{|\widehat{\mathbf{\Sigma}}|}{|\widehat{\mathbf{\Sigma}}_1|} \right) \approx \chi_{m(r-q)}^2
\]</span></p>
<p>es decir, se distribuye aproximadamente como una chi-cuadrado con <span class="math inline">\(m(r-q)\)</span>-grados de libertad.</p>
<p><strong>Nota:</strong></p>
<p>Los criterios de información también están disponibles para ayudar en la selección de un Modelo de Regresión Múltiple Multivariado adecuado pero simple. Para un modelo que incluye <span class="math inline">\(d\)</span> variables predictoras contando el intercepto, sea
<span class="math display">\[
\widehat{\mathbf{\Sigma}}_d= \frac{1}{n}\biggl(\text{Matriz de Suma Residual de Cuadrados y Productos Cruzados}\biggr)
\]</span>
La versión del Criterio de Información de Akaike en Modelos de Regresión Múltiple Multivariada es:
<span class="math display">\[
AIC=n\ \text{Ln}\biggl(\bigl| \widehat{\mathbf{\Sigma}}_d  \bigr|\biggr)-2p \times d
\]</span>
Este criterio intenta balancear la Varianza Generalziada con el número de parámetros. Los Modelos con valores de <span class="math inline">\(AIC´s\)</span> más pequeños son preferidos.</p>
</div>
<div id="prueba-de-subconjunto-de-parámetros-vía-contrastes" class="section level4 hasAnchor" number="5.3.6.2">
<h4><span class="header-section-number">5.3.6.2</span> Prueba de SubConjunto de Parámetros Vía Contrastes<a href="mrl-multiv.html#prueba-de-subconjunto-de-parámetros-vía-contrastes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>De manera más general, se puede considerar la hipótesis nula de la forma:
<span class="math display">\[
\begin{cases}
H_0\ \ : \ \ \underset{(r-q)\times (r+1)}{\mathbf{C} } \underset{(r+1)\times m}{{\Large \boldsymbol \beta} }= \underset{(r-q)\times m}{\mathbf{\Gamma}_0 }\\ \\
H_a\ \ : \ \ \mathbf{C} {\Large \boldsymbol \beta} \neq \mathbf{\Gamma}_0
\end{cases}
\]</span></p>
<p>donde, <span class="math inline">\(\mathbf{C}\)</span>-es de rango completo dado por <span class="math inline">\(Rango(\mathbf{C})=r-q\)</span>.</p>
<p>Para la Elección de:
<span class="math display">\[
\underset{(r-q)\times (r+1)}{\mathbf{C} }= \begin{bmatrix} \underset{(r-q)\times (q+1)}{\mathbf{O} } &amp; \vdots &amp;  \underset{(r-q)\times (r-q)}{\mathbf{I} } \end{bmatrix} \ \ \ \text{y} \ \ \ \ \underset{(r-q)\times m}{\mathbf{\Gamma}_0 }=\underset{(r-q)\times m}{\mathbf{O} }
\]</span></p>
<p>se tiene que la hipótesis nula <span class="math inline">\(H_0\)</span>-se convierte en:
<span class="math display">\[
H_0 \ \ : \ \ \underset{(r-q)\times (r+1)}{\mathbf{C} } \underset{(r+1)\times m}{{\Large \boldsymbol \beta} }\\
=\begin{bmatrix} \underset{(r-q)\times (q+1)}{\mathbf{O} } &amp; \vdots &amp;  \underset{(r-q)\times (r-q)}{\mathbf{I} } \end{bmatrix}\begin{bmatrix} \underset{(q+1)\times m}{\Large{\boldsymbol \beta}_{\ (1)} }\\ --- \\
\underset{(r-q)\times m}{\Large{\boldsymbol \beta}_{\ (2)} } \end{bmatrix}=\underset{(r-q)\times m}{\Large{\boldsymbol \beta}_{\ (2)} }
\]</span></p>
<p>es decir,
<span class="math display">\[
H_0 \ \ : \ \ \underset{(r-q)\times (r+1)}{\mathbf{C} } \underset{(r+1)\times m}{{\Large \boldsymbol \beta} }
=\underset{(r-q)\times m}{\Large{\boldsymbol \beta}_{\ (2)} }=\mathbf{O}
\]</span></p>
<p>la misma prueba considerada anteriormente.</p>
<p>Se puede demostrar que <em>La Suma Extra de Cuadrados y Productos Cruzados</em> generada por la hipótesis <span class="math inline">\(H_0\)</span> está dada por:
<span class="math display">\[
\begin{align}
SSR\biggl( \Large{\boldsymbol \beta}_{\ (2)} \ \biggl|\ \Large{\boldsymbol \beta}_{\ (1)} \biggr)&amp;=SS_{res}(red)-SS_{res}(full)\\
&amp;=\widehat{\Large{\boldsymbol \varepsilon}}_1^{\ t}\widehat{\Large{\boldsymbol \varepsilon}}_1 - \widehat{\Large{\boldsymbol \varepsilon}}^{\ t}\widehat{\Large{\boldsymbol \varepsilon}} \\
&amp;= \biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr)^t\biggl( \mathbf{Y}-\mathbf{Z}_{\ (1)}\ \widehat{\Large{\boldsymbol \beta}}_{\ (1)} \biggr)-\biggl( \mathbf{Y}-\mathbf{Z}\ \widehat{\Large{\boldsymbol \beta}} \biggr)^t \biggl( \mathbf{Y}-\mathbf{Z}\ \widehat{\Large{\boldsymbol \beta}} \biggr) \\
&amp; = n\biggl( \widehat{\mathbf{\Sigma}}_1 - \widehat{\mathbf{\Sigma}} \biggr)\\
SSR\biggl( \Large{\boldsymbol \beta}_{\ (2)} \ \biggl|\ \Large{\boldsymbol \beta}_{\ (1)} \biggr)&amp;=\biggl( \mathbf{C}\widehat{{\Large \boldsymbol \beta}} - \mathbf{\Gamma}_0\biggr)^t \biggl( \mathbf{C}(\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{C}^t\biggr)^{-1}\biggl(\mathbf{C}\widehat{{\Large \boldsymbol \beta}} - \mathbf{\Gamma}_0\biggr)
\end{align}
\]</span></p>
<p>bajo <span class="math inline">\(H_0\)</span>-cierta, la Estadística <span class="math inline">\(SSR\biggl( \Large{\boldsymbol \beta}_{\ (2)} \ \biggl|\ \Large{\boldsymbol \beta}_{\ (1)} \biggr)=n\biggl( \widehat{\mathbf{\Sigma}}_1 - \widehat{\mathbf{\Sigma}} \biggr)\)</span> se distribuye como una Wishart, <span class="math inline">\(W_{r-q}(\mathbf{\Sigma})\)</span>-independientemente de <span class="math inline">\(\widehat{\mathbf{\Sigma}}\)</span>.</p>
<p>Ésta teoría de distribución se puede emplear para desarrollar una prueba de:
<span class="math display">\[
H_0 \ \ : \ \ \underset{(r-q)\times (r+1)}{\mathbf{C} } \underset{(r+1)\times m}{{\Large \boldsymbol \beta} }
=\underset{(r-q)\times m}{\Large{\boldsymbol \Gamma}_{\ 0} }
\]</span></p>
<p>similar a la prueba llevada a cabo en el resultado <a href="mrl-multiv.html#thm:teorema-distribucion-lambda-wilks-mrl-mul">5.11</a>, ver <span class="citation">(<a href="#ref-khattree2018">Khattree and Naik 2018</a>)</span> para mas detalles.</p>
</div>
</div>
<div id="otras-pruebas-estadísticas-multivariadas" class="section level3 hasAnchor" number="5.3.7">
<h3><span class="header-section-number">5.3.7</span> Otras Pruebas Estadísticas Multivariadas<a href="mrl-multiv.html#otras-pruebas-estadísticas-multivariadas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Además de la prueba de Razón de Verosimilitud se han propuesto otras pruebas para probar:
<span class="math display">\[
H_0 \ \ : \ \ \underset{(r-q)\times m}{{\Large \boldsymbol \beta}_{(2)}  }
=\mathbf{O}
\]</span>
en un Modelo de Regresión Múltiple Multivariado.</p>
<p>Los <em>paquetes de programas de cómputos populares</em> calculan retunariamente 4-pruebas Estadísticas Multivariadas. Para conectarnos con sus salidas, se introduce la siguiente notación alternativa necesaria.</p>
<p>Sea <span class="math inline">\(\mathbf{E}_{p\times p}\)</span>-<em>la Matriz Error</em> o <em>Matriz Residual</em> o <em>Matriz de Cuadrados y Productos Cruzados</em> dada por:
<span class="math display">\[
\underset{p\times p}{\mathbf{E}}= n\widehat{\mathbf{\Sigma}}
\]</span></p>
<p>que Resulta del Ajuste del Modelo Completo o Full.</p>
<p>La <em>Matriz de Cuadrados y Productos Cruzados</em> o <em>Matriz Suma Extra de Cuadrados y Productos Cruzados</em> que resulta de la hipótesis <span class="math inline">\(H_0\)</span> se denota por:
<span class="math display">\[
\underset{p\times p}{\mathbf{H} }= n\biggl(\widehat{\mathbf{\Sigma}}_1 - \widehat{\mathbf{\Sigma}} \biggr)
\]</span></p>
<p>La <em>Estadística de Prueba</em> se puede definir directamente en términos de <span class="math inline">\(\mathbf{E}\)</span> y <span class="math inline">\(\mathbf{H}\)</span> o en términos de los <em>Valores Propios diferentes de cero</em> <span class="math inline">\(\eta_1\geq \eta_2\geq \ldots \geq \eta_s\)</span> de <span class="math inline">\(\mathbf{H}\mathbf{E}^{-1}\)</span>-donde <span class="math inline">\(s=\min(p,r-q)\)</span>.</p>
<p>Equivalentemente, esos valores propios son las raíces de la Ecuación:
<span class="math display">\[
\biggl| \biggl(\widehat{\mathbf{\Sigma}}_1-\widehat{\mathbf{\Sigma}}\biggr)-\eta\widehat{\mathbf{\Sigma}} \biggr|=0
\]</span></p>
<div id="estadísticas-utilizadas" class="section level4 hasAnchor" number="5.3.7.1">
<h4><span class="header-section-number">5.3.7.1</span> Estadísticas Utilizadas<a href="mrl-multiv.html#estadísticas-utilizadas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Lambda de Wilk</strong></li>
</ul>
<p><span class="math display">\[
\text{Lambda de Wilk}=\prod_{i=1}^s \left(\frac{1}{1+\eta_i} \right)=\frac{\bigl|\mathbf{E} \bigr|}{\bigl|\mathbf{E} +\mathbf{H}\bigr|}
\]</span></p>
<ul>
<li><strong>Traza de Pillai</strong></li>
</ul>
<p><span class="math display">\[
\text{Traza de Pillai}=\sum_{i=1}^s \left(\frac{\eta_i}{1+\eta_i} \right)=tr\biggl(\mathbf{H}\bigl(\mathbf{H} +\mathbf{E}\bigr)^{-1} \biggr)
\]</span></p>
<ul>
<li><strong>Traza de Hotelling-Lawley</strong></li>
</ul>
<p><span class="math display">\[
\text{Traza de Hotelling-Lawley}=\sum_{i=1}^s \eta_i =tr\bigl(\mathbf{H}\mathbf{E}^{-1} \bigr)
\]</span></p>
<ul>
<li><strong>La Raíz Más Grande de Roy</strong></li>
</ul>
<p><span class="math display">\[
\text{La Raíz Más Grande de Roy}=\frac{\eta_1}{1+\eta_1}
\]</span></p>
<p><strong>Observaciones:</strong></p>
<p>La prueba de <em>Roy</em> selecciona el vector de coeficientes <span class="math inline">\(\underline{\mathbf{a}}\)</span> de modo que el <em>Estadístico <span class="math inline">\(F\)</span>-Univariado</em> basado en una combinación <span class="math inline">\(\underline{\mathbf{a}}^t \underline{\mathbf{y}}_{\ (j)}\)</span> tiene el máximo valor posible. Cuando varios de los valores propios <span class="math inline">\(\eta_i\)</span>, son moderadamente grandes, la prueba de <em>Roy</em> tendrá un pobre desempeño con relación a las otras tres pruebas. Estudios de simulación sugieren que su potencia será mejor cuando solo haya un valor propio grande. Existen gráficos y tablas de valores propios disponibles para la prueba de <em>Roy</em>, ver <span class="citation">(<a href="#ref-pillai1967">Pillai 1967</a>)</span> y <span class="citation">(<a href="#ref-heck1960">Heck 1960</a>)</span>.</p>
<p>Las pruebas <em>Lambda de Willk, Raíz más Grande de Roy y Traza de Hotelling-Lawley</em> son <em>cercanamente equivalentes</em> para tamaños de muestras grandes.</p>
<p>Si existe una gran discrepancia entre los <span class="math inline">\(p\)</span>-valores informados para las cuatro pruebas, los valores propios y los vectores propios pueden dar lugar a una interpretación. En este libro, se reporta la Prueba Lambda de Wilks, que es la prueba de Razón de Verosimilitud.</p>
</div>
</div>
<div id="predicciones-a-partir-de-un-modelo-de-regresión-múltiple-multivariado" class="section level3 hasAnchor" number="5.3.8">
<h3><span class="header-section-number">5.3.8</span> Predicciones A Partir de un Modelo de Regresión Múltiple Multivariado<a href="mrl-multiv.html#predicciones-a-partir-de-un-modelo-de-regresión-múltiple-multivariado" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suponga que el Modelo
<span class="math display">\[
\underset{n\times m}{\mathbf{Y}}=\underset{n\times (r+1)}{\mathbf{Z}}\  \underset{(r+1) \times m}{\Large \boldsymbol \beta} + \underset{n\times m}{\Large \boldsymbol \varepsilon}
\]</span></p>
<p>con errores <span class="math inline">\({\huge \boldsymbol \varepsilon}\)</span>-normales, ha sido ajustado y verificado por algún tipo de insuficiencia. Si el Modelo es Adecuado entonces se puede emplear para
propósitos predictivos.</p>
<div id="predicción-de-la-respuesta-media-en-un-underlinemathbfz_0" class="section level4 hasAnchor" number="5.3.8.1">
<h4><span class="header-section-number">5.3.8.1</span> Predicción de la Respuesta Media en un <span class="math inline">\(\underline{\mathbf{z}}_0\)</span><a href="mrl-multiv.html#predicción-de-la-respuesta-media-en-un-underlinemathbfz_0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Un problema es predecir las respuestas medias correspondientes a valores fijos dados en un vector <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span> de las variables predictoras. Las inferencias acerca de las respuestas medias se pueden hacer usando la teoría de la distribución del resultado <a href="mrl-multiv.html#thm:teorema-estimadores-mle-mrl-multivariada">5.10</a>. A partir de este resultado, se tiene que:
<span class="math display">\[
\underset{m\times (r+1)\ (r+1)\times 1}{\widehat{{\Large \boldsymbol \beta}}^t  \underline{\mathbf{z}}_{\ 0} } \sim N_m \biggl( {\Large \boldsymbol \beta}^t  \underline{\mathbf{z}}_{\ 0} \ , \ \underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}\ \mathbf{\Sigma}  \biggr)
\]</span>
y
<span class="math display">\[
n\widehat{\mathbf{\Sigma}}\ \ \  \text{Es independientemente} \ \ \sim W_{n-(r+1)}(\mathbf{\Sigma}).
\]</span></p>
<p>El valor desconocido de la función de Regresión en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span> esta dado por: <span class="math inline">\({\Large \boldsymbol \beta}^t \underline{\mathbf{z}}_{\ 0}\)</span>.
A partir de la discusión sobre la Estadística <span class="math inline">\(T 2\)</span> dada en la sección <a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#distribucion-t2">4.3.1.1</a>, se tiene que:
<span class="math display">\[
T^2= \left( \frac{  \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0}  - {\Large \boldsymbol \beta}^{\ t}  \underline{\mathbf{z}}_{\ 0} }{\sqrt{\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}} } \right)^t \ \left(\frac{n}{n-(r+1)}\ \widehat{\mathbf{\Sigma}} \right)^{-1} \ \left( \frac{  \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0}  - {\Large \boldsymbol \beta}^{\ t}  \underline{\mathbf{z}}_{\ 0} }{\sqrt{\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}} } \right)
\]</span></p>
<p>y una Elipse de Confianza del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\({\Large \boldsymbol \beta}^{\ t} \underline{\mathbf{z}}_{\ 0}\)</span> está dada por la desigualdad:
<span class="math display">\[
\left(   {\Large \boldsymbol \beta}^{\ t}  \underline{\mathbf{z}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0} \right)^t \ \left(\frac{n}{n-(r+1)}\ \widehat{\mathbf{\Sigma}} \right)^{-1} \ \left({\Large \boldsymbol \beta}^{\ t}  \underline{\mathbf{z}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0} \right) \\
\leq \underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0} \left[ \frac{m\biggl((n-(r+1)\biggr)}{n-(r+m)} \right]\ F_{m\ ; \ n-(r+m)}(1-\alpha)
\]</span></p>
<p>donde, <span class="math inline">\(F_{m\ ; \ n-(r+m)}(1-\alpha)\)</span>-es el percentil superior <span class="math inline">\((1-\alpha)100\%\)</span> de la distribución <span class="math inline">\(F\)</span>-Snedecor con <span class="math inline">\(m\)</span> y y <span class="math inline">\(n-(r+m)\)</span>-grados de libertad respectivamente.</p>
<p>Los Intervalos de Confianza Simultáneos del <span class="math inline">\((1-\alpha)100\%\)</span>-para <span class="math inline">\(E[Y_i]=\underline{\mathbf{z}}_{\ 0}^t\ \underline{\boldsymbol \beta}_{\ (i)}\)</span> están dados por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}
\ \ \pm \ \ \sqrt{\left[ \frac{m\biggl((n-(r+1)\biggr)}{n-(r+m)} \right]\ F_{m\ ; \ n-(r+m)}(1-\alpha)}\ \sqrt{\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0} \left( \frac{n}{n-(r+1)}\ \hat{\sigma_{ii}} \right)}
\]</span></p>
<p><span class="math inline">\(i=1,2,\ldots,m\)</span>, donde, <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span>-es la <span class="math inline">\(i\)</span>-ésima columna de <span class="math inline">\({\huge \widehat{\boldsymbol \beta}}\)</span> y <span class="math inline">\(\hat{\sigma}_{ii}\)</span>-es el <span class="math inline">\(í\)</span>-ésimo elemento de la diagonal de <span class="math inline">\(\widehat{\mathbf{\Sigma}}\)</span>.</p>
</div>
<div id="predicción-de-nuevos-valores-de-las-variables-respuestas-de-mathbfy-en-un-underlinemathbfz_0" class="section level4 hasAnchor" number="5.3.8.2">
<h4><span class="header-section-number">5.3.8.2</span> Predicción de nuevos valores de las variables Respuestas de <span class="math inline">\(\mathbf{Y}\)</span> en un <span class="math inline">\(\underline{\mathbf{z}}_0\)</span><a href="mrl-multiv.html#predicción-de-nuevos-valores-de-las-variables-respuestas-de-mathbfy-en-un-underlinemathbfz_0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Se desean pronosticar los valores de las variables respuestas <span class="math inline">\(Y_1,Y_2,\ldots,Y_m\)</span> en un vector de valores de las varaibles regresoras dado por <span class="math inline">\(\underline{\mathbf{z}}_0\)</span>, es decir se desea predecir a:
<span class="math display">\[
\underset{m\times 1}{\underline{\mathbf{y}}_{\ 0}}=\underset{m\times (r+1)\ (r+1)\times 1}{{\Large \boldsymbol \beta}^{\ t}\  \underline{\mathbf{z}}_{\ 0}}\ + \ \ \ \underset{m\times 1}{\underline{ \boldsymbol \varepsilon}_{\ 0} }
\]</span></p>
<p>En este caso, <span class="math inline">\(\underline{ \boldsymbol \varepsilon}_{\ 0}\)</span>-es independiente de <span class="math inline">\(\huge{\boldsymbol \varepsilon}\)</span>.</p>
<p>Ahora se tiene que:
<span class="math display">\[
\underline{\mathbf{y}}_{\ 0}- {\Large \widehat{\boldsymbol \beta}}^{\ t}\  \underline{\mathbf{z}}_{\ 0} = {\Large \boldsymbol \beta}^{\ t}\  \underline{\mathbf{z}}_{\ 0}\ + \ \ \ \underline{ \boldsymbol \varepsilon}_{\ 0} \ \  - \ \  {\Large \widehat{\boldsymbol \beta}}^{\ t}\  \underline{\mathbf{z}}_{\ 0} = \left( {\Large \boldsymbol \beta}^{\ t} - {\Large \widehat{\boldsymbol \beta}} \right)^t \  \underline{\mathbf{z}}_{\ 0} \ \ +\ \   \underline{ \boldsymbol \varepsilon}_{\ 0} \\
\sim N_m \biggl( \ \underline{\mathbf{0}} \ , \ (1+\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0})\ \mathbf{\Sigma}  \biggr)
\]</span></p>
<p>independientemente de: <span class="math inline">\(n \widehat{\mathbf{\Sigma}}\)</span>, de donde, una Elipse de Predicción del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\(\underline{\mathbf{y}}_{\ 0}\)</span> esta dada por:
<span class="math display">\[
\left(   \underline{\mathbf{y}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0} \right)^t \ \left(\frac{n}{n-(r+1)}\ \widehat{\mathbf{\Sigma}} \right)^{-1} \ \left(\underline{\mathbf{y}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}^{\ t}  \underline{\mathbf{z}}_{\ 0} \right) \\
\leq \biggl(1+\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}\biggr) \left[ \frac{m\biggl((n-(r+1)\biggr)}{n-(r+m)} \right]\ F_{m\ ; \ n-(r+m)}(1-\alpha)
\]</span></p>
<p>y los Intervalos de Predicción Simultáneos del <span class="math inline">\((1-\alpha)100\%\)</span>-para las respuestas individuales <span class="math inline">\(\underline{\mathbf{y}}_{\ 0i}\)</span>, están dados por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}_{\ (i)}
\ \ \pm \ \ \sqrt{\left[ \frac{m\biggl((n-(r+1)\biggr)}{n-(r+m)} \right]\ F_{m\ ; \ n-(r+m)}(1-\alpha)}\ \sqrt{ \biggl( 1 + \underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0} \biggr) \left( \frac{n}{n-(r+1)}\ \hat{\sigma_{ii}} \right)}
\]</span></p>
<p><span class="math inline">\(i=1,2,\ldots,m\)</span>, donde, <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (i)}\)</span>-es la <span class="math inline">\(i\)</span>-ésima columna de <span class="math inline">\({\huge \widehat{\boldsymbol \beta}}\)</span> y <span class="math inline">\(\hat{\sigma}_{ii}\)</span>-es el <span class="math inline">\(í\)</span>-ésimo elemento de la diagonal de <span class="math inline">\(\widehat{\mathbf{\Sigma}}\)</span>.</p>
</div>
</div>
<div id="el-concepto-de-regresión-lineal" class="section level3 hasAnchor" number="5.3.9">
<h3><span class="header-section-number">5.3.9</span> El Concepto de Regresión Lineal<a href="mrl-multiv.html#el-concepto-de-regresión-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El <em>Modelo de Regresión Lineal Clásico</em> se ocupa de la asociación entre una
única variable dependiente o respuesta <span class="math inline">\(Y\)</span> y una colección de variables predictoras <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span>. El modelo de regresión considerado trata a la respuesta <span class="math inline">\(Y\)</span> como una variable aleatoria cuya media depende de los valores fijos de las variables regresoras <span class="math inline">\(Z´s\)</span>.</p>
<p>Se supone que dicha media es una función lineal de los coeficientes de regresión <span class="math display">\[
\beta_0,\beta_1,\beta_2,\ldots,\beta_r
\]</span></p>
<p>Se considera la variable respuesta dada por <span class="math inline">\(Y\)</span> y el vector de variables regresoras dado por:
<span class="math display">\[
\underline{\mathbf{z}}=\begin{bmatrix}Z_1 \\ Z_2 \\ \vdots\\ Z_r \end{bmatrix}_{r\times 1}
\]</span></p>
<div id="modelo-de-regresión-lineal-cuyas-variables-siguen-una-cierta-distribución-conjunta-una-sola-variable-respuesta-y" class="section level4 hasAnchor" number="5.3.9.1">
<h4><span class="header-section-number">5.3.9.1</span> Modelo de Regresión Lineal Cuyas Variables siguen una Cierta Distribución Conjunta (Una Sola Variable Respuesta <span class="math inline">\(Y\)</span>)<a href="mrl-multiv.html#modelo-de-regresión-lineal-cuyas-variables-siguen-una-cierta-distribución-conjunta-una-sola-variable-respuesta-y" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El modelo de regresión lineal también surge en un escenario diferente. Suponga que todas las variables involucradas <span class="math inline">\(Y,Z_1,Z_2,\ldots,Z_r\)</span>-son <strong>variables aleatorias</strong> y que tienen una <strong>distribución de probabilidad conjunta</strong>, <strong>no-necesariamente normal</strong>, con vector de medias y matriz de varianzas-covarianzas dados por:
<span class="math display">\[
\underset{(r+1) \times 1}{\underline{ {\Large \boldsymbol \mu } }}  \ \ \ \ \ \text{y} \ \ \ \ \ \underset{(r+1)\times (r+1)}{\mathbf{\Sigma}}
\]</span></p>
<p>Particionando a <span class="math inline">\(\underline{ {\Large \boldsymbol \mu } }\)</span> y a <span class="math inline">\(\mathbf{\Sigma}\)</span> de manera obvia se obtiene que:
<span class="math display">\[
\underset{(r+1) \times 1}{\underline{ {\Large \boldsymbol \mu } }}= \begin{bmatrix}  \underset{1\times 1}{ \mu_{Y} } \\ --- \\ \underset{r \times 1}{ \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} }  \end{bmatrix} \ \ \ \ \ \text{y} \ \ \ \ \ \underset{(r+1)\times (r+1)}{\mathbf{\Sigma}}= \begin{bmatrix} \underset{1\times 1}{\sigma_{YY}} &amp; \vdots &amp; \underset{1\times r}{\underline{ \boldsymbol \sigma}^t_{\ \underline{\mathbf{z}}Y}} \\ --- &amp; &amp; --- \\
\underset{r\times 1}{\underline{ \boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}} &amp; \vdots &amp; \underset{r\times r}{\mathbf{\Sigma}_{\underline{\mathbf{z}}}}   \end{bmatrix}
\]</span>
con,
<span class="math display">\[
\underset{r \times 1}{ \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} }=E[\underline{\mathbf{z}}]=\begin{bmatrix} E[Z_1]  \\ E[Z_2] \\ \vdots \\ E[Z_r] \end{bmatrix}_{r \times 1}\ \ \ \ \ \ ; \ \ \ \ \ \underline{ \boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y} =Cov(\ \underline{\mathbf{z}} , Y\ )= \begin{bmatrix} \sigma_{YZ_1}  \\ \sigma_{YZ_2} \\ \vdots \\ \sigma_{YZ_r} \end{bmatrix}_{r \times 1}=\begin{bmatrix} Cov(Y,Z_1)  \\ Cov(Y,Z_2) \\ \vdots \\ Cov(Y,Z_r) \end{bmatrix}_{r \times 1}
\]</span></p>
<p>y <span class="math inline">\(\mathbf{\Sigma}_{\underline{\mathbf{z}}}\)</span>-se puede tomar de Rango Completo igual a <span class="math inline">\(r\)</span>.</p>
</div>
<div id="mejor-predictor-lineal-de-un-modelo-de-regresión-lineal-cuyas-variables-siguen-una-cierta-distribución-conjunta" class="section level4 hasAnchor" number="5.3.9.2">
<h4><span class="header-section-number">5.3.9.2</span> Mejor Predictor Lineal de un Modelo de Regresión Lineal Cuyas Variables siguen una Cierta Distribución Conjunta<a href="mrl-multiv.html#mejor-predictor-lineal-de-un-modelo-de-regresión-lineal-cuyas-variables-siguen-una-cierta-distribución-conjunta" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Se considera el problema de predecir la variable respuesta <span class="math inline">\(Y\)</span> usando <strong>El Predictor Lineal</strong> dado por:
<span class="math display" id="eq:predictor-lineal">\[
\begin{equation}
\text{Predictor Lineal}=b_0+b_1Z_1+b_2Z_2+\cdots+b_rZ_r=b_0+\underline{\mathbf{b}}^t\underline{\mathbf{z}}
\end{equation}
\tag{5.47}
\]</span></p>
<p>Para un predictor lineal de la forma dada en <a href="mrl-multiv.html#eq:predictor-lineal">(5.47)</a>, se tiene que el <strong>Error en la Predicción de</strong> <span class="math inline">\(Y\)</span> está dado por:
<span class="math display" id="eq:error-del-predictor-lineal">\[
\begin{equation}
\text{Error de Predicción  Lineal}=Y-b_0-b_1Z_1-b_2Z_2-\cdots-b_rZ_r=Y-b_0-\underline{\mathbf{b}}^t\underline{\mathbf{z}}
\end{equation}
\tag{5.48}
\]</span></p>
<p>Debido a que éste error es <strong>Aleatorio</strong>, se acostumbran a seleccionar a <span class="math inline">\(b_0\)</span> y a <span class="math inline">\(\underline{\mathbf{b}}\)</span> de tal forma que minimicen <strong>El Error Cuadrático Medio</strong> dado por:
<span class="math display" id="eq:error-cuadratico-medio-lineal">\[
\begin{equation}
\text{Error Cuadrático Medio}=E\biggl(Y-b_0-\underline{\mathbf{b}}^t\underline{\mathbf{z}} \biggr)^2
\end{equation}
\tag{5.49}
\]</span></p>
<p>Ahora, Este Error Cuadrático Medio depende de la distribución conjunta de <span class="math inline">\(Y\)</span> y <span class="math inline">\(\underline{\mathbf{z}}\)</span> solamente a través de los parámetros <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span>. Es posible expresar El Predictor Lineal “óptimo” en términos de estas últimas dos cantidades, como se verá en el siguiente resultado.</p>
<div class="theorem">
<p><span id="thm:teorema-mejor-predictor-lineal-en-mrlm" class="theorem"><strong>Teorema 5.12  (Mejor Predictor Linea en un MRLM) </strong></span>El predictor lineal para <span class="math inline">\(b_0+\underline{\mathbf{b}}^t\underline{\mathbf{z}}\)</span></p>
</div>
<p>dado por:
<span class="math display">\[
\text{Predictor Lineal}=\beta_0+\underline{\boldsymbol \beta }^t\underline{\mathbf{z}}
\]</span></p>
<p>donde los coeficientes están dados por:
<span class="math display">\[
\underset{r\times 1}{\underline{\boldsymbol \beta } } = \underset{r\times r \ \ r \times 1}{\mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\ \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y} } = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_r \end{bmatrix}_{r\times 1} \ \ \ \ \ \ \text{y} \ \ \ \ \ \ \beta_0=\mu_Y - \underline{\boldsymbol \beta }^t \  \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}
\]</span></p>
<p>tiene <strong>Mínimo Error Cuadrático Medio</strong> entre todos los <strong>Predictores Lineales</strong> de la respuesta <span class="math inline">\(Y\)</span>.</p>
<p>Ahora, El Error Cuadrático Medio es:
<span class="math display">\[
\text{Error Cuadrático Medio}=E\biggl(Y-\beta_0-\underline{\boldsymbol \beta}^t\underline{\mathbf{z}} \biggr)^2\\
=E\biggl(Y-(\mu_Y-\underline{\boldsymbol \beta }^t \  \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}) -\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\ \underline{\mathbf{z}} \biggr)^2 \\
=E\biggl[Y-\mu_Y- \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\underline{\ \mathbf{z}}}) \biggr]^2\\
\text{Error Cuadrático Medio}=\sigma_{YY}-\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}
\]</span></p>
<p>Además,
<span class="math display">\[
\beta_0+\underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}} = \mu_Y +  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})
\]</span></p>
<p>es el predictor lineal que tiene <strong>Máxima Correlación</strong> con <span class="math inline">\(Y\)</span>, es decir:
<span class="math display">\[
Corr\biggl( Y \ ,\ \beta_0+\underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}}  \biggr) = \underset{b_0\ , \ \underline{\mathbf{b}}}{Máx} \ \  Corr\biggl( Y \ ,\ b_0+\underline{\mathbf b}^t\ \underline{\mathbf{z}}  \biggr) \\
= \sqrt{\frac{\underline{\boldsymbol \beta}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}  \  \underline{\boldsymbol \beta}}{\sigma_{YY}}} \\
= \sqrt{\frac{\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}^{-1}  \  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}}{\sigma_{YY}}}
\]</span></p>
<div id="coeficiente-de-corelación-múltiple-poblacional" class="section level5 hasAnchor" number="5.3.9.2.1">
<h5><span class="header-section-number">5.3.9.2.1</span> Coeficiente de Corelación Múltiple Poblacional<a href="mrl-multiv.html#coeficiente-de-corelación-múltiple-poblacional" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A la correlación entre <span class="math inline">\(Y\)</span> y su Mejor Predictor Lineal se le llama <em>Coeficiente de Correlación Múltiple Poblacional</em>, y se denota por <span class="math inline">\(\rho_{Y(\underline{\mathbf{z}})}\)</span>, es decir:
<span class="math display">\[
\rho_{Y(\underline{\mathbf{z}})}= + \sqrt{\frac{\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}^{-1}  \  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}}{\sigma_{YY}}}
\]</span></p>
<p>Note que a diferencia de otros coeficientes de correlación, el coeficiente de correlación múltiple poblacional, es una raíz cuadrada positiva, es decir que: <span class="math inline">\(0 \leq \rho_{Y(\underline{\mathbf{z}})} \leq 1\)</span>.</p>
</div>
<div id="coeficiente-de-determinación-poblacional" class="section level5 hasAnchor" number="5.3.9.2.2">
<h5><span class="header-section-number">5.3.9.2.2</span> Coeficiente de Determinación Poblacional<a href="mrl-multiv.html#coeficiente-de-determinación-poblacional" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Al cuadrado del Coeficiente de Correlación Múltiple Poblacional se le llama <em>Coeficiente de Determinación Poblacional</em> y se denota por <span class="math inline">\(\rho_{Y(\underline{\mathbf{z}})}^2\)</span>, es decir:
<span class="math display">\[
\rho_{Y(\underline{\mathbf{z}})}^2 = \frac{\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}^{-1}  \  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}}{\sigma_{YY}}
\]</span></p>
<p>El Coeficiente de Determinación Poblacional, tiene una improtante interpretación, a saber.</p>
<p>El Error Cuadrático Medio al usar el predictor lineal dado por <span class="math inline">\(\beta_0+\underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}}\)</span> para pronosticar el valor de <span class="math inline">\(Y\)</span> es:
<span class="math display">\[
\text{El Error Cuadrático Medio}=\sigma_{YY} - \sigma_{YY}\biggl( \frac{  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y} }{\sigma_{YY}} \biggr) = \sigma_{YY}\biggl(1-\rho_{Y(\underline{\mathbf{z}})}^2 \biggr)
\]</span></p>
<p>de donde, si <span class="math inline">\(\rho_{Y(\underline{\mathbf{z}})}^2=0\)</span> entonces, no hay potencia predictiva de <span class="math inline">\(\underline{\mathbf{z}}\)</span> sobre <span class="math inline">\(Y\)</span>. En el otro extremo, si <span class="math inline">\(\rho_{Y(\underline{\mathbf{z}})}^2=1\)</span> indica que <span class="math inline">\(Y\)</span> se puede predecir a partir de <span class="math inline">\(\underline{\mathbf{z}}\)</span> sin error.</p>
<div class="example">
<p><span id="exm:ejemplo1-mrlm-mejor-predictor-lineal" class="example"><strong>Ejemplo 5.9  (Mejor Predictor Lineal en un MRLM) </strong></span>Determinar el Mejor Predictor Lineal, el Error Cuadrático Medio, El Coeficiente de Correlación Múltiple y El Coeficiente de Determinación Múltiple de un MRLM de una variable respuesta <span class="math inline">\(Y\)</span> versus dos variables regresoras <span class="math inline">\(Z_1,Z_2\)</span>.</p>
</div>
<p>Dado el vector de medias y la matriz de varianzas-covarianzas de la distribución conjunta entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(\underline{\mathbf{z}}=[Z_1,Z_2]^t\)</span> iguales a:
<span class="math display">\[
\underset{(r+1) \times 1}{\underline{ {\Large \boldsymbol \mu } }}= \begin{bmatrix}  \underset{1\times 1}{ \mu_{Y} } \\ --- \\ \underset{2 \times 1}{ \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} }  \end{bmatrix}=\begin{bmatrix}  5 \\ --- \\ 2 \\ 0  \end{bmatrix}_{3\times 1}
\]</span></p>
<p><span class="math display">\[
\underset{(r+1)\times (r+1)}{\mathbf{\Sigma}}= \begin{bmatrix} \underset{1\times 1}{\sigma_{YY}} &amp; \vdots &amp; \underset{1\times 2}{\underline{ \boldsymbol \sigma}^t_{\ \underline{\mathbf{z}}Y}} \\ --- &amp; &amp; --- \\
\underset{2\times 1}{\underline{ \boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}} &amp; \vdots &amp; \underset{2\times 2}{\mathbf{\Sigma}_{\underline{\mathbf{z}}}}   \end{bmatrix}=
\begin{bmatrix} 10 &amp; \vdots &amp; 1 &amp; -1 \\
--- &amp; &amp;  -- &amp; -- &amp; \\
1  &amp; \vdots &amp; 7 &amp; 3 \\
-1 &amp; \vdots &amp; 3 &amp; 2\end{bmatrix}_{3\times 3}
\]</span></p>
<p><strong>Solución:</strong></p>
<p>Los Coeficientes del Mejor Predictor lineal son:
<span class="math display">\[
\underset{2\times 1}{\underline{\boldsymbol \beta } } = \underset{2\times 2 \ \ 2 \times 1}{\mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\ \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y} } = \begin{bmatrix} 7 &amp; 3  \\ 3 &amp; 2 \end{bmatrix}^{-1}\begin{bmatrix}1 \\ -1 \end{bmatrix}\\ =  
\begin{bmatrix} 0.4 &amp; -0.6  \\ -0.6 &amp; 1.4 \end{bmatrix}\begin{bmatrix}1 \\ -1 \end{bmatrix}\\
\underset{2\times 1}{\underline{\boldsymbol \beta } } =\begin{bmatrix} \beta_1 \\ \beta_2 \end{bmatrix}=\begin{bmatrix} 1 \\ -2 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\beta_0=\mu_Y - \underline{\boldsymbol \beta }^t \  \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} = 5 - \begin{bmatrix} 1 &amp; -2 \end{bmatrix}\begin{bmatrix} 2 \\ 0 \end{bmatrix}=5-2=3
\]</span></p>
<p>Luego el Mejor Predictor Lineal es:
<span class="math display">\[
\beta_0+\underline{\boldsymbol \beta }^t\ \underline{\mathbf{z}}  = \beta_0 + \beta_1Z_1+\beta_2Z_2=2+Z_1-2\ Z2
\]</span></p>
<p>El Error Cuadrático Medio de dicho Estimador es:
<span class="math display">\[
\text{El Error Cuadrático Medio}=\sigma_{YY} -  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y} \\
= 10 - \begin{bmatrix}1 &amp; -1 \end{bmatrix} \begin{bmatrix} 0.4 &amp; -0.6  \\ -0.6 &amp; 1.4 \end{bmatrix}\begin{bmatrix}1 \\ -1 \end{bmatrix}= 10 - 3 = 7
\]</span></p>
<p>El Coeficiente de Correlación Múltiple Poblacional es:
<span class="math display">\[
\rho_{Y(\underline{\mathbf{z}})}= + \sqrt{\frac{\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}^{-1}  \  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}}{\sigma_{YY}}}= \sqrt{\frac{3}{10}}=0.548
\]</span></p>
<p>El Coeficiente de Determinación Poblacional es:
<span class="math display">\[
\rho_{Y(\underline{\mathbf{z}})}^2= \frac{\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t \ \mathbf{\Sigma}_{\underline{\mathbf{z}}}^{-1}  \  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}}{\sigma_{YY}} = 0.548^2 =  0.3003
\]</span></p>
<p>Notar que:
<span class="math display">\[
\text{El Error Cuadrático Medio}= \sigma_{YY}\biggl(1-\rho_{Y(\underline{\mathbf{z}})}^2 \biggr)=10(1-0.3003)=6.997=7
\]</span></p>
<p>Se puede demostrar que:
<span class="math display">\[
1-\rho_{Y(\underline{\mathbf{z}})}^2=\frac{1}{\rho^{YY}}
\]</span></p>
<p>donde <span class="math inline">\(\frac{1}{\rho^{YY}}\)</span>-Es el elemento de la esquina superior derecha de la Inversa de <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<p>En este ejemplo:
<span class="math display">\[
\mathbf{\Sigma}_{3\times 3}=\begin{bmatrix} 10 &amp; \vdots &amp; 1 &amp; -1 \\
--- &amp; &amp;  -- &amp; -- &amp; \\
1  &amp; \vdots &amp; 7 &amp; 3 \\
-1 &amp; \vdots &amp; 3 &amp; 2\end{bmatrix} \ \ \ ; \ \ \ \text{de donde:} \ \ \ \mathbf{\Sigma}^{-1}= \begin{bmatrix} 0.1429 &amp;  -0.1429 &amp; 0.2857 \\
  &amp; 0.5429 &amp; -0.8857 \\
&amp;  &amp; 1.9714 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
1-\rho_{Y(\underline{\mathbf{z}})}^2=\frac{1}{\rho^{YY}}=\frac{1}{0.1429}=7 = 7
\]</span></p>
</div>
</div>
<div id="mejor-predictor-lineal-de-un-modelo-de-regresión-lineal-cuyas-variables-siguen-una-distribución-conjunta-normal-mrl-normal" class="section level4 hasAnchor" number="5.3.9.3">
<h4><span class="header-section-number">5.3.9.3</span> Mejor Predictor Lineal de un Modelo de Regresión Lineal Cuyas Variables siguen una Distribución Conjunta Normal (MRL-Normal)<a href="mrl-multiv.html#mejor-predictor-lineal-de-un-modelo-de-regresión-lineal-cuyas-variables-siguen-una-distribución-conjunta-normal-mrl-normal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La restricción a obtener un Predictor Lineal está estrechamente relacionada con la Suposición de Normalidad. Específicamente si se tiene que:
<span class="math display">\[
\begin{bmatrix} Y \\ --- \\ \underline{\mathbf{z}} \end{bmatrix}_{(r+1)\times 1}=\begin{bmatrix} Y \\ --- \\Z_1 \\ Z_2 \\ \vdots \\ Z_r \end{bmatrix} \sim N_{r+1} \biggl( \ \underline{\boldsymbol\mu} \ , \ \mathbf{\Sigma} \biggr)
\]</span></p>
<p>entonces, la Distribución Condicional de <span class="math inline">\(Y\)</span> dado <span class="math inline">\(Z_1=z_1,Z_2=z_2,\cdots,Z_r=z_r\)</span> está dada por:
<span class="math display">\[
Y\ \biggl |\ (z_1,z_2,\ldots,z_r) \ \sim N \biggl( \  \mu_Y +  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}) \ , \ \sigma_{YY}-\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}   \  \biggr)
\]</span></p>
<p>Es decir, La Media de esta Distribución es el Predictor Lineal del resultado <a href="mrl-multiv.html#thm:teorema-mejor-predictor-lineal-en-mrlm">5.12</a>, es decir:
<span class="math display" id="eq:esperanza-condicional">\[
\begin{equation}
E\biggl[Y\ \biggl |\ (z_1,z_2,\ldots,z_r) \biggr] = \mu_Y +  \underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}Y}^t\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})  \\
= \beta_0 + \underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}}
\end{equation}
\tag{5.50}
\]</span></p>
<p>y se concluye que: <span class="math inline">\(E\biggl[Y\ \biggl |\ z_1,z_2,\ldots,z_r \biggr]\)</span>-es el Mejor Predictor Lineal de <span class="math inline">\(Y\)</span> cuando la población es: <span class="math inline">\(N_{r+1} \biggl( \ \underline{\boldsymbol\mu} \ , \ \mathbf{\Sigma} \biggr)\)</span>.</p>
<p>A la Esperanza Condicional de <span class="math inline">\(Y\)</span> dada en <a href="mrl-multiv.html#eq:esperanza-condicional">(5.50)</a>
), se le llama <strong>Función de Regresión</strong> y para poblaciones normales esta función es lineal.</p>
<p>Cuando la población no es normal, la función de regresión <span class="math inline">\(E\biggl[Y\ \biggl |\ z_1,z_2,\ldots,z_r \biggr]\)</span> no Necesariamente es de la forma <span class="math inline">\(\beta_0 + \underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}}\)</span>. Sin embargo se puede demostrar, ver <span class="citation">(<a href="#ref-rao1973">Rao et al. 1973</a>)</span>, que <span class="math inline">\(E\biggl[Y\ \biggl |\ z_1,z_2,\ldots,z_r \biggr]\)</span>, independientemente de su forma, predice a <span class="math inline">\(Y\)</span> con el menor Error Cuadrático Medio. Afortunadamente, esta optimización más amplia entre todos los estimadores, la posee el predictor lineal cuando la población es normal.</p>
<div id="estimadores-de-máxima-verosimilitud-de-un-modelo-de-regresión-lineal-de-una-población-normal" class="section level5 hasAnchor" number="5.3.9.3.1">
<h5><span class="header-section-number">5.3.9.3.1</span> Estimadores de Máxima-Verosimilitud de un Modelo de Regresión Lineal de una Población Normal<a href="mrl-multiv.html#estimadores-de-máxima-verosimilitud-de-un-modelo-de-regresión-lineal-de-una-población-normal" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="theorem">
<p><span id="thm:teorema-estimadores-mle-mrl" class="theorem"><strong>Teorema 5.13  (Estimadores de Máxima-Verosimilitud en el MRLM) </strong></span>Suponga que la distribución conjunta de <span class="math inline">\(Y\)</span> y <span class="math inline">\(\underline{\mathbf{z}}\)</span> esta dada por:</p>
</div>
<p><span class="math display">\[
\begin{bmatrix} Y \\ --- \\ \underline{\mathbf{z}} \end{bmatrix}_{(r+1)\times 1}=\begin{bmatrix} Y \\ --- \\Z_1 \\ Z_2 \\ \vdots \\ Z_r \end{bmatrix} \sim N_{r+1} \biggl( \ \underline{\boldsymbol\mu} \ , \ \mathbf{\Sigma} \biggr)
\]</span></p>
<p>y sean
<span class="math display">\[
\underset{(r+1) \times 1}{\underline{\widehat{ {\Large \boldsymbol \mu } }} }= \begin{bmatrix}  \underset{1\times 1}{ \overline{Y} } \\ --- \\ \underset{r \times 1}{ \underline{{\underline{\overline{\mathbf{z}}}}} }  \end{bmatrix} \ \ \ \ \ \text{y} \ \ \ \ \ \underset{(r+1)\times (r+1)}{\mathbf{S}}= \begin{bmatrix} \underset{1\times 1}{S_{YY}} &amp; \vdots &amp; \underset{1\times r}{\underline{ \mathbf S}^t_{\ \underline{\mathbf{z}}Y}} \\ --- &amp; &amp; --- \\
\underset{r\times 1}{\underline{ \mathbf s}_{\ \underline{\mathbf{z}}Y}} &amp; \vdots &amp; \underset{r\times r}{\mathbf{S}_{\underline{\mathbf{z}}}}   \end{bmatrix}
\]</span></p>
<p>el Vector de Medias Muestral y la Matriz de Varianzas-Covarianzas Muestral, respectivamente, para una muestra aleatoria de tamaño <span class="math inline">\(n\)</span>-de dicha población. Los Estimadores de Máxima-Verosimilitud de los Coeficientes del Predictor Lineal son:
<span class="math display">\[
\underset{r\times 1}{\widehat{ \underline{\boldsymbol \beta } }} = \underset{r\times r \ \ r \times 1}{\mathbf{S_{\underline{\mathbf{z}}}}^{-1}\ \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y} } = \begin{bmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_r \end{bmatrix}_{r\times 1} \ \ \ \ \ \ \text{y} \ \ \ \ \ \ \widehat{\beta_0} = \overline{Y} - \underset{1\times r \ \ r \times r \ \ r\times 1}{\underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}^{\ t} \ \mathbf{S_{\underline{\mathbf{z}}}}^{-1}\ \underline{{\overline{\mathbf{z}}}} } = \overline{Y} - \underline{\widehat{ \boldsymbol \beta }}^{\ t} \  \underline{{\overline{\mathbf{z}}}}
\]</span></p>
<p>Por lo tanto, <em>El Estimador de Máxima-Verosimilitud</em> de la función de regresión linea es:
<span class="math display">\[
\widehat{\beta_0} + \widehat{ \underline{\boldsymbol \beta } }^{\ t}\ \underline{{\mathbf{z}}}=    \overline{Y} + \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}^{\ t} \ \mathbf{S_{\underline{\mathbf{z}}}}^{-1}\ \bigl(  \underline{{\mathbf{z}}}-\underline{{\overline{\mathbf{z}}}} \bigr)
\]</span></p>
<p>y <em>El Estimador de Máxima-Verosimilitud</em> del error cuadrático medio es:
<span class="math display">\[
\widehat{E\biggl[ Y - \beta_0- \underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}} \biggr] ^2} = \widehat{\sigma}_{YY.\underline{\mathbf{z}}} = \frac{n-1}{n} \biggl( S_{YY} - \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}^{\ t} \ \mathbf{S_{\underline{\mathbf{z}}}}^{-1}\underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}  \biggr)
\]</span></p>
<p><strong>Nota:</strong> Es costumbre cambiar el divisor <span class="math inline">\(n\)</span> por <span class="math inline">\(n-(r+1)\)</span> en la expresión del estimador MLE del Error Cuadrático Medio, con el fin de obtener un <em>Estimador Insesgado</em>, es decir:
<span class="math display" id="eq:estimador-insesgado-del-error-cuadratico-medio">\[
\begin{equation}
\widehat{E\biggl[ Y - \beta_0- \underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}} \biggr] ^2} = \frac{n-1}{n-(r+1)} \biggl( S_{YY} - \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}^{\ t} \ \mathbf{S_{\underline{\mathbf{z}}}}^{-1}\underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}  \biggr)\\
= \frac{\sum_{j=1}^n (y_j - \hat{\beta}_0 - \underline{\boldsymbol \beta}^t\ \underline{\mathbf{z}}_j )^2 }{n-(r+1)}=\frac{SS_E}{n-(r+1)}
\end{equation}
\tag{5.51}
\]</span></p>
<div class="example">
<p><span id="exm:ejemplo-estimadores-mle-predictor-lineal" class="example"><strong>Ejemplo 5.10  (Estimador MLE del Predictor Lineal en un MRLM) </strong></span>Continuando con el ejemplo <a href="rlm.html#exm:ejemplo2-mrlm">5.4</a>, sobre los datos de compañías que al considerar la compra de una computadora primero evaluán sus necesidades futuras con el fin de determinar el equipo adecuado.</p>
</div>
<p>Las variables medidas son:</p>
<p><span class="math inline">\(Z_1:\)</span> Pedidos de clientes (en miles)</p>
<p><span class="math inline">\(Z_2:\)</span> Recuento de elementos para agregar y/o eliminar (en miles)</p>
<p><span class="math inline">\(Y:\)</span> Tiempo de la CPU (unidad central de procesamiento) (en horas)</p>
<p>Los datos se dan en la siguiente tabla.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-143">Tabla 5.6: </span>Datos del Ejemplo de Compra de Computadores
</caption>
<thead>
<tr>
<th style="text-align:center;">
Z1-Pedidos
</th>
<th style="text-align:center;">
Z2-Elementos
</th>
<th style="text-align:center;">
Tiempo-Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
123.5
</td>
<td style="text-align:center;">
2.108
</td>
<td style="text-align:center;">
141.5
</td>
</tr>
<tr>
<td style="text-align:center;">
146.1
</td>
<td style="text-align:center;">
9.213
</td>
<td style="text-align:center;">
168.9
</td>
</tr>
<tr>
<td style="text-align:center;">
133.9
</td>
<td style="text-align:center;">
1.905
</td>
<td style="text-align:center;">
154.8
</td>
</tr>
<tr>
<td style="text-align:center;">
128.5
</td>
<td style="text-align:center;">
0.815
</td>
<td style="text-align:center;">
146.5
</td>
</tr>
<tr>
<td style="text-align:center;">
151.5
</td>
<td style="text-align:center;">
1.061
</td>
<td style="text-align:center;">
172.8
</td>
</tr>
<tr>
<td style="text-align:center;">
136.2
</td>
<td style="text-align:center;">
8.603
</td>
<td style="text-align:center;">
160.1
</td>
</tr>
<tr>
<td style="text-align:center;">
92.0
</td>
<td style="text-align:center;">
1.125
</td>
<td style="text-align:center;">
108.5
</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\underset{(r+1) \times 1}{\underline{\widehat{ {\Large \boldsymbol \mu } }} }= \begin{bmatrix}  \underset{1\times 1}{ \overline{Y} } \\ --- \\ \underset{2 \times 1}{ \underline{{\underline{\overline{\mathbf{z}}}}} }  \end{bmatrix}=\begin{bmatrix}  150.443 \\ --- \\ 130.243 \\ 3.547   \end{bmatrix}_{3\times 1}
\]</span></p>
<p><span class="math display">\[
\underset{(r+1)\times (r+1)}{\mathbf{S}}= \begin{bmatrix} \underset{1\times 1}{S_{YY}} &amp; \vdots &amp; \underset{1\times 2}{\underline{ \mathbf S}^t_{\ \underline{\mathbf{z}}Y}} \\ --- &amp; &amp; --- \\
\underset{2\times 1}{\underline{ \mathbf s}_{\ \underline{\mathbf{z}}Y}} &amp; \vdots &amp; \underset{2\times 2}{\mathbf{S}_{\underline{\mathbf{z}}}}   \end{bmatrix}= \begin{bmatrix} 467.913 &amp; \vdots &amp; 418.763&amp; 35.983 \\ --- &amp; &amp; --- --- --- \\
418.763 &amp; \vdots &amp; 377.2 &amp; 28.034 \\
35.983 &amp; \vdots &amp; 28.034 &amp; 13.657 \end{bmatrix}
\]</span></p>
<p>Asumiendo que <span class="math inline">\(Y,Z_1\)</span> y <span class="math inline">\(Z_2\)</span> se distribuyen conjuntamente como una normal 3-variada, se tiene que usando los resultados del teorema <a href="mrl-multiv.html#thm:teorema-estimadores-mle-mrl">5.13</a> la función de regresión estimada y el error cuadrático medio estimado están dados por:
<span class="math display">\[
\underset{2\times 1}{\widehat{ \underline{\boldsymbol \beta } }} = \underset{2\times 2 \ \ 2 \times 1}{\mathbf{S_{\underline{\mathbf{z}}}}^{-1}\ \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y} } = \begin{bmatrix} 0.0031287 &amp; -0.006422 \\ -0.006422 &amp; 0.086404 \end{bmatrix} \begin{bmatrix} 418.763 \\ 35.983 \end{bmatrix}  =  \begin{bmatrix} 1.079 \\ 0.420 \end{bmatrix}= \begin{bmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2  \end{bmatrix}_{2\times 1}
\]</span></p>
<p><span class="math display">\[
\widehat{\beta_0} = \overline{Y} - \underline{\widehat{ \boldsymbol \beta }}^{\ t} \  \underline{{\overline{\mathbf{z}}}} = 150.443  -\begin{bmatrix} 1.079 &amp;  0.420 \end{bmatrix} \begin{bmatrix} 130.243 \\  3.547 \end{bmatrix} = 150.443 - 142.019 = 8.421
\]</span></p>
<p>de donde, la función de regresión estimada es:
<span class="math display">\[
\widehat{y}=\widehat{\beta_0} + \widehat{ \underline{\boldsymbol \beta } }^t \ \underline{\mathbf{z}} = 8.42-1.079\ Z_1+0.42\ Z_2
\]</span></p>
<p>y el estimador de máxima-verosimilitud del error cuadrático medio es:
<span class="math display">\[
\frac{n-1}{n} \biggl( S_{YY} - \underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}^{\ t} \ \mathbf{S_{\underline{\mathbf{z}}}}^{-1}\underline{\mathbf S}_{\ \underline{\mathbf{z}}Y}  \biggr)\\
= \frac{6}{7} \biggl( 467.913 - \begin{bmatrix} 418.763 &amp; 35.983 \end{bmatrix} \begin{bmatrix} 0.003128 &amp; -0.006422 \\ -0.006422 &amp; 0.086404 \end{bmatrix}\begin{bmatrix} 418.763 \\ 34.983 \end{bmatrix} \biggr) \\
= 0.894
\]</span></p>
</div>
</div>
<div id="predicción-de-varias-variables-respuestas" class="section level4 hasAnchor" number="5.3.9.4">
<h4><span class="header-section-number">5.3.9.4</span> Predicción de Varias Variables Respuestas<a href="mrl-multiv.html#predicción-de-varias-variables-respuestas" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La extensión de los resultados de las secciones anteriores al caso de predicción de varias variables respuestas <span class="math inline">\(Y_1,Y_2,\ldots.Y_m\)</span> es casi inmediata. Ahora se presenta dicha extensión para el caso de poblaciones normales multivariadas.</p>
<p>Suponga que
<span class="math display">\[
\begin{bmatrix} \underset{m\times 1}{\underline{\mathbf{y}} } \\ --- \\ \underset{r\times 1}{\underline{\mathbf{z}} } \end{bmatrix}_{(r+1)\times 1}=\begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_m \\ --- \\Z_1 \\ Z_2 \\ \vdots \\ Z_r \end{bmatrix}_{(r+1)\times 1} \sim N_{m+r} \biggl( \ \underline{\boldsymbol\mu} \ , \ \mathbf{\Sigma} \biggr)
\]</span></p>
<p>con:
<span class="math display">\[
\underset{(m+r) \times 1}{\underline{ {\Large \boldsymbol \mu } }}= \begin{bmatrix}  \underset{m\times 1}{ \underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}} } \\ --- \\ \underset{r \times 1}{ \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} }  \end{bmatrix} \ \ \ \ \ \text{y} \ \ \ \ \ \underset{(m+r)\times (m+r)}{\mathbf{\Sigma}}= \begin{bmatrix} \underset{m\times m}{\mathbf{ \Sigma}_{\ \underline{\mathbf{y}}} } &amp; \vdots &amp; \underset{m\times r}{ \mathbf{ \Sigma}_{\ Y \underline{\mathbf{z}}}} \\ --- &amp; &amp; --- \\
\underset{r\times m}{ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}Y}} &amp; \vdots &amp; \underset{r\times r}{\mathbf{\Sigma}_{\ \underline{\mathbf{z}}}}   \end{bmatrix}
\]</span></p>
<p>luego, por el resultado xxxx de la sección xxxx, se tiene que la Distribución Condicional de <span class="math inline">\(\underline{\mathbf{y}}=[Y_1,Y_2,\ldots,Y_m]^t\)</span> dado los valores fijos de: <span class="math inline">\(Z_1=z_1,Z_2=z_2,\cdots,Z_r=z_r\)</span> está dada por:
<span class="math display">\[
\underline{\mathbf{y}} \ \biggl | \ \underline{\mathbf{z}} =\underline{\mathbf{y}}\ \biggl |\ (z_1,z_2,\ldots,z_r) \ \\ \sim N_m \biggl( \ \underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  +  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}) \ , \ \mathbf{ \Sigma}_{\  \underline{\mathbf{y}}}-\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\underline{\boldsymbol \sigma}_{\ \underline{\mathbf{z}}\underline{\mathbf{y}}}   \  \biggr)
\]</span></p>
<p>de donde, el predictor de las variables es:
<span class="math display" id="eq:predictor-lineal-varias-respuestas">\[
\begin{equation}
E\biggl[ \underline{\mathbf{y}} \ \biggl | \ \underline{\mathbf{z}} \biggr] = E \biggl[ \underline{\mathbf{y}}\ \biggl |\ (z_1,z_2,\ldots,z_r) \ \biggr] =  \underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  +  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})
\end{equation}
\tag{5.52}
\]</span></p>
<p>El valor esperado condicional dado el <a href="mrl-multiv.html#eq:predictor-lineal-varias-respuestas">(5.52)</a>, considerado como una función de <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span> se le llama la <strong>Regresión Multivariada</strong> del vector <span class="math inline">\(\underline{\mathbf{y}}\)</span> sobre el vector <span class="math inline">\(\underline{\mathbf{z}}\)</span>, la cual está compuesta de <span class="math inline">\(m\)</span>-<em>Regresiones Múltiples Univariadas</em>. Por ejemplo, la primera componente del vector de medias condicionales es:
<span class="math display">\[
E\biggl[ \underline{Y}_1 \ \biggl | \ \underline{\mathbf{z}} \biggr] = E \biggl[ \underline{Y}_1\ \biggl |\ (z_1,z_2,\ldots,z_r) \ \biggr] =  \underline{ \mu}_{\ Y_1}  +  \mathbf{ \Sigma}_{\ Y_1\ \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})
\]</span></p>
<p>la cual minimiza el Error Cuadrático Medio en la predicción de <span class="math inline">\(Y_1\)</span>.</p>
<p>A la matriz <span class="math inline">\(m\times r\)</span> dada por:
<span class="math display">\[
\underset{m\times r}{ {\huge \boldsymbol \beta}}= \underset{m\times r \ r\times r}{ \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1} }
\]</span></p>
<p>se le llama <em>Matriz de Coeficinetes de Regresión</em>.</p>
<p>El vector <span class="math inline">\(m\times 1\)</span> de Error de Predicción dado por:
<span class="math display">\[
\underline{\mathbf{y}}-\underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  -  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})
\]</span></p>
<p>tiene la Matriz de Cuadrados y Productos Cruzados esperada, dada por:
<span class="math display">\[
\begin{align}
\mathbf{\Sigma}_{\underline{\mathbf{y}}\underline{\mathbf{y}}.\underline{\mathbf{z}}} &amp; = E\biggl[ \biggl(  \underline{\mathbf{y}}-\underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  -  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}) \biggr) \biggl(  \underline{\mathbf{y}}-\underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  -  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}}) \biggr)^t \biggr] \\
&amp; = \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1} (\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}})^t -
\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1} \mathbf{ \Sigma}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}} +
\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\  \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}  (\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}})^t \\
&amp; = \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}}
\end{align}
\]</span></p>
<p>Debido a que <span class="math inline">\(\Large{\underline{ \boldsymbol \mu}}\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span>-son generalmente desconocidos, ellos se deben estimar a partir de una muestra aleatoria con el fin de construir <em>El Predictor Lineal Multivariado</em> y determinar los <em>Errores de Predicción Esperados</em>.</p>
<div class="theorem">
<p><span id="thm:teorema-estimadores-mle-mrl-multiv-varias-respuestas" class="theorem"><strong>Teorema 5.14  (Estimadores MLE en el MRL-Multivariado (Varias Respuestas)) </strong></span>Suponga que <span class="math inline">\(\underline{\mathbf{y}}\)</span> y <span class="math inline">\(\underline{\mathbf{z}}\)</span> son conjuntamente distribuídas como una <span class="math inline">\(N_{m+r}(\Large{\underline{ \boldsymbol \mu}}\ , \ \mathbf{\Sigma})\)</span> entonces, la Regresión de <span class="math inline">\(\underline{\mathbf{y}}\)</span> sobre <span class="math inline">\(\underline{\mathbf{z}}\)</span> está dada por:</p>
</div>
<p><span class="math display">\[
\underline{\boldsymbol \beta}_{\ 0} + {\Large \boldsymbol \beta}\ \underline{\mathbf{z}} = \underset{\underline{\boldsymbol \beta}_{\ 0}}{ \underbrace{ \underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}} -   \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}   \  \underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}} } } + \underset{{\Large \boldsymbol \beta}}{ \underbrace{ \mathbf{ \Sigma}_{\ \underline{\mathbf{y}}  \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1} }} \  \underline{\mathbf{z}} \\
= \underline{ \boldsymbol \mu}_{\ \underline{\mathbf{y}}}  +  \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{\Sigma_{\underline{\mathbf{z}}}}^{-1}\  ( \underline{\mathbf{z}}-\underline{{\large \boldsymbol \mu}}_{\ \underline{\mathbf{z}}})\\
\]</span></p>
<p>La <em>Matriz de Cuadrados y Productos Cruzados de Errores</em> es:
<span class="math display">\[
E\biggl[ \biggl(\underline{\mathbf{y}}-  \underline{\boldsymbol \beta}_{\ 0} - {\Large \boldsymbol \beta}\ \underline{\mathbf{z}}\biggr)\biggl(\underline{\mathbf{y}}-  \underline{\boldsymbol \beta}_{\ 0} - {\Large \boldsymbol \beta}\ \underline{\mathbf{z}}\biggr)^t\  \biggr] =\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} =\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}}
\]</span></p>
<p>Ahora, Basados en una muestra aleatoria de tamaño <span class="math inline">\(n\)</span>-el <em>Estimador de Máxima-Verosimilitud</em> de la función de regresión es:
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}}_{\ 0} + \widehat{ {\Large \boldsymbol \beta}}\ \underline{\mathbf{z}} = \underline{ \overline{\mathbf{y}}}   +  \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{S}^{-1}_{\underline{\mathbf{z}}}\  ( \underline{\mathbf{z}}-\underline{\overline{\mathbf{z}}})\\
\]</span></p>
<p>y el <em>Estimador de Máxima-Verosimilitud</em> de <span class="math inline">\(\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}}\)</span> es:
<span class="math display">\[
\widehat{\mathbf{ \Sigma}}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} = \left(\frac{n-1}{n} \right) \left(\mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ S}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ S}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}} \right)
\]</span></p>
<p><strong>Nota:</strong> Se puede demostrar que un <em>Estimador Insesgado</em> de <span class="math inline">\(\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}}\)</span> es:
<span class="math display">\[
\left(\frac{n-1}{n-(r+1)} \right) \left(\mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ S}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ S}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}} \right)\\
= \left(\frac{n-1}{n-(r+1)} \right) \sum_{j=1}^n\  \biggl( \underline{\mathbf{y}}_{\ (j)}-  \widehat{\underline{\boldsymbol \beta}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}\ \underline{\mathbf{z}}_{\ (j)}\biggr)\ \biggl(\underline{\mathbf{y}}_{\ (j)}-  \widehat{\underline{\boldsymbol \beta}}_{\ 0} - \widehat{{\Large \boldsymbol \beta}}\ \underline{\mathbf{z}}_{\ (j)} \biggr)^t
\]</span></p>
<div class="example">
<p><span id="exm:ejemplo1-mrl-multivariado-varias-respuestas" class="example"><strong>Ejemplo 5.11  (Ajuste de un MRL-Multivariado (Varias Respuestas)) </strong></span>Continuando con el ejemplo <a href="rlm.html#exm:ejemplo2-mrlm">5.4</a>, sobre los datos de compañías que al considerar la compra de una computadora primero evaluán sus necesidades futuras con el fin de determinar el equipo adecuado. Ahora se tiene una nueva variable respuesta <span class="math inline">\(Y_2\)</span>, para tener así dos variables repsuestas, <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span>.</p>
</div>
<p>Las variables medidas son:</p>
<p><span class="math inline">\(Z_1:\)</span> Pedidos de clientes (en miles)</p>
<p><span class="math inline">\(Z_2:\)</span> Recuento de elementos para agregar y/o eliminar (en miles)</p>
<p><span class="math inline">\(Y_1:\)</span> Tiempo de la CPU (unidad central de procesamiento) (en horas)</p>
<p><span class="math inline">\(Y_2:\)</span> Capacidad de entrada/salida del disco duro.</p>
<p>Los datos se dan en la siguiente tabla.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-144">Tabla 5.7: </span>Datos del Ejemplo de Compra de Computadores
</caption>
<thead>
<tr>
<th style="text-align:center;">
Z1-Pedidos
</th>
<th style="text-align:center;">
Z2-Elementos
</th>
<th style="text-align:center;">
Tiempo-Y1
</th>
<th style="text-align:center;">
Disco-Y2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
123.5
</td>
<td style="text-align:center;">
2.108
</td>
<td style="text-align:center;">
141.5
</td>
<td style="text-align:center;">
301.8
</td>
</tr>
<tr>
<td style="text-align:center;">
146.1
</td>
<td style="text-align:center;">
9.213
</td>
<td style="text-align:center;">
168.9
</td>
<td style="text-align:center;">
396.1
</td>
</tr>
<tr>
<td style="text-align:center;">
133.9
</td>
<td style="text-align:center;">
1.905
</td>
<td style="text-align:center;">
154.8
</td>
<td style="text-align:center;">
328.2
</td>
</tr>
<tr>
<td style="text-align:center;">
128.5
</td>
<td style="text-align:center;">
0.815
</td>
<td style="text-align:center;">
146.5
</td>
<td style="text-align:center;">
307.4
</td>
</tr>
<tr>
<td style="text-align:center;">
151.5
</td>
<td style="text-align:center;">
1.061
</td>
<td style="text-align:center;">
172.8
</td>
<td style="text-align:center;">
362.4
</td>
</tr>
<tr>
<td style="text-align:center;">
136.2
</td>
<td style="text-align:center;">
8.603
</td>
<td style="text-align:center;">
160.1
</td>
<td style="text-align:center;">
369.5
</td>
</tr>
<tr>
<td style="text-align:center;">
92.0
</td>
<td style="text-align:center;">
1.125
</td>
<td style="text-align:center;">
108.5
</td>
<td style="text-align:center;">
229.1
</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\underset{4 \times 1}{\underline{\widehat{ {\Large \boldsymbol \mu } }} }= \begin{bmatrix}  \underset{2 \times 1}{ \underline{{\overline{\mathbf{y}}}} }  \\ --- \\ \underset{2 \times 1}{ \underline{{\overline{\mathbf{z}}}} }  \end{bmatrix}=\begin{bmatrix} \overline{Y}_1 \\ \overline{Y}_2 \\ --- \\ \overline{Z}_1 \\ \overline{Z}_2 \end{bmatrix} =\begin{bmatrix}  150.443 \\ 327.786 \\ --- \\ 130.243 \\ 3.547   \end{bmatrix}_{4\times 1}
\]</span></p>
<p><span class="math display">\[
\underset{4\times 4}{\mathbf{S}}= \begin{bmatrix} \underset{ 2\times 2 }{ \mathbf{S}_{\underline{\mathbf{y}}} } &amp; \vdots &amp; \underset{2\times 2}{ \mathbf{S}_{\underline{\mathbf{y}}\ \underline{\mathbf{z}}} } \\ --- &amp; &amp; --- \\
\underset{2\times 2}{ \mathbf{S}_{\underline{\mathbf{z}}\ \underline{\mathbf{y}}} } &amp; \vdots &amp; \underset{2\times 2}{ \mathbf{S}_{\underline{\mathbf{z}}}  } \end{bmatrix}\\
\begin{bmatrix}
S^2_{Y_1} &amp; S_{Y_1Y_2} &amp;  \vdots &amp;
S_{Y_1Z_1} &amp; S_{Y_1Z_2} \\
S_{Y_2Y_1} &amp; S^2_{Y_2} &amp;  \vdots &amp;
S_{Y_2Z_1} &amp; S_{Y_2Z_2} \\ --- &amp;
--- &amp; &amp; --- &amp; --- \\
S_{Z_1Y_1} &amp; S_{Z_1Y_2} &amp;  \vdots &amp;
S^2_{Z_1} &amp; S_{Z_1Z_2} \\
S_{Z_2Y_1} &amp; S_{Z_2Y_2} &amp;  \vdots &amp;
S_{Z_2Z_1} &amp; S^2_{Z_2}
\end{bmatrix} \\
=  \begin{bmatrix}
467.913 &amp; 1148.556 &amp;  \vdots &amp;
418.763 &amp; 35.983 \\
1148.556 &amp; 3072.491 &amp;  \vdots &amp;
1008.976 &amp; 140.558 \\ --- &amp;
--- &amp; &amp; --- &amp; --- \\
418.763 &amp; 1008.976 &amp; \vdots &amp;
377.2 &amp; 28.034 \\  
35.983 &amp; 140.558 &amp;  \vdots &amp;
28.034 &amp; 13.657
\end{bmatrix}_{4\times 4}
\]</span></p>
<p>Asumiendo Normalidad Conjunta Multivariada, se halla que la <em>Función de Regresión Estimada</em> es:
<span class="math display">\[
\begin{align}
\underset{2\times 1}{\widehat{\underline{\mathbf{y}}}}
&amp;=\widehat{\underline{\boldsymbol \beta}}_{\ 0} + \widehat{ {\Large \boldsymbol \beta}}\ \underline{\mathbf{z}}\\
&amp; = \underline{ \overline{\mathbf{y}}}   +  \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{S}^{-1}_{\underline{\mathbf{z}}}\  ( \underline{\mathbf{z}}-\underline{\overline{\mathbf{z}}})\\ \\
&amp;= \begin{bmatrix}  150.443 \\ 327.786 \end{bmatrix} + \begin{bmatrix}
418.763 &amp; 35.983 \\
1008.976 &amp; 140.558
\end{bmatrix} \times \begin{bmatrix}
377.2 &amp; 28.034 \\
28.034 &amp; 13.657
\end{bmatrix}^{-1} \begin{bmatrix} Z_1 - 130.24 \\ Z_2 - 3.547 \end{bmatrix} \\ \\
&amp;= \begin{bmatrix}  150.443 \\ 327.786 \end{bmatrix} + \begin{bmatrix}
418.763 &amp; 35.983 \\
1008.976 &amp; 140.558
\end{bmatrix} \times \begin{bmatrix}
0.003128 &amp; -0.006422 \\
-0.006422 &amp; 0.086404
\end{bmatrix} \begin{bmatrix} Z_1 - 130.24 \\ Z_2 - 3.547 \end{bmatrix} \\ \\
&amp;= \begin{bmatrix}  150.443 \\ 327.786 \end{bmatrix} + \begin{bmatrix}
1.079 &amp; 0.420 \\
2.254 &amp; 5.665
\end{bmatrix} \times \begin{bmatrix} Z_1 - 130.24 \\ Z_2 - 3.547 \end{bmatrix} \\ \\
\begin{bmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \end{bmatrix} &amp;= \begin{bmatrix}  150.443 \\ 327.786 \end{bmatrix} + \begin{bmatrix}
1.079(Z_1 - 130.24) + 0.420(Z_2 - 3.547) \\
2.254(Z_1 - 130.24) + 5.665(Z_2 - 3.547)
\end{bmatrix}
\end{align}
\]</span>
De donde el <em>Predictor Lineal</em> de <span class="math inline">\(Y_1\)</span> con el Mínimo Error Cuadrático Medio es:
<span class="math display">\[
\widehat{Y}_1 = 150.443 +
1.079(Z_1 - 130.24) + 0.420(Z_2 - 3.547) = 8.42 + 1.08\ Z_1 + 0.42\ Z_2  
\]</span></p>
<p>Similarmente, el <em>Predictor Lineal</em> de <span class="math inline">\(Y_2\)</span> con el Mínimo Error Cuadrático Medio es:
<span class="math display">\[
\widehat{Y}_2  = 327.786 +  
2.254(Z_1 - 130.24) + 5.665(Z_2 - 3.547) = 14.14 + 2.25\ Z_1 + 5.67\ Z_2
\]</span></p>
<p>El Estimador de Máxima-Verosimilitud de la Matriz de Cuadrados y Productos Cruzados de Errores es:
<span class="math display">\[
\begin{align}
&amp; \left(\frac{n-1}{n} \right) \left(\mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ S}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ S}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}} \right) \\
&amp; = \left(\frac{6}{7}\right) \left( \begin{bmatrix}
467.913 &amp; 1148.556 \\
1148.556 &amp; 3072.491 \end{bmatrix} \\ - \begin{bmatrix}
418.763 &amp; 35.983 \\
1008.976 &amp; 140.558\end{bmatrix} \ \begin{bmatrix}
0.003128 &amp; -0.006422 \\
-0.006422 &amp; 0.086404
\end{bmatrix}\ \begin{bmatrix}
418.763 &amp; 1008.976 \\  
35.983 &amp; 140.558
\end{bmatrix}  \right) \\ \\
&amp; = \left(\frac{6}{7}\right) \begin{bmatrix} 1.043 &amp; 1.042 \\  1.042 &amp; 2.572 \end{bmatrix} \\ \\
&amp; = \begin{bmatrix} 0.894 &amp; 0.893 \\ 0.893 &amp; 2.205 \end{bmatrix}
\end{align}
\]</span></p>
<p>De lo anterior se observa que, La primera función de regresión estimada, <span class="math inline">\(8.42 + 1.08\ Z_1 + 0.42\ Z_2\)</span>, tiene un error cuadrático medio asociado de <span class="math inline">\(0.894\)</span>. De manera similar, la segunda función de regresión estimada, <span class="math inline">\(14.14 + 2.25\ Z_1 + 5.67\ Z_2\)</span>, tiene un error cuadrático medio asociado de <span class="math inline">\(2.205\)</span>. Con esto se observa que los datos nos permiten predecir la primera respuesta, <span class="math inline">\(Y_1\)</span>, con un menor error que la segunda respuesta, <span class="math inline">\(Y_2\)</span>.</p>
<p>La covarianza positiva de <span class="math inline">\(0.893\)</span> indica una sobre-predicción (sub-predicción) del tiempo de la CPU <span class="math inline">\(Y_1\)</span> que tiende a ir acompañada de una sobrepredicción ción (sub-predicción) de la capacidad del disco duro <span class="math inline">\(Y_2\)</span>.</p>
</div>
<div id="coeficiente-de-correlación-parcial" class="section level4 hasAnchor" number="5.3.9.5">
<h4><span class="header-section-number">5.3.9.5</span> Coeficiente de Correlación Parcial<a href="mrl-multiv.html#coeficiente-de-correlación-parcial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consideremos el par de errores dados por:
<span class="math display">\[
\varepsilon_1 = Y_1 - E[\ Y_1 \ \bigl| \ \underline{\mathbf{z}}\   ] = Y_1 - \mu_{Y_1} - \mathbf{\Sigma}_{\ Y_1\underline{\mathbf{z}} }\mathbf{\Sigma}^{-1}_{\underline{\mathbf{z}}}\left( \underline{\mathbf{z}} - \underline{\boldsymbol \mu}_{\ \underline{\mathbf{z}}}  \right)
\]</span>
<span class="math display">\[
\varepsilon_2 = Y_2 - E[\ Y_2 \ \bigl| \ \underline{\mathbf{z}}\   ] = Y_2 - \mu_{Y_2} - \mathbf{\Sigma}_{\ Y_2\underline{\mathbf{z}} }\mathbf{\Sigma}^{-1}_{\underline{\mathbf{z}}}\left( \underline{\mathbf{z}} - \underline{\boldsymbol \mu}_{\ \underline{\mathbf{z}}}  \right)
\]</span></p>
<p>obtenidos a partir del uso de los mejores predictores lineales de <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span>. La correlación entre estos dos errores, determinada a partir de la Matriz de Covarianzas de Error dada por:
<span class="math display">\[
\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} =\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}}
\]</span></p>
<p>mide la Asociación entre <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span> después de eliminar los efectos de las variables regresoras <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span>.</p>
<p><strong>El Coeficiente de Correlación Parcial</strong></p>
<p>Se define <em>El Coeficiente de Correlación Parcial</em> entre <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span> al Eliminar <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span> como sigue:
<span class="math display" id="eq:coeficiente-de-correlacion-parcial">\[
\begin{equation}
\rho_{Y_1Y_2.\underline{\mathbf{z}}}= \frac{\sigma_{Y_1Y_2.\underline{\mathbf{z}}}}{\sqrt{\sigma_{Y_1Y_1.\underline{\mathbf{z}}}}\sqrt{\sigma_{Y_2Y_2.\underline{\mathbf{z}}}}}
\end{equation}
\tag{5.53}
\]</span></p>
<p>donde, <span class="math inline">\(\sigma_{Y_iY_k.\underline{\mathbf{z}}}\)</span>-es el elemento en la entrada <span class="math inline">\((i,k)\)</span> de la matriz <span class="math inline">\(\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} =\mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ \Sigma}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{ \Sigma}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}}\)</span>.</p>
<p>El correspondiente <em>Coeficiente de Correlación Parcial Muestral</em> está dado por:
<span class="math display" id="eq:coeficiente-de-correlacion-parcial-muestral">\[
\begin{equation}
r_{Y_1Y_2.\underline{\mathbf{z}}}= \frac{S_{Y_1Y_2.\underline{\mathbf{z}}}}{\sqrt{S_{Y_1Y_1.\underline{\mathbf{z}}}}\sqrt{S_{Y_2Y_2.\underline{\mathbf{z}}}}}
\end{equation}
\tag{5.54}
\]</span></p>
<p>donde, <span class="math inline">\(S_{Y_iY_k.\underline{\mathbf{z}}}\)</span>-es el elemento en la entrada <span class="math inline">\((i,k)\)</span> de la matriz <span class="math inline">\(\mathbf{S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} =\mathbf{S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{S}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{S}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}}\)</span>.</p>
<p>Asumiendo Distribución Normal Conjunta de <span class="math inline">\(\mathbf{Y}\)</span> y <span class="math inline">\(\underline{\mathbf{z}}\)</span>, se tiene que <span class="math inline">\(r_{Y_1Y_2.\underline{\mathbf{z}}}\)</span>-es el Estimador de Máxima-Verosimilitud de <span class="math inline">\(\rho_{Y_1Y_2.\underline{\mathbf{z}}}\)</span>.</p>
<div class="example">
<p><span id="exm:ejemplo1-coeficiente-de-correlacion-parcial" class="example"><strong>Ejemplo 5.12  (Cálculo de un Coeficiente de Correlación Parcial (Varias Respuestas)) </strong></span>Continuando con el ejemplo <a href="mrl-multiv.html#exm:ejemplo1-mrl-multivariado-varias-respuestas">5.11</a>, sobre los datos de compañías que al considerar la compra de una computadora primero evaluán sus necesidades futuras con el fin de determinar el equipo adecuado. Ahora se tiene que:</p>
</div>
<p><span class="math display">\[
\mathbf{S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}.\underline{\mathbf{z}}} =\mathbf{S}_{\ \underline{\mathbf{y}} \underline{\mathbf{y}}} - \mathbf{ S}_{\ \underline{\mathbf{y}} \underline{\mathbf{z}}}\ \mathbf{S}_{\ \underline{\mathbf{z}}}^{-1}\ \mathbf{S}_{\ \underline{\mathbf{z}} \underline{\mathbf{y}}} = \begin{bmatrix} 1.043 &amp; 1.042 \\ 1.042 &amp; 2.572 \end{bmatrix}
\]</span></p>
<p>luego se tiene que:
<span class="math display">\[
r_{Y_1Y_2.\underline{\mathbf{z}}}= \frac{S_{Y_1Y_2.\underline{\mathbf{z}}}}{\sqrt{S_{Y_1Y_1.\underline{\mathbf{z}}}}\sqrt{S_{Y_2Y_2.\underline{\mathbf{z}}}}}= \frac{1.042}{\sqrt{1.043}\sqrt{2.572}}=0.64
\]</span></p>
<p>Al calcular el Coeficiente de Correlación Ordinario o tradicional, se tiene que:
<span class="math display">\[
r_{Y_1Y_2}= \frac{S_{Y_1Y_2}}{\sqrt{S_{Y_1}}\sqrt{S_{Y_2}} }= \frac{1148.556}{\sqrt{467.913}\sqrt{3072.491}} = 0.96
\]</span></p>
<p>Al comparar los dos coeficientes de correlación, vemos que la asociación entre <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span> ha sido bruscamente reducida después de eliminar los efectos de las variables regresoras <span class="math inline">\(Z_1\)</span> y <span class="math inline">\(Z_2\)</span> en este caso, sobre ambas variables respuestas <span class="math inline">\(Y_1\)</span> y <span class="math inline">\(Y_2\)</span>.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-heck1960" class="csl-entry">
Heck, DL. 1960. <span>“Charts of Some Upper Percentage Points of the Distribution of the Largest Characteristic Root.”</span> <em>The Annals of Mathematical Statistics</em> 31 (3): 625–42.
</div>
<div id="ref-khattree2018" class="csl-entry">
Khattree, Ravindra, and Dayanand N Naik. 2018. <em>Applied Multivariate Statistics with SAS Software</em>. SAS Institute Inc.
</div>
<div id="ref-pillai1967" class="csl-entry">
Pillai, KC Sreedharan. 1967. <span>“Upper Percentage Points of the Largest Root of a Matrix in Multivariate Analysis.”</span> <em>Biometrika</em> 54 (1-2): 189–94.
</div>
<div id="ref-rao1973" class="csl-entry">
Rao, Calyampudi Radhakrishna, Calyampudi Radhakrishna Rao, Mathematischer Statistiker, Calyampudi Radhakrishna Rao, and Calyampudi Radhakrishna Rao. 1973. <em>Linear Statistical Inference and Its Applications</em>. Vol. 2. Wiley New York.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rlm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ACP.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-iam.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
