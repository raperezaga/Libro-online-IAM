<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.5 Evaluación de las Funciones de Clasificación | Chapter 10</title>
  <meta name="description" content="Este libro contine ……" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="9.5 Evaluación de las Funciones de Clasificación | Chapter 10" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/imagenes/imagen1.png" />
  <meta property="og:description" content="Este libro contine ……" />
  <meta name="github-repo" content="raperezaga/iam" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.5 Evaluación de las Funciones de Clasificación | Chapter 10" />
  
  <meta name="twitter:description" content="Este libro contine ……" />
  <meta name="twitter:image" content="/imagenes/imagen1.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dos-pob-nm.html"/>
<link rel="next" href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preámbulo</a>
<ul>
<li class="chapter" data-level="" data-path="descripción.html"><a href="descripción.html"><i class="fa fa-check"></i>Descripción</a></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="" data-path="dedicación.html"><a href="dedicación.html"><i class="fa fa-check"></i>Dedicación</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="paquetes-usados-en-el-libro.html"><a href="paquetes-usados-en-el-libro.html"><i class="fa fa-check"></i>Paquetes usados en el libro</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="rep-al.html"><a href="rep-al.html"><i class="fa fa-check"></i><b>1</b> Repaso de álgebra lineal</a>
<ul>
<li class="chapter" data-level="1.1" data-path="acb-al.html"><a href="acb-al.html"><i class="fa fa-check"></i><b>1.1</b> Algunos conceptos básicos</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="acb-al.html"><a href="acb-al.html#matrices"><i class="fa fa-check"></i><b>1.1.1</b> Matrices</a></li>
<li class="chapter" data-level="1.1.2" data-path="acb-al.html"><a href="acb-al.html#vectores"><i class="fa fa-check"></i><b>1.1.2</b> Vectores</a></li>
<li class="chapter" data-level="1.1.3" data-path="acb-al.html"><a href="acb-al.html#operaciones-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Operaciones Matriciales</a></li>
<li class="chapter" data-level="1.1.4" data-path="acb-al.html"><a href="acb-al.html#matrices-especiales"><i class="fa fa-check"></i><b>1.1.4</b> Matrices Especiales</a></li>
<li class="chapter" data-level="1.1.5" data-path="acb-al.html"><a href="acb-al.html#descomp-espectral"><i class="fa fa-check"></i><b>1.1.5</b> Descomposición Espectral de una Matriz (eigen-descomposición)</a></li>
<li class="chapter" data-level="1.1.6" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-una-matriz"><i class="fa fa-check"></i><b>1.1.6</b> Diagonalización de una Matriz</a></li>
<li class="chapter" data-level="1.1.7" data-path="acb-al.html"><a href="acb-al.html#diagonalización-ortogonal"><i class="fa fa-check"></i><b>1.1.7</b> Diagonalización Ortogonal</a></li>
<li class="chapter" data-level="1.1.8" data-path="acb-al.html"><a href="acb-al.html#diagonalizacion-inversa"><i class="fa fa-check"></i><b>1.1.8</b> Diagonalización de la Inversa de una Matriz</a></li>
<li class="chapter" data-level="1.1.9" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-la-matriz-raíz-cuadrada"><i class="fa fa-check"></i><b>1.1.9</b> Diagonalización de la Matriz Raíz Cuadrada</a></li>
<li class="chapter" data-level="1.1.10" data-path="acb-al.html"><a href="acb-al.html#traza-determinante-y-rango-de-una-matriz"><i class="fa fa-check"></i><b>1.1.10</b> Traza, Determinante y Rango de una Matriz</a></li>
<li class="chapter" data-level="1.1.11" data-path="acb-al.html"><a href="acb-al.html#formas-cuadraticas"><i class="fa fa-check"></i><b>1.1.11</b> Formas Cuadráticas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><a href="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><i class="fa fa-check"></i><b>1.2</b> Algunas Propiedades Estadísticas de la Descomposición de una Matriz en Valores y Vectores Propios</a></li>
<li class="chapter" data-level="1.3" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html"><i class="fa fa-check"></i><b>1.3</b> Diferenciación con Vectores y Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html#vector-gradiente-para-diferentes-definiciones-de-funderlinemathbfx"><i class="fa fa-check"></i><b>1.3.1</b> Vector-Gradiente para diferentes definiciones de <span class="math inline">\(f(\underline{\mathbf{x}})\)</span>:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="org_pres_dat.html"><a href="org_pres_dat.html"><i class="fa fa-check"></i><b>2</b> Organización y Presentación de Datos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html"><i class="fa fa-check"></i><b>2.1</b> Resumenes Descriptivos</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#matriz-de-datos"><i class="fa fa-check"></i><b>2.1.1</b> Matriz de Datos</a></li>
<li class="chapter" data-level="2.1.2" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#estadísticos-descriptivos"><i class="fa fa-check"></i><b>2.1.2</b> Estadísticos descriptivos</a></li>
<li class="chapter" data-level="2.1.3" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#algunas-notaciones"><i class="fa fa-check"></i><b>2.1.3</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="2.1.4" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#representación-gráfica-de-observaciones-multivariadas"><i class="fa fa-check"></i><b>2.1.4</b> Representación Gráfica de Observaciones multivariadas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vectores-y-matrices-aleatorias.html"><a href="vectores-y-matrices-aleatorias.html"><i class="fa fa-check"></i><b>2.2</b> Vectores y Matrices Aleatorias</a></li>
<li class="chapter" data-level="2.3" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html"><i class="fa fa-check"></i><b>2.3</b> Vectores y Matrices Poblacionales Particionados</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-del-vector-de-medias-poblacionales"><i class="fa fa-check"></i><b>2.3.1</b> Particionamiento del Vector de Medias-Poblacionales</a></li>
<li class="chapter" data-level="2.3.2" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-de-la-matriz-de-var-cov-poblacional-mathbfsigma"><i class="fa fa-check"></i><b>2.3.2</b> Particionamiento de la Matriz de Var-Cov Poblacional <span class="math inline">\(\mathbf{\Sigma}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.4</b> Propiedades Sobre la Media y Varianza de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.5" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html"><i class="fa fa-check"></i><b>2.5</b> Vectores y Matrices Muestrales Particionadas</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-del-vector-de-medias-muestrales"><i class="fa fa-check"></i><b>2.5.1</b> Particionamiento del Vector de Medias Muestrales</a></li>
<li class="chapter" data-level="2.5.2" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-de-la-matriz-de-var-cov-muestrales"><i class="fa fa-check"></i><b>2.5.2</b> Particionamiento de la Matriz de Var-Cov Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="algunas-formas-matriciales-eficientes.html"><a href="algunas-formas-matriciales-eficientes.html"><i class="fa fa-check"></i><b>2.6</b> Algunas formas matriciales Eficientes</a></li>
<li class="chapter" data-level="2.7" data-path="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.7</b> Propiedades Sobre la Media y Varianza Muestral de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.8" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><i class="fa fa-check"></i><b>2.8</b> Varianza Generalizada Muestral y su Interpretación Geométrica</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-1-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.1</b> Interpretación Geométrica-1 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.2" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-2-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.2</b> Interpretación Geométrica-2 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.3" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#varianza-generalizada-determinada-por-la-matriz-de-correlación-muestral-mathbfr"><i class="fa fa-check"></i><b>2.8.3</b> Varianza Generalizada Determinada por la Matriz de Correlación Muestral <span class="math inline">\(\mathbf{R}\)</span></a></li>
<li class="chapter" data-level="2.8.4" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#relación-entre-mathbfs-y-mathbfr"><i class="fa fa-check"></i><b>2.8.4</b> Relación entre <span class="math inline">\(|\mathbf{S}|\)</span> y <span class="math inline">\(|\mathbf{R}|\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="varianza-total-muestral.html"><a href="varianza-total-muestral.html"><i class="fa fa-check"></i><b>2.9</b> Varianza Total Muestral</a></li>
<li class="chapter" data-level="2.10" data-path="muestra-aleatoria-de-distribuciones-p-variadas.html"><a href="muestra-aleatoria-de-distribuciones-p-variadas.html"><i class="fa fa-check"></i><b>2.10</b> Muestra Aleatoria de Distribuciones <span class="math inline">\(p\)</span>-Variadas</a></li>
<li class="chapter" data-level="2.11" data-path="distancias.html"><a href="distancias.html"><i class="fa fa-check"></i><b>2.11</b> Distancias</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="distancias.html"><a href="distancias.html#dist_euclidea"><i class="fa fa-check"></i><b>2.11.1</b> Definición de Algunas Distancias</a></li>
<li class="chapter" data-level="2.11.2" data-path="distancias.html"><a href="distancias.html#relación-de-la-distancia-de-mahalanobis-con-la-distribución-chi-cuadrado"><i class="fa fa-check"></i><b>2.11.2</b> Relación de la Distancia de Mahalanobis con la Distribución chi-Cuadrado</a></li>
<li class="chapter" data-level="2.11.3" data-path="distancias.html"><a href="distancias.html#ejemplo"><i class="fa fa-check"></i><b>2.11.3</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Normal-Multiv.html"><a href="Normal-Multiv.html"><i class="fa fa-check"></i><b>3</b> Distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="geometria-NM.html"><a href="geometria-NM.html"><i class="fa fa-check"></i><b>3.1</b> Geometría y propiedades de la NM</a></li>
<li class="chapter" data-level="3.2" data-path="normal-univariada.html"><a href="normal-univariada.html"><i class="fa fa-check"></i><b>3.2</b> Normal Univariada</a></li>
<li class="chapter" data-level="3.3" data-path="normal-multivariada.html"><a href="normal-multivariada.html"><i class="fa fa-check"></i><b>3.3</b> Normal Multivariada</a></li>
<li class="chapter" data-level="3.4" data-path="algunos-aspectos-geométricos-de-la-nm.html"><a href="algunos-aspectos-geométricos-de-la-nm.html"><i class="fa fa-check"></i><b>3.4</b> Algunos Aspectos Geométricos de la NM</a></li>
<li class="chapter" data-level="3.5" data-path="prop-nm.html"><a href="prop-nm.html"><i class="fa fa-check"></i><b>3.5</b> Propiedades de la distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="prop-nm.html"><a href="prop-nm.html#prop1"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Propiedad-1:</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="prop-nm.html"><a href="prop-nm.html#prop2"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Propiedad-2:</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="prop-nm.html"><a href="prop-nm.html#prop3"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Propiedad-3:</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="prop-nm.html"><a href="prop-nm.html#prop4"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Propiedad-4:</strong></a></li>
<li class="chapter" data-level="3.5.5" data-path="prop-nm.html"><a href="prop-nm.html#prop5"><i class="fa fa-check"></i><b>3.5.5</b> <strong>Propiedad-5:</strong></a></li>
<li class="chapter" data-level="3.5.6" data-path="prop-nm.html"><a href="prop-nm.html#prop6"><i class="fa fa-check"></i><b>3.5.6</b> <strong>Propiedad-6:</strong></a></li>
<li class="chapter" data-level="3.5.7" data-path="prop-nm.html"><a href="prop-nm.html#prop7"><i class="fa fa-check"></i><b>3.5.7</b> <strong>Propiedad-7:</strong></a></li>
<li class="chapter" data-level="3.5.8" data-path="prop-nm.html"><a href="prop-nm.html#prop8"><i class="fa fa-check"></i><b>3.5.8</b> <strong>Propiedad-8:</strong></a></li>
<li class="chapter" data-level="3.5.9" data-path="prop-nm.html"><a href="prop-nm.html#prop9"><i class="fa fa-check"></i><b>3.5.9</b> <strong>Propiedad-9:</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.6</b> Evaluación del Supuesto de Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-a-nivel-marginal-ie.-normalidad-univariada"><i class="fa fa-check"></i><b>3.6.1</b> Evaluación a nivel marginal (ie. Normalidad Univariada)</a></li>
<li class="chapter" data-level="3.6.2" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-de-la-normalidad-bi-variada"><i class="fa fa-check"></i><b>3.6.2</b> Evaluación de la Normalidad Bi-variada</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="detección-de-observaciones-atípicas.html"><a href="detección-de-observaciones-atípicas.html"><i class="fa fa-check"></i><b>3.7</b> Detección de Observaciones Atípicas</a></li>
<li class="chapter" data-level="3.8" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.8</b> Transformaciones para Acercar a la Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#familia-de-transformaciones-de-potencia-de-box-y-cox-1964"><i class="fa fa-check"></i><b>3.8.1</b> Familia de Transformaciones de Potencia de Box y Cox (1964)</a></li>
<li class="chapter" data-level="3.8.2" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#transformaciones-para-el-caso-multivariado"><i class="fa fa-check"></i><b>3.8.2</b> Transformaciones para el Caso Multivariado:</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html"><i class="fa fa-check"></i><b>3.9</b> Muestra Aleatoria Normal <span class="math inline">\(p\)</span>-Variada</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#estimadores-de-máx-ver-de-una-normal-multivariada"><i class="fa fa-check"></i><b>3.9.1</b> Estimadores de Máx-Ver de una Normal-Multivariada</a></li>
<li class="chapter" data-level="3.9.2" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#distribución-muestral-de-overlineunderlinemathbfx-y-mathbfs_n"><i class="fa fa-check"></i><b>3.9.2</b> Distribución Muestral de <span class="math inline">\(\overline{\underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\mathbf{S}_n\)</span></a></li>
<li class="chapter" data-level="3.9.3" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#comportamiento-de-underlineoverlinemathbfx_p-y-mathbfs-para-tamaños-muestrales-grandes"><i class="fa fa-check"></i><b>3.9.3</b> Comportamiento de <span class="math inline">\(\underline{\overline{\mathbf{x}}}_p\)</span> y <span class="math inline">\(\mathbf{S}\)</span> para Tamaños Muestrales Grandes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferen-estad.html"><a href="inferen-estad.html"><i class="fa fa-check"></i><b>4</b> Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i><b>4.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2" data-path="inferencia-estadística-para-la-media-mu-caso-univariado.html"><a href="inferencia-estadística-para-la-media-mu-caso-univariado.html"><i class="fa fa-check"></i><b>4.2</b> Inferencia Estadística para la Media (<span class="math inline">\(\mu\)</span>)-caso univariado</a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><i class="fa fa-check"></i><b>4.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-desconocida-normal"><i class="fa fa-check"></i><b>4.3.1</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Desconocida (Normal)</a></li>
<li class="chapter" data-level="4.3.2" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-conocida-población-normal"><i class="fa fa-check"></i><b>4.3.2</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Conocida (Población: Normal)</a></li>
<li class="chapter" data-level="4.3.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-en-muestra-grande"><i class="fa fa-check"></i><b>4.3.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> en Muestra Grande</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><i class="fa fa-check"></i><b>4.4</b> PH Acerca de Contrastes del Vector de Medias Poblacional <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span>, de una <span class="math inline">\(N_p(\underline{\boldsymbol{\mu}} \ , \ \mathbf{\Sigma} )\)</span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-para-el-caso-de-pnm"><i class="fa fa-check"></i><b>4.4.1</b> PH de Contrastes para el Caso de PNM</a></li>
<li class="chapter" data-level="4.4.2" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-en-en-caso-de-n-grande"><i class="fa fa-check"></i><b>4.4.2</b> PH de Contrastes en en caso de <span class="math inline">\(n\)</span>-Grande</a></li>
<li class="chapter" data-level="4.4.3" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#prueba-de-hipótesis-para-igualdad-de-medias"><i class="fa fa-check"></i><b>4.4.3</b> Prueba de hipótesis para igualdad de medias</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><i class="fa fa-check"></i><b>4.5</b> PH para Igualdad de Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}=\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida"><i class="fa fa-check"></i><b>4.5.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.5.2" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida"><i class="fa fa-check"></i><b>4.5.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.5.3" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-3.-mathbfsigma_-underlinemathbfxneq-mathbfsigma_-underlinemathbfy-desconocida"><i class="fa fa-check"></i><b>4.5.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}\neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocida</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><i class="fa fa-check"></i><b>4.6</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,   para muestras grandes</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida-1"><i class="fa fa-check"></i><b>4.6.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.6.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida-1"><i class="fa fa-check"></i><b>4.6.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.6.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-3.-mathbfsigma_-underlinemathbfx-neq-mathbfsigma_-underlinemathbfy-desconocidas"><i class="fa fa-check"></i><b>4.6.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}} \neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocidas</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><i class="fa fa-check"></i><b>4.7</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,      Observaciones Pareadas</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-univariado"><i class="fa fa-check"></i><b>4.7.1</b> Caso Univariado</a></li>
<li class="chapter" data-level="4.7.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-multivariado"><i class="fa fa-check"></i><b>4.7.2</b> Caso Multivariado</a></li>
<li class="chapter" data-level="4.7.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#uso-de-contrastes-para-la-comparación-de-medias-en-muestras-pareadas"><i class="fa fa-check"></i><b>4.7.3</b> Uso de Contrastes para la Comparación de Medias en Muestras Pareadas</a></li>
<li class="chapter" data-level="4.7.4" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#un-diseño-de-medidas-repetidas-para-comparar-tratamientos"><i class="fa fa-check"></i><b>4.7.4</b> Un Diseño de Medidas Repetidas para Comparar Tratamientos</a></li>
<li class="chapter" data-level="4.7.5" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#análisis-de-perfiles"><i class="fa fa-check"></i><b>4.7.5</b> Análisis de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html"><i class="fa fa-check"></i><b>4.8</b> Inferencia para la Matriz de Varianzas Covarianza</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueva-de-rv-sigma"><i class="fa fa-check"></i><b>4.8.1</b> Pruba de Razón de Verosimilitud</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueba-de-bartlet"><i class="fa fa-check"></i><b>4.8.2</b> Prueba de Bartlet</a></li>
<li class="chapter" data-level="4.8.3" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#dos-o-más-matrices-de-varianzas-covarianzas"><i class="fa fa-check"></i><b>4.8.3</b> Dos o más Matrices de Varianzas Covarianzas</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><i class="fa fa-check"></i><b>4.9</b> Regiones de Confianza y Comparaciones Simultáneas entre las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#construcción-de-una-región-de-confianza-para-underlineboldsymbol-mu-cuando-la-población-tiene-distribución-n_punderlineboldsymbol-mumathbfsigma-con-underlineboldsymbol-mu-y-mathbfsigma-desconocidos"><i class="fa fa-check"></i><b>4.9.1</b> Construcción de una Región de Confianza para <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> Cuando la Población tiene Distribución <span class="math inline">\(N_p(\underline{\boldsymbol \mu},\mathbf{\Sigma})\)</span>, con <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span> desconocidos</a></li>
<li class="chapter" data-level="4.9.2" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#región-de-confianza-para-el-caso-de-p2-elipse-de-confianza"><i class="fa fa-check"></i><b>4.9.2</b> Región de Confianza para el Caso de <span class="math inline">\(p=2\)</span> (Elipse de Confianza)</a></li>
<li class="chapter" data-level="4.9.3" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-para-las-componentes-del-vector-de-medias-underlineboldsymbol-mu"><i class="fa fa-check"></i><b>4.9.3</b> Intervalos de Confianza Simultáneos para las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a></li>
<li class="chapter" data-level="4.9.4" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-t2-simultáneos-para-diferencias-de-medias"><i class="fa fa-check"></i><b>4.9.4</b> IC <span class="math inline">\(T^2\)</span> Simultáneos para Diferencias de Medias</a></li>
<li class="chapter" data-level="4.9.5" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#IC-Bonferroni"><i class="fa fa-check"></i><b>4.9.5</b> Método de Bonferroni para Comparaciones Múltiples</a></li>
<li class="chapter" data-level="4.9.6" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-chi2-caso-n-grande"><i class="fa fa-check"></i><b>4.9.6</b> Intervalos de Confianza Simultáneos <span class="math inline">\(\chi^2\)</span> (caso n-Grande)</a></li>
<li class="chapter" data-level="4.9.7" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-simultáneos-para-n-grande-usando-la-normal-estándar-z_alpha"><i class="fa fa-check"></i><b>4.9.7</b> IC Simultáneos para n-Grande Usando la Normal Estándar <span class="math inline">\(Z_\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mrlm.html"><a href="mrlm.html"><i class="fa fa-check"></i><b>5</b> Modelos de Regresión Lineal Multivariados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introducción-2.html"><a href="introducción-2.html"><i class="fa fa-check"></i><b>5.1</b> Introducción</a></li>
<li class="chapter" data-level="5.2" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>5.2</b> Modelo de Regresión Lineal Múltiple (RLM)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="rlm.html"><a href="rlm.html#estimación-de-mínimos-cuadrados"><i class="fa fa-check"></i><b>5.2.1</b> Estimación de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="5.2.2" data-path="rlm.html"><a href="rlm.html#inferencias-acerca-del-modelo-de-regresión-lineal-múltiple"><i class="fa fa-check"></i><b>5.2.2</b> Inferencias Acerca del Modelo de Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="5.2.3" data-path="rlm.html"><a href="rlm.html#inferencias-a-partir-de-la-función-de-regresión-estimada"><i class="fa fa-check"></i><b>5.2.3</b> Inferencias A partir de la Función de Regresión Estimada</a></li>
<li class="chapter" data-level="5.2.4" data-path="rlm.html"><a href="rlm.html#validación-de-los-supuestos-del-modelo-y-otros-aspectos-del-la-regresión"><i class="fa fa-check"></i><b>5.2.4</b> Validación de los Supuestos del Modelo y Otros Aspectos del la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html"><i class="fa fa-check"></i><b>5.3</b> Modelo de Regresión Lineal Multivariado (RL-Multivariado)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mrl-multiv.html"><a href="mrl-multiv.html#algunas-notaciones-1"><i class="fa fa-check"></i><b>5.3.1</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="5.3.2" data-path="mrl-multiv.html"><a href="mrl-multiv.html#mrl-multivariado-en-forma-matricial"><i class="fa fa-check"></i><b>5.3.2</b> MRL-Multivariado en forma Matricial</a></li>
<li class="chapter" data-level="5.3.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html#m-mrl-múltiples"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(m\)</span>-MRL-Múltiples</a></li>
<li class="chapter" data-level="5.3.4" data-path="mrl-multiv.html"><a href="mrl-multiv.html#estimador-de-mínimos-cuadrados-de-los-parámetros"><i class="fa fa-check"></i><b>5.3.4</b> Estimador de Mínimos Cuadrados de los Parámetros</a></li>
<li class="chapter" data-level="5.3.5" data-path="mrl-multiv.html"><a href="mrl-multiv.html#propiedades-de-estimadores-de-mínimos-cuadrados-del-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.5</b> Propiedades de Estimadores de Mínimos Cuadrados del MRL-Multivariado</a></li>
<li class="chapter" data-level="5.3.6" data-path="mrl-multiv.html"><a href="mrl-multiv.html#prv-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.6</b> Prueba de Razón de Verosimilitud para Parámetros de Regresión en MRL-Multivariados</a></li>
<li class="chapter" data-level="5.3.7" data-path="mrl-multiv.html"><a href="mrl-multiv.html#otras-pruebas-estadísticas-multivariadas"><i class="fa fa-check"></i><b>5.3.7</b> Otras Pruebas Estadísticas Multivariadas</a></li>
<li class="chapter" data-level="5.3.8" data-path="mrl-multiv.html"><a href="mrl-multiv.html#predicciones-a-partir-de-un-modelo-de-regresión-múltiple-multivariado"><i class="fa fa-check"></i><b>5.3.8</b> Predicciones A Partir de un Modelo de Regresión Múltiple Multivariado</a></li>
<li class="chapter" data-level="5.3.9" data-path="mrl-multiv.html"><a href="mrl-multiv.html#el-concepto-de-regresión-lineal"><i class="fa fa-check"></i><b>5.3.9</b> El Concepto de Regresión Lineal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ACP.html"><a href="ACP.html"><i class="fa fa-check"></i><b>6</b> Análisis de Componentes Principales (ACP)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducción-3.html"><a href="introducción-3.html"><i class="fa fa-check"></i><b>6.1</b> Introducción</a></li>
<li class="chapter" data-level="6.2" data-path="interpretaciones-geométricas-y-algebraícas-del-acp.html"><a href="interpretaciones-geométricas-y-algebraícas-del-acp.html"><i class="fa fa-check"></i><b>6.2</b> Interpretaciones Geométricas y Algebraícas del ACP</a></li>
<li class="chapter" data-level="6.3" data-path="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><a href="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><i class="fa fa-check"></i><b>6.3</b> Interpretación Geométrica del ACP Mediante un Ejemplo Simple <span class="math inline">\(p=2\)</span></a></li>
<li class="chapter" data-level="6.4" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html"><i class="fa fa-check"></i><b>6.4</b> Mas de dos Ejes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#min-cuadrados"><i class="fa fa-check"></i><b>6.4.1</b> Ajuste de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="6.4.2" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#relación-entre-los-sub-espacios-de-mathbbrp-y-de-mathbbrn"><i class="fa fa-check"></i><b>6.4.2</b> Relación entre los Sub-espacios de <span class="math inline">\(\mathbb{R}^p\)</span> y de <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="componentes-principales-en-análisis-de-regresión.html"><a href="componentes-principales-en-análisis-de-regresión.html"><i class="fa fa-check"></i><b>6.5</b> Componentes Principales en Análisis de Regresión</a></li>
<li class="chapter" data-level="6.6" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html"><i class="fa fa-check"></i><b>6.6</b> Componentes Principales Poblacionales</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#definicón-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.1</b> Definicón de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.2" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#determinación-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.2</b> Determinación de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.3" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-derivadas-de-una-normal-multivariada"><i class="fa fa-check"></i><b>6.6.3</b> Componentes Principales Derivadas de una Normal Multivariada</a></li>
<li class="chapter" data-level="6.6.4" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-usando-variables-estandarizadas"><i class="fa fa-check"></i><b>6.6.4</b> Componentes Principales usando Variables Estandarizadas</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><i class="fa fa-check"></i><b>6.7</b> Componentes Principales para Matrices de Var-Cov <span class="math inline">\(\mathbf{\Sigma}\)</span> con Estructuras Especiales</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#variables-no-correlacionadas"><i class="fa fa-check"></i><b>6.7.1</b> Variables No-Correlacionadas</a></li>
<li class="chapter" data-level="6.7.2" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#var-correlacionadas"><i class="fa fa-check"></i><b>6.7.2</b> Variables Altamente Correlacionadas</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="componentes-principales-muestrales.html"><a href="componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.8</b> Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.9" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html"><i class="fa fa-check"></i><b>6.9</b> Algunos Ejemplos de ACP</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo1"><i class="fa fa-check"></i><b>6.9.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.9.2" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-2"><i class="fa fa-check"></i><b>6.9.2</b> Ejemplo-2</a></li>
<li class="chapter" data-level="6.9.3" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-3"><i class="fa fa-check"></i><b>6.9.3</b> Ejemplo-3</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="cómo-elegir-el-número-de-componentes-principales.html"><a href="cómo-elegir-el-número-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.10</b> Cómo elegir el número de Componentes Principales?</a></li>
<li class="chapter" data-level="6.11" data-path="interpretación-de-las-componentes-principales-muestrales.html"><a href="interpretación-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.11</b> Interpretación de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.12" data-path="estandarización-de-las-componentes-principales-muestrales.html"><a href="estandarización-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.12</b> Estandarización de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.13" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.13</b> Gráficas en un Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="6.13.1" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#graficos-de-las-cps"><i class="fa fa-check"></i><b>6.13.1</b> Graficos de las CPS</a></li>
<li class="chapter" data-level="6.13.2" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#el-gráfico-biplot"><i class="fa fa-check"></i><b>6.13.2</b> El gráfico biplot:</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><a href="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><i class="fa fa-check"></i><b>6.14</b> Propiedades de <span class="math inline">\(\hat{\lambda}_i\)</span> y <span class="math inline">\(\underline{\hat{\mathbf{e}}}_i\)</span> en Muestras Grandes</a></li>
<li class="chapter" data-level="6.15" data-path="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><a href="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><i class="fa fa-check"></i><b>6.15</b> Prueba de Hipótesis para Estructura de Correlación Igual</a></li>
<li class="chapter" data-level="6.16" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html"><i class="fa fa-check"></i><b>6.16</b> Ejemplos Finales</a>
<ul>
<li class="chapter" data-level="6.16.1" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-1"><i class="fa fa-check"></i><b>6.16.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.16.2" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-acp-con-individuos-y-variables-suplementarias"><i class="fa fa-check"></i><b>6.16.2</b> Ejemplo ACP con Individuos y Variables Suplementarias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="affc.html"><a href="affc.html"><i class="fa fa-check"></i><b>7</b> Análisis Factorial de Factores Comunes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introducción-4.html"><a href="introducción-4.html"><i class="fa fa-check"></i><b>7.1</b> Introducción</a></li>
<li class="chapter" data-level="7.2" data-path="tipos-de-factores.html"><a href="tipos-de-factores.html"><i class="fa fa-check"></i><b>7.2</b> Tipos de Factores</a></li>
<li class="chapter" data-level="7.3" data-path="motivación-del-análisis-factorial.html"><a href="motivación-del-análisis-factorial.html"><i class="fa fa-check"></i><b>7.3</b> Motivación del Análisis Factorial</a></li>
<li class="chapter" data-level="7.4" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html"><i class="fa fa-check"></i><b>7.4</b> Enfoques del AF</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#métodos-para-realizar-la-extracción-de-los-factores"><i class="fa fa-check"></i><b>7.4.1</b> Métodos para realizar la extracción de los factores</a></li>
<li class="chapter" data-level="7.4.2" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#rotación-de-ejes"><i class="fa fa-check"></i><b>7.4.2</b> Rotación de Ejes</a></li>
<li class="chapter" data-level="7.4.3" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#puntuaciones-o-scores-de-los-sujetos-en-los-factores-extraídos"><i class="fa fa-check"></i><b>7.4.3</b> Puntuaciones o Scores de los Sujetos en los Factores Extraídos</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html"><i class="fa fa-check"></i><b>7.5</b> El Modelo de Factor Ortogonal</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#estructura-de-covarianzas-del-modelo-de-factor-ortogonal"><i class="fa fa-check"></i><b>7.5.1</b> Estructura de Covarianzas del Modelo de Factor Ortogonal</a></li>
<li class="chapter" data-level="7.5.2" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#ejemplos"><i class="fa fa-check"></i><b>7.5.2</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html"><i class="fa fa-check"></i><b>7.6</b> Métodos de Estimación</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-cp"><i class="fa fa-check"></i><b>7.6.1</b> El Método de la Componente Principal</a></li>
<li class="chapter" data-level="7.6.2" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-pa"><i class="fa fa-check"></i><b>7.6.2</b> Una Aproximación Modificada (La Solución del Factor Principal o de las CP-Iteradas o de Ejes Principales-PA)</a></li>
<li class="chapter" data-level="7.6.3" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-mle"><i class="fa fa-check"></i><b>7.6.3</b> El Método de Máxima Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="prueba-para-el-número-de-factores-muestra-grande..html"><a href="prueba-para-el-número-de-factores-muestra-grande..html"><i class="fa fa-check"></i><b>7.7</b> Prueba para el Número de Factores (Muestra Grande).</a></li>
<li class="chapter" data-level="7.8" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html"><i class="fa fa-check"></i><b>7.8</b> Rotación de Factores</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-cuando-m2-factores"><i class="fa fa-check"></i><b>7.8.1</b> Rotaciones cuando <span class="math inline">\(m=2\)</span>-Factores</a></li>
<li class="chapter" data-level="7.8.2" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#criterio-varimáx"><i class="fa fa-check"></i><b>7.8.2</b> Criterio Varimáx:</a></li>
<li class="chapter" data-level="7.8.3" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>7.8.3</b> Rotaciones Oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html"><i class="fa fa-check"></i><b>7.9</b> Factores-Scores (o Puntuaciones) de los Factores</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#método-de-los-mínimos-cuadrados-ponderados"><i class="fa fa-check"></i><b>7.9.1</b> Método de los Mínimos Cuadrados Ponderados</a></li>
<li class="chapter" data-level="7.9.2" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#el-método-de-la-regresión"><i class="fa fa-check"></i><b>7.9.2</b> El Método de la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><a href="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><i class="fa fa-check"></i><b>7.10</b> Perspectivas y Estrategías para el Análisis de Factor</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="avm.html"><a href="avm.html"><i class="fa fa-check"></i><b>8</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introducción-5.html"><a href="introducción-5.html"><i class="fa fa-check"></i><b>8.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introducción-5.html"><a href="introducción-5.html#suposiciones-del-manova"><i class="fa fa-check"></i><b>8.1.1</b> Suposiciones del MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>8.2</b> Análisis de Varianza Univariado (ANOVA)</a></li>
<li class="chapter" data-level="8.3" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>8.3</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="manova.html"><a href="manova.html#modelo-manova-para-comparar-g-vectores-de-medias-poblacionales"><i class="fa fa-check"></i><b>8.3.1</b> Modelo MANOVA para comparar <span class="math inline">\(g\)</span>-Vectores de Medias Poblacionales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adiscriminante.html"><a href="adiscriminante.html"><i class="fa fa-check"></i><b>9</b> Análisis Discriminante y Clasificación</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introducción-6.html"><a href="introducción-6.html"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.2</b> Separación y Clasificación para el caso de dos poblaciones</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html#clasificación-de-dos-poblaciones"><i class="fa fa-check"></i><b>9.2.1</b> Clasificación de dos Poblaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.3</b> Regla de Discriminación para dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#costo-esperado-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.1</b> Costo Esperado de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.2" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#probabilidad-total-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.2</b> Probabilidad Total de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#regla-de-clasificación-de-bayes"><i class="fa fa-check"></i><b>9.3.3</b> Regla de Clasificación de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html"><i class="fa fa-check"></i><b>9.4</b> Clasificación con Dos Poblaciones Normales Multivariadas</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1mathbfsigma_2mathbfsigma"><i class="fa fa-check"></i><b>9.4.1</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\mathbf{\Sigma}\)</span></a></li>
<li class="chapter" data-level="9.4.2" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1neq-mathbfsigma_2"><i class="fa fa-check"></i><b>9.4.2</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1\neq \mathbf{\Sigma}_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html"><i class="fa fa-check"></i><b>9.5</b> Evaluación de las Funciones de Clasificación</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-óptimo-teo"><i class="fa fa-check"></i><b>9.5.1</b> Tasa de Error Óptimo (TEO)</a></li>
<li class="chapter" data-level="9.5.2" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual"><i class="fa fa-check"></i><b>9.5.2</b> Tasa de Error Actual</a></li>
<li class="chapter" data-level="9.5.3" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-aparente-teap"><i class="fa fa-check"></i><b>9.5.3</b> Tasa de Error Aparente (TEAP)</a></li>
<li class="chapter" data-level="9.5.4" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual-esperada"><i class="fa fa-check"></i><b>9.5.4</b> Tasa de Error Actual Esperada</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.6</b> Clasificación con Varias Poblaciones (Es decir Más de dos Poblaciones)</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html#costo-esperado-de-mal-clasificación-ecm"><i class="fa fa-check"></i><b>9.6.1</b> Costo Esperado de Mal Clasificación (ECM)</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.7</b> Clasificación con Poblaciones Normales y más de dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#un-clasificador-equivalente-para-el-caso-de-matrices-de-var-cov-iguales"><i class="fa fa-check"></i><b>9.7.1</b> Un Clasificador Equivalente para el Caso de Matrices de Var-Cov Iguales</a></li>
<li class="chapter" data-level="9.7.2" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#comparación-dos-a-dos-de-los-scores-de-la-función-de-clasificación-lineal"><i class="fa fa-check"></i><b>9.7.2</b> Comparación Dos a Dos de los Scores de la Función de Clasificación Lineal</a></li>
<li class="chapter" data-level="9.7.3" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#tasa-de-error-actual-esperada-aer-estimada"><i class="fa fa-check"></i><b>9.7.3</b> Tasa de Error Actual Esperada (AER) Estimada</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><i class="fa fa-check"></i><b>9.8</b> Método de Fisher Para Discriminar Entre Varias Poblaciones</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#uso-de-la-función-de-discriminación-de-fisher-para-clasificación"><i class="fa fa-check"></i><b>9.8.1</b> Uso de la Función de Discriminación de Fisher Para Clasificación</a></li>
<li class="chapter" data-level="9.8.2" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#relación-entre-la-regla-de-clasificación-de-fisher-y-las-funciones-de-discriminación-de-la-teoría-normal"><i class="fa fa-check"></i><b>9.8.2</b> Relación entre la Regla de Clasificación de Fisher y las Funciones de Discriminación de la Teoría Normal</a></li>
<li class="chapter" data-level="9.8.3" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#regla-de-clasificación-de-fisher-basado-en-discriminantes-muestrales"><i class="fa fa-check"></i><b>9.8.3</b> Regla de Clasificación de Fisher Basado en Discriminantes Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html"><i class="fa fa-check"></i><b>9.9</b> Regresión Logística y Clasificación</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#el-modelo-logit"><i class="fa fa-check"></i><b>9.9.1</b> El Modelo Logit</a></li>
<li class="chapter" data-level="9.9.2" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#análisis-de-regresión-logística"><i class="fa fa-check"></i><b>9.9.2</b> Análisis de Regresión Logística</a></li>
<li class="chapter" data-level="9.9.3" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#regresión-logística-con-respuestas-binomiales"><i class="fa fa-check"></i><b>9.9.3</b> Regresión Logística con Respuestas Binomiales</a></li>
<li class="chapter" data-level="9.9.4" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#chequeo-del-modelo"><i class="fa fa-check"></i><b>9.9.4</b> Chequeo del Modelo</a></li>
<li class="chapter" data-level="9.9.5" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#prueba-de-residuales-y-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>9.9.5</b> Prueba de Residuales y de Bondad de Ajuste</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="acluster.html"><a href="acluster.html"><i class="fa fa-check"></i><b>10</b> Análisis de Agrupamiento o Cluster</a>
<ul>
<li class="chapter" data-level="10.1" data-path="introducción-7.html"><a href="introducción-7.html"><i class="fa fa-check"></i><b>10.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2" data-path="consideraciones-iniciales.html"><a href="consideraciones-iniciales.html"><i class="fa fa-check"></i><b>10.2</b> Consideraciones Iniciales</a></li>
<li class="chapter" data-level="10.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html"><i class="fa fa-check"></i><b>10.3</b> Medidas de Similaridad entre Pares de Observaciones</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#medidas-de-distancia"><i class="fa fa-check"></i><b>10.3.1</b> Medidas de Distancia</a></li>
<li class="chapter" data-level="10.3.2" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-de-correlación-entre-casos"><i class="fa fa-check"></i><b>10.3.2</b> Coeficientes de Correlación (entre casos)</a></li>
<li class="chapter" data-level="10.3.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-binarios-de-asociación-o-similaridad-entre-observaciones"><i class="fa fa-check"></i><b>10.3.3</b> Coeficientes Binarios de Asociación o Similaridad Entre Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><a href="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><i class="fa fa-check"></i><b>10.4</b> Medidas de Similaridad o Asociación para Pares de Variables</a></li>
<li class="chapter" data-level="10.5" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html"><i class="fa fa-check"></i><b>10.5</b> Métodos Jerárquicos de Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-enlaces-para-agupamientos-aglomerativos"><i class="fa fa-check"></i><b>10.5.1</b> Métodos Jerarquicos de Enlaces para Agupamientos Aglomerativos</a></li>
<li class="chapter" data-level="10.5.2" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-agrupamientos-desaglomerativos"><i class="fa fa-check"></i><b>10.5.2</b> Métodos Jerarquicos de Agrupamientos Desaglomerativos</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><i class="fa fa-check"></i><b>10.6</b> Métodos NO-Jerárquicos de Partición o Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#pasos-o-etapas-de-un-método-de-clasificación-no-jararquico"><i class="fa fa-check"></i><b>10.6.1</b> Pasos o etapas de un Método de Clasificación No-Jararquico</a></li>
<li class="chapter" data-level="10.6.2" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#método-de-las-k-medias-o-k-means"><i class="fa fa-check"></i><b>10.6.2</b> Método de las <span class="math inline">\(k\)</span>-Medias o <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="cómo-determinar-el-número-apropiado-de-conglomerados.html"><a href="cómo-determinar-el-número-apropiado-de-conglomerados.html"><i class="fa fa-check"></i><b>10.7</b> Cómo determinar el número apropiado de conglomerados?</a></li>
<li class="chapter" data-level="10.8" data-path="agrupamientos-basados-en-modelos-estadísticos.html"><a href="agrupamientos-basados-en-modelos-estadísticos.html"><i class="fa fa-check"></i><b>10.8</b> Agrupamientos Basados en Modelos Estadísticos</a></li>
<li class="chapter" data-level="10.9" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html"><i class="fa fa-check"></i><b>10.9</b> Escalamiento Multidimensional</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html#el-algoritmo-básico"><i class="fa fa-check"></i><b>10.9.1</b> El Algoritmo Básico</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html"><i class="fa fa-check"></i><b>10.10</b> Análisis de Correspondencia</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#desarrollo-algebraíco-del-análsisi-de-correspondencia"><i class="fa fa-check"></i><b>10.10.1</b> Desarrollo Algebraíco del Análsisi de Correspondencia</a></li>
<li class="chapter" data-level="10.10.2" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-como-un-problema-de-mínimos-cuadrados-ponderado"><i class="fa fa-check"></i><b>10.10.2</b> Análisis de Correspondencia como un Problema de Mínimos Cuadrados Ponderado</a></li>
<li class="chapter" data-level="10.10.3" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-medinate-el-método-de-aproximación-de-perfiles"><i class="fa fa-check"></i><b>10.10.3</b> Análisis de Correspondencia Medinate el Método de Aproximación de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><i class="fa fa-check"></i><b>10.11</b> Biplots para la Visualización de Unidades Muestrales y Variables</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html#construcción-de-un-biplot"><i class="fa fa-check"></i><b>10.11.1</b> Construcción de un Biplot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Chapter 10</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="evaluación-de-las-funciones-de-clasificación" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Evaluación de las Funciones de Clasificación<a href="evaluación-de-las-funciones-de-clasificación.html#evaluación-de-las-funciones-de-clasificación" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Una forma importante de juzgar el desempeño de cualesquier proceso de clasificación es calcular sus <em>Tasas de Errores o Probabilidades de Mal Clasificación</em>. Cuando las formas de las poblaciones son completamente conocidas, las probabilidades de mal clasificación pueden ser calculadas con relativa facilidad. Debido a que raramente las poblaciones de origen son conocidas, nos concentraremos en las <strong>Tasas de Errores Asociadas con la Función de Clasificación Muestral</strong>.</p>
<p>Una ves la función de clasificación está construida, nos interesa contar con una medida de su desempeño en muestras futuras.</p>
<p>De la ecuación <a href="regla-de-discriminación-para-dos-poblaciones.html#eq:proba-total-mal-clasificacion">(9.8)</a>, se tiene que <em>La Probabilidad Total de Mal Clasificación (PTM)</em>, está dada por:
<span class="math display">\[
PTM=p_1 \int_{R_2} f_1( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}+
p_2 \int_{R_1} f_2( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}
\]</span></p>
<div id="tasa-de-error-óptimo-teo" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Tasa de Error Óptimo (TEO)<a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-óptimo-teo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>El valor mas pequeños de esta cantidad</strong>, obtenido mediante una elección juiciosa de <span class="math inline">\(\mathcal{R}_1\)</span> y <span class="math inline">\(\mathcal{R}_2\)</span>, se llama <strong>Tasa de Error Óptimo (TEO)</strong>, es decir
<span class="math display" id="eq:tasa-de-error-optimo">\[
\begin{equation}
TEO=Min\biggl\{ p_1 \int_{R_2} f_1( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}+
p_2 \int_{R_1} f_2( \underline{\mathbf{x}} )
d\underline{\mathbf{x}} \biggr\}
\end{equation}
\tag{9.25}
\]</span></p>
<p>donde <span class="math inline">\(R_1\)</span> y <span class="math inline">\(R_2\)</span> son regiones determinadas con el Mínimo Error Esperado de Mala Clasificación (CEM). Así, la Tasa de Error Óptimo (TEO) dada en <a href="evaluación-de-las-funciones-de-clasificación.html#eq:tasa-de-error-optimo">(9.25)</a>, es la <em>Tasa de Error</em> para la Regla de Clasificación con Mínima Probabilidad Total de Mala Clasificación (PTM).</p>
<div class="example">
<p><span id="exm:ejemplo1-tasa-error-optima" class="example"><strong>Ejemplo 9.5  (Calculo de Probabilidades de Mal CLasificación (PTM) y Tasa de Error Óptima (TEO)) </strong></span>En este ejemplo se calculan la Probabilidad Total de Mal Clasificación (PTM) y Tasa de Error Óptima (TEO), para el caso de dos poblaciones normales con matrices de Varianzas Covarianzas iguales, costos de mal clasificación iguales y probabilidades Aprioris iguales.</p>
</div>
<p>Se derivará una expresión para la TEO cuando <span class="math inline">\(p_1=p_2=\frac{1}{2}\)</span>, <span class="math inline">\(f_1(\underline{\mathbf{x}})\)</span> y <span class="math inline">\(f_2(\underline{\mathbf{x}})\)</span> densidades normales multivariadas con <span class="math inline">\(\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\mathbf{\Sigma}\)</span>.</p>
<p>Las reglas de clasificación con mínimo Error Esperado de Mala Clasificación (CEM) y mínima Probabilidad Total de Mala Clasificación (PTM) coinciden cuando <span class="math inline">\(c(1\ |\ 2)=c(2\ |\ 1)\)</span>.</p>
<p>Ahora, como en este caso las Probabilidades A Prioris también son iguales, entonces las reglas de clasificación con mínima Probabilidad Total de Mala Clasificación (PTM), para poblaciones normales son definidas por:
<span class="math display">\[
\underline{\mathbf{x}}_0 \in \pi_1: \ \text{Si}: \ \ (\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t \mathbf{\Sigma}^{-1}\underline{\mathbf{x}}_0 -\frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t \mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} ) \geq Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]
\]</span></p>
<p><span class="math inline">\(\underline{\mathbf{x}}_0 \in \pi_2\)</span>, en caso contrario.</p>
<p>Con
<span class="math display">\[
Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]=0.
\]</span></p>
<p>Es decir,
<span class="math display">\[
R_1: \ \ \ \ (\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t
\mathbf{\Sigma}^{-1}\underline{\mathbf{x}} -\frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} )
  \geq 0
\]</span></p>
<p><span class="math display">\[
R_2: \ \ \ \ (\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t
\mathbf{\Sigma}^{-1}\underline{\mathbf{x}} -\frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} )
  &lt; 0
\]</span></p>
<p>Dichos conjuntos pueden ser expresados en términos de:
<span class="math display">\[
y=(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t
\mathbf{\Sigma}^{-1}\underline{\mathbf{x}}=\mathbf{\underline{a}}^t\underline{\mathbf{x}},
\]</span></p>
<p>como sigue:
<span class="math display">\[
R_1(y): \ \ \ \ y \geq \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} )
\]</span></p>
<p><span class="math display">\[
R_2(y): \ \ \ \ y &lt; \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} )
\]</span></p>
<p>Pero, <span class="math inline">\(Y=\mathbf{\underline{a}}^t\underline{\mathbf{x}}\)</span> es una combinación lineal de variables aleatorias normales, por lo que las funciones de densidades de probabilidad de <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_1(y)\)</span> y <span class="math inline">\(f_2(y)\)</span>, son normales univariadas con medias y varianzas dadas por:
<span class="math display">\[
\mu_{1Y}=\mathbf{\underline{a}}^t\underline{\boldsymbol \mu}_{\ 1}=(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}\underline{\boldsymbol \mu}_{\ 1}
\]</span></p>
<p><span class="math display">\[
\mu_{2Y}=\mathbf{\underline{a}}^t\underline{\boldsymbol \mu}_{\ 2}=(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}\underline{\boldsymbol \mu}_{\ 2}
\]</span></p>
<p><span class="math display">\[
\sigma_Y^2=\mathbf{\underline{a}}^t\mathbf{\Sigma}\mathbf{\underline{a}}=(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})=\Delta^2
\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:graf-dispersion3-ejemplo-pob-nm"></span>
<img src="imagenes/graf48.png" alt="Funciones de Densidad de f1(y) y f2(y)" width="50%" />
<p class="caption">
Figura 9.5: Funciones de Densidad de f1(y) y f2(y)
</p>
</div>
<p>Ahora,
<span class="math display">\[
\begin{align*}
PTM&amp;=p_1 \int_{R_2} f_1( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}+
p_2 \int_{R_1} f_2( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}\\ \\
&amp;=\frac{1}{2}\ P[\text{Clasificar una Obs. de}\ \pi_1 \  \text{como de}\ \pi_2]\\ &amp; + \frac{1}{2}\ P[\text{Clasificar una Obs. de}\ \pi_2\ \text{como de}\ \pi_1]\\ \\
&amp;=\frac{1}{2}\ P(2\ |\ 1) + \frac{1}{2}\ P(1\ |\ 2)
\end{align*}
\]</span></p>
<p>pero a partir de la figura se tiene que:
<span class="math display">\[
\begin{align*}
P(2\ |\ 1)&amp;=P[\text{Clasificar una Obs. de}\ \pi_1 \  \text{como de}\ \pi_2]\\ \\
&amp;=P \left[Y &lt; \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} ) \right]\\ \\
&amp;= P \left[\frac{ Y - \mu_{1Y} }{\sigma_Y}&lt; \frac{ \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} ) - (\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}\underline{\boldsymbol \mu}_{\ 1}  }{\sigma_Y} \right]\\ \\
&amp;=P\left(Z &lt; \frac{-\frac{1}{2}\Delta^2}{\Delta} \right)=\Phi\left( -\frac{\Delta}{2} \right)
\end{align*}
\]</span></p>
<p>con <span class="math inline">\(\Phi(.)\)</span>-función de distribución acumulada de la normal estándar.</p>
<p>Similarmente se obtiene que,</p>
<p><span class="math display">\[
\begin{align*}
P(1\ |\ 2)&amp;=P[\text{Clasificar una Obs. de}\ \pi_2 \  \text{como de}\ \pi_1]\\ \\
&amp;=P \left[Y \geq \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} ) \right]\\ \\
&amp;= P \left[\frac{ Y - \mu_{2Y} }{\sigma_Y} \geq \frac{ \frac{1}{2} (\underline{\boldsymbol \mu}_{\ 1} -\underline{\boldsymbol \mu}_{\ 2} )^t
\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1} + \underline{\boldsymbol \mu}_{\ 2} ) - (\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}\underline{\boldsymbol \mu}_{\ 2}  }{\sigma_Y} \right]\\ \\
&amp;=P\left(Z \geq \frac{-\frac{1}{2}\Delta^2}{\Delta} \right)\\ \\
&amp;=1- \Phi\left( \frac{\Delta}{2} \right)= \Phi\left( -\frac{\Delta}{2} \right)
\end{align*}
\]</span></p>
<p>Por lo tanto, la Tasa de Error Óptimo (TEO) esta dada por:
<span class="math display" id="eq:tasa-error-optimo2">\[
\begin{equation}
TEO=\text{Mínimo}\biggl\{ PTM \biggr\} = \frac{1}{2}\Phi\left( -\frac{\Delta}{2} \right) + \frac{1}{2}\Phi\left( -\frac{\Delta}{2} \right)=\Phi\left( -\frac{\Delta}{2} \right)
\end{equation}
\tag{9.26}
\]</span></p>
<p><strong>Si por ejemplo:</strong>
<span class="math display">\[
\sigma_Y^2=\Delta^2=\mathbf{\underline{a}}^t\mathbf{\Sigma}\mathbf{\underline{a}}=(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})^t\mathbf{\Sigma}^{-1}(\underline{\boldsymbol \mu}_{\ 1}-\underline{\boldsymbol \mu}_{\ 2})=2.56, \ \ \ \text{es decir}, \ \ \Delta=\sqrt{2.56}=1.6
\]</span></p>
<p>luego,
<span class="math display">\[
TEO=\text{Mínimo}\biggl\{ PTM \biggr\}=\Phi \left(-\frac{1.6}{2} \right)=\Phi(-0.8)=0.2119,
\]</span></p>
<p>es decir que, la <em>Regla de Clasificación Óptima</em>, clasificará incorrectamente alrededor del <span class="math inline">\(21\%\)</span> de las observaciones a una población u otra.</p>
</div>
<div id="tasa-de-error-actual" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> Tasa de Error Actual<a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si se da lo que usualmente sucede en la práctica, es decir que ciertos parámetros que aparecen en las Reglas de Clasificación son desconocidos y por lo tanto deben estimarse a partir de la muestra, entonces la evaluación de las Tasas de Errores no es tan directa. En estos casos, el desempeño de las <em>Funciones de Clasificación Muestral</em>, puede en principio, se evaluada calculando la <strong>Tasa de Error Actual (TEA)</strong>, definida por:
<span class="math display" id="eq:tasa-error-actual">\[
\begin{equation}
TEA=p_1 \int_{\widehat{R\ }_2} f_1( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}+
p_2 \int_{\widehat{R\ }_1} f_2( \underline{\mathbf{x}} )
d\underline{\mathbf{x}}
\end{equation}
\tag{9.27}
\]</span></p>
<p>donde,<span class="math inline">\(\widehat{R\ }_1\)</span> y <span class="math inline">\(\widehat{R\ }_2\)</span> representan las regiones de clasificación determinadas por las muestras de tamaño <span class="math inline">\(n_1\)</span> y <span class="math inline">\(n_2\)</span>, respectivamente.</p>
<p>Por ejemplo, si la función de clasificación empleada es la definida en <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm">(9.18)</a>, dada por:
<span class="math display">\[
\underline{\mathbf{x}}_0 \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_1-\overline{\underline{\mathbf{x}}}_2)^t
\mathbf{S}_{Pol}^{-1}\underline{\mathbf{x}}_0 -\frac{1}{2} (\overline{\underline{\mathbf{x}}}_1 -\overline{\underline{\mathbf{x}}}_2 )^t
\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_1 + \overline{\underline{\mathbf{x}}}_2 ) \\
  \geq Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]
\]</span></p>
<p>entonces, las regiones <span class="math inline">\(\widehat{R}_1\)</span> y <span class="math inline">\(\widehat{R}_2\)</span> son definidas por el conjunto de <span class="math inline">\(\underline{\mathbf{x}}\)</span> para los cuales se cumplen las siguientes desigualdades:
<span class="math display">\[
\widehat{R\ }_1: \ \ (\overline{\underline{\mathbf{x}}}_1-\overline{\underline{\mathbf{x}}}_2)^t\
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}_0 -\frac{1}{2} (\overline{\underline{\mathbf{x}}}_1 -\overline{\underline{\mathbf{x}}}_2 )^t\
\mathbf{S}_{Pol}^{-1}\ (\overline{\underline{\mathbf{x}}}_1 + \overline{\underline{\mathbf{x}}}_2 ) \\  
  \geq Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]
\]</span></p>
<p><span class="math display">\[
\widehat{R\ }_2: \ \ (\overline{\underline{\mathbf{x}}}_1-\overline{\underline{\mathbf{x}}}_2)^t\
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}_0 -\frac{1}{2} (\overline{\underline{\mathbf{x}}}_1 -\overline{\underline{\mathbf{x}}}_2 )^t\
\mathbf{S}_{Pol}^{-1}\ (\overline{\underline{\mathbf{x}}}_1 + \overline{\underline{\mathbf{x}}}_2 ) \\  
  &lt; Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]
\]</span></p>
<p>La Tasa de Error Actual (TEA) indica como se comportará la <em>Función de Clasificación Muestral</em> en muestras futuras.</p>
<p>AL igual que La Tasa de Error Óptimo (TEO), La Tasa de Error Actual (TEA), no puede en general, ser calculada, debido a que depende de las funciones de densidad desconocidas <span class="math inline">\(f_1(\underline{\mathbf{x}})\)</span> y <span class="math inline">\(f_2(\underline{\mathbf{x}})\)</span>. Sin embargo, una estimación de una cantidad relacionada a La Tasa de Error Actual (TEA) puede ser calculada, como se indica a continuación.</p>
</div>
<div id="tasa-de-error-aparente-teap" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> Tasa de Error Aparente (TEAP)<a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-aparente-teap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Existe una medida de desempeño que no depende de la forma de las poblaciones y que puede ser calculada para cualesquier proceso de clasificación. Esta medida se llama <strong>Tasa de Error Aparente (TEAP)</strong> y se define como la fracción de observaciones en la muestra de entrenamiento que son mal clasificadas por la función de clasificación muestral.</p>
<p>La TEAP puede ser fácilmente calculada a partir de la <strong>Matriz de Confusión</strong> la cual muestra los grupos de miembros actuales versus los predichos. Para <span class="math inline">\(n_1\)</span>-observaciones de <span class="math inline">\(\pi_1\)</span> y <span class="math inline">\(n_2\)</span>-observaciones de <span class="math inline">\(\pi_2\)</span>, la <em>Matriz de Confusión</em> tiene la siguiente forma:
<span class="math display">\[
\begin{array}{cc|cc|c}
&amp; &amp; Miembros &amp; predichos &amp; \\
&amp; &amp; \pi_1  &amp; \pi_2 &amp; Totales\\\hline
Miembros &amp;  \pi_1 &amp; n_{11} &amp; n_{12}=n_1-n_{11} &amp; n_1 \\
Actuales &amp; \pi_2 &amp; n_{21}=n_2-n_{n_2} &amp;  n_{22} &amp; n_2
\end{array}
\]</span></p>
<p>donde:
<span class="math display">\[
n_{11}: \ \ \text{Observaciones de} \ \pi_1 \ \text{correctamente clasificadas en} \ \pi_1
\]</span></p>
<p><span class="math display">\[
n_{12}: \ \ \text{Observaciones de} \ \pi_1 \ \text{incorrectamente clasificadas en} \ \pi_2
\]</span></p>
<p><span class="math display">\[
n_{21}: \ \ \text{Observaciones de} \ \pi_2 \ \text{incorrectamente clasificadas en} \ \pi_1
\]</span></p>
<p><span class="math display">\[
n_{22:} \ \ \text{Observaciones de} \ \pi_2 \ \text{correctamente clasificadas en} \ \pi_2
\]</span></p>
<p><strong>La tasa de error aparente (TEAP) se define como</strong>:
<span class="math display" id="eq:tasa-de-error-aparente">\[
\begin{equation}
TEAP=\frac{n_{12}+n_{21}}{n_{1}+n_{2}}=\frac{n_{12}+n_{21}}{n}
\end{equation}
\tag{9.28}
\]</span></p>
<p>que representa <em>El Porcentaje de Observaciones en el Conjunto de Datos de Entrenamiento Mal Clasificadas</em>.</p>
<div class="example">
<p><span id="exm:ejemplo3-tasa-error-optima" class="example"><strong>Ejemplo 9.6  (Calculo de Tasa de Error Aparente (TEAP)) </strong></span>Considere los datos del ejemplo <a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html#exm:ejemplo1-analisis-discriminante">9.1</a>, donde se tienen dos grupos de personas en una ciudad: <span class="math inline">\(\pi_1:\)</span> personas propietarias de tractores cortacéspedes, y <span class="math inline">\(\pi_2:\)</span>, aquellas personas que no poseen tractores cortacéspedes, es decir, personas no propietarias de cortacéspedes. Con el fin de identificar las mejores perspectivas de venta a través de una campaña intensiva, un fabricante de cortadoras de césped está interesado en clasificar a las familias como posibles propietarias o no propietarias, sobre la base de las mediciones de las variables siguientes: <span class="math inline">\(X_1\)</span>-ingresos y <span class="math inline">\(X_2\)</span>-tamaño del lote donde tienen su vivienda. Muestras aleatorias de <span class="math inline">\(n_1=12\)</span>-propietarios actuales y <span class="math inline">\(n_2=12\)</span>-no propietarios actuales arrojaron los valores de la siguiente Tabla.´</p>
</div>
<p><span class="math display">\[
\begin{array}{cccc}
\begin{array}{rrrr}
  \hline
&amp;  X_1-\text{Ingresos} &amp; X_2-\text{Tamaño} &amp;  \\
&amp; \text{en}\ \ 1000s &amp; \text{en} \ \  1000\ ft^2 &amp; \text{Grupo}\\\hline
1 &amp; 90.0 &amp; 18.4 &amp;  1 \\
  2 &amp; 115.5 &amp; 16.8 &amp;  1 \\
  3 &amp; 94.8 &amp; 21.6 &amp;  1 \\
  4 &amp; 91.5 &amp; 20.8 &amp;  1 \\
  5 &amp; 117.0 &amp; 23.6 &amp;  1 \\
  6 &amp; 140.1 &amp; 19.2 &amp;  1 \\
  7 &amp; 138.0 &amp; 17.6 &amp;  1 \\
  8 &amp; 112.8 &amp; 22.4 &amp;  1 \\
  9 &amp; 99.0 &amp; 20.0 &amp;  1 \\
  10 &amp; 123.0 &amp; 20.8 &amp;  1 \\
  11 &amp; 81.0 &amp; 22.0 &amp;  1 \\
  12 &amp; 111.0 &amp; 20.0 &amp;  1 \\\hline
\end{array} &amp;&amp;
\begin{array}{rrrr}
  \hline
&amp;  X_1-\text{Ingresos} &amp; X_2-\text{Tamaño} &amp;  \\
&amp; \text{en}\ \ 1000s &amp; \text{en} \ \  1000\ ft^2 &amp; \text{Grupo}\\\hline
  13 &amp; 105.0 &amp; 19.6 &amp;  2 \\
  14 &amp; 82.8 &amp; 20.8 &amp;  2 \\
  15 &amp; 94.8 &amp; 17.2 &amp;  2 \\
  16 &amp; 73.2 &amp; 20.4 &amp;  2 \\
  17 &amp; 114.0 &amp; 17.6 &amp;  2 \\
  18 &amp; 79.2 &amp; 17.6 &amp;  2 \\
  19 &amp; 89.4 &amp; 16.0 &amp;  2 \\
  20 &amp; 96.0 &amp; 18.4 &amp;  2 \\
  21 &amp; 77.4 &amp; 16.4 &amp;  2 \\
  22 &amp; 63.0 &amp; 18.8 &amp;  2 \\
  23 &amp; 81.0 &amp; 14.0 &amp;  2 \\
  24 &amp; 93.0 &amp; 14.8 &amp;  2 \\
   \hline
\end{array}
\end{array}
\]</span></p>
<p>Los datos están representados en la siguiente gráfica.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico-discriminante-ejemplo3"></span>
<img src="bookdown-iam_files/figure-html/grafico-discriminante-ejemplo3-1.png" alt="Grafico de Dispersión de Datos (Ambas Poblaciones)" width="80%" />
<p class="caption">
Figura 9.6: Grafico de Dispersión de Datos (Ambas Poblaciones)
</p>
</div>
<p>Suponga que se tiene la siguiente tabla de frecuencia (o matriz de confusión):</p>
<p><span class="math display">\[
\begin{array}{cc|cc|c}
&amp; &amp; Miembros &amp; predichos &amp; \\
&amp; &amp; \pi_1  &amp; \pi_2 &amp; Totales\\\hline
Miembros &amp;  \pi_1 &amp; n_{11}=10 &amp; n_{12}=2 &amp; n_1=12\\
Actuales &amp; \pi_2 &amp; n_{21}=2 &amp; n_{22}=10 &amp; n_2=12
\end{array}
\]</span></p>
<p>De esta matriz se obtiene que la <em>Tasa de Error Aparente es</em>:
<span class="math display">\[
TEAP:=\frac{n_{12}+n_{21}}{n_1+n_2}=\frac{4}{24}=0.167.
\]</span></p>
<p>Esto indica que el <span class="math inline">\(16.7\%\)</span> de las personas son mal clasificadas.</p>
<p><strong>Observaciones:</strong></p>
<p>La Tasa de Error Aparente (TEAP) es intuitivamente atractiva y fácil de calcular, pero ésta tiende a sub-estimar a La Tasa de Error Actual (TEA), dicho problema no desaparece a menos que se utilicen tamaños de muestras <span class="math inline">\(n_1\)</span> y <span class="math inline">\(n_2\)</span> muy grandes. Esto sucede porque los datos usados para construir la función de clasificación son los mismos usados para evaluarla.</p>
<p>Se pueden construir estimaciones de la tasa de error que son mejores que la tasa de error aparente, las cuales siguen siendo relativamente fáciles de calcular, y no requieren de suposición de distribución.</p>
<p><strong>Un primer procedimiento</strong>, consiste en dividir la muestra total de los datos en dos muestras, una muestra de entrenamiento y una muestra de validación. La muestra de entrenamiento se utiliza para construir la función de clasificación, y la muestra de validación se utiliza para evaluarla. La tasa de error está determinada por el porcentaje de observaciones mal clasificadas en la muestra de validación. Aunque este método supera la problema de sesgo al no usar los mismos datos para construir y juzgar la función de clasificación, adolece de dos defectos principales:</p>
<p>1). Requiere muestras grandes.</p>
<p>2). La función evaluada no es la función de interés. Al final, casi todos los datos deben usarse para construir la función de clasificación. De lo contrario, se puede perder información valiosa.</p>
</div>
<div id="tasa-de-error-actual-esperada" class="section level3 hasAnchor" number="9.5.4">
<h3><span class="header-section-number">9.5.4</span> Tasa de Error Actual Esperada<a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual-esperada" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Un segundo enfoque o procedimiento</strong>, que parece funcionar bien se llama: proceso de “retención” de Lachenbruch. ver también Lachenbruch y Mickey [24]. Consta de los sigueintes pasos:</p>
<p>1). Comienza con el grupo <span class="math inline">\(\pi_1\)</span> de observaciones. Se Omite una observación de este grupo, y se desarrolla una función de clasificación basada en las restantes <span class="math inline">\((n_1-1)\)</span> observaciones y las <span class="math inline">\((n_2)\)</span> observaciones del grupo <span class="math inline">\(\pi_2\)</span>.</p>
<p>2). Clasifica la observación de “retención”, usando la función construida en el Paso 1.</p>
<p>3). Se repiten los pasos 1 y 2 hasta que todas las observaciones del grupo <span class="math inline">\(\pi_1\)</span> son clasificadas. Sea <span class="math inline">\(n_{1M}^{H}\)</span>-el número de observaciones reservadas o retenidas (<span class="math inline">\(H\)</span>) mal clasificadas en este grupo <span class="math inline">\(\pi_1\)</span>.</p>
<p>4). Repita los pasos 1 a 3 para las observaciones del grupo <span class="math inline">\(\pi_2\)</span>. Sea Sea <span class="math inline">\(n_{2M}^{H}\)</span>-el número de observaciones reservadas o retenidas (<span class="math inline">\(H\)</span>) mal clasificadas en este grupo <span class="math inline">\(\pi_2\)</span>.</p>
<p>Ahora, se tienen Estimaciones de las Probabilidades Condicionales de Observaciones Mal Clasificadas <span class="math inline">\(P(2\ \ |\ \ 1)\)</span> y de <span class="math inline">\(P(1\ \ |\ \ 2)\)</span> dadas por:
<span class="math display">\[
\widehat{\ P\ }(2\ \bigl| \ 1)= \frac{n_{1M}^H}{n_1} \ \ \ \ \ \text{y} \ \ \ \ \ \ \ \widehat{\ P\ }(1\ \bigl| \ 2)= \frac{n_{2M}^H}{n_2}
\]</span></p>
<p>y para Muestras Moderadas se tiene que, <strong>El Porcentaje Total de Observaciones Mal Clasificadas</strong> dado por:
<span class="math display">\[
\frac{n_{1M}^{(H)}+n_{2M}^{(H)}}{n_1+n_2}
\]</span></p>
<p>es un <em>Estimador Cercanamente Insesgado</em> de la <em>Tasa de Error Actual Esperada</em>, es decir:
<span class="math display" id="eq:estimador-de-la-tasa-de-error-actual-esperada">\[
\begin{equation}
\widehat{\ E\ }(TEA)=\frac{n_{1M}^{(H)}+n_{2M}^{(H)}}{n_1+n_2}
\end{equation}
\tag{9.29}
\]</span></p>
<p>El <em>Método de Retención de Lachenbruch</em> es computacionalmente factible cuando se usa en conjunto con las Estadísticas de Clasificación lineal dadas en <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm">(9.18)</a> o en <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm2">(9.19)</a>. Éste método se ofrece como una opción en algunos programas informáticos con Análisis Discriminante disponibles.</p>
<div class="example">
<p><span id="exm:ejemplo-tasa-de-error-actual-esperada" class="example"><strong>Ejemplo 9.7  (Cálculo de la Tasa de Error Actual Esperada) </strong></span>En este ejemplo se ilustra el procedimiento de <strong>Retención de Lachenbruch</strong> y el cálculo de tasas de error estimadas para situaciones de costos iguales y proporciones A prioris iguales dadas en <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm">(9.18)</a>. Consideremos las siguientes matrices de datos y estadísticas de resúmenes descriptivos. Se supone además que <span class="math inline">\(n_1=n_2=3\)</span> observaciones bivariadas fueron seleccionadas al azar de dos poblaciones <span class="math inline">\(\pi_1\)</span> y <span class="math inline">\(\pi_2\)</span> con una matrices de varianzas y covarianzas común, es decir, <span class="math inline">\(\mathbf{\Sigma}_{\ 1}=\mathbf{\Sigma}_{\ 2}=\mathbf{\Sigma}\)</span>.</p>
</div>
<p><span class="math display">\[
\mathbf{X}_1=\begin{bmatrix} 2&amp;12 \\4&amp;10 \\ 3&amp;8  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 1}=\begin{bmatrix} 3 \\ 10 \end{bmatrix}\ \ \ ; \ \ \ (n_1-1)\mathbf{S}_{\ 1}=2\ \mathbf{S}_{\ 1}=\begin{bmatrix} 2 &amp; -2 \\ -2 &amp;  8 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{X}_2=\begin{bmatrix} 5&amp;7 \\3&amp;9 \\ 43&amp;5  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 1}=\begin{bmatrix} 4 \\ 7 \end{bmatrix}\ \ \ ; \ \ \ (n_1-1)\mathbf{S}_{\ 1}=2\ \mathbf{S}_{\ 1}=\begin{bmatrix} 2 &amp; -2 \\ -2 &amp;  8 \end{bmatrix}
\]</span></p>
<p>La Matriz de varianza covarinzas ponderada es:
<span class="math display">\[
\mathbf{S}_{pooled}=\frac{(n_1-1)\ \mathbf{S}_1+(n_2-1)\ \mathbf{S}_2}{n_1+n_2-2}=\frac{1}{4}(2\ \mathbf{S}_12+ 2 \ \mathbf{S}_2 )=\begin{bmatrix} 1 &amp; -1 \\ -1 &amp;  4 \end{bmatrix}
\]</span></p>
<p>Usando la Regla de Clasificación dada en <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm">(9.18)</a>
<span class="math display">\[
\overline{\underline{\mathbf{x}}}_{\ 0} \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}_{\ 0} -\frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2} ) \\ \space \\
  \geq Ln \left[ \left(\frac{c(1|2)}{c(2|1)}  \right)\left(\frac{p_2}{p_1} \right) \right]
\]</span></p>
<p>con costos iguales y A prioris iguales se puede realziar la clasificación de las observaciones muestrales, es decir usando:
<span class="math display">\[
\overline{\underline{\mathbf{x}}}_{\ 0} \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}_{\ 0} -\frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2} ) \\ \space \\
  \geq 0
\]</span></p>
<p>o equivalentemente,
<span class="math display">\[
\overline{\underline{\mathbf{x}}}_{\ 0} \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}_{\ 0} \geq \frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2} ) \\ \space \\
\]</span></p>
<p>es decir:
<span class="math display">\[
\overline{\underline{\mathbf{x}}}_{\ 0} \in \pi_1: \ \text{Si}: \ \ \widehat{\ y\ }_0 \geq \widehat{\ m\ }
\]</span></p>
<p>Se puede verificar (ver ejercicio 11.19 de <span class="citation">(<a href="#ref-johnson2007applied">Johnson and Wichern 2007</a>)</span>) que la <em>Matriz de Confusión</em> está dada por:
<span class="math display">\[
\begin{array}{cc|cc|c}
&amp; &amp; Miembros &amp; predichos &amp; \\
&amp; &amp; \pi_1  &amp; \pi_2 &amp; Totales\\\hline
Miembros &amp;  \pi_1 &amp; n_{11}=2 &amp; n_{12}=1 &amp; n_1=3\\
Actuales &amp; \pi_2 &amp; n_{21}=1 &amp; n_{22}=2 &amp; n_2=3
\end{array}
\]</span></p>
<p>y por lo tanto, <strong>La Tasa de Error Aparente (TEA)</strong> es:
<span class="math display">\[
TAE=\frac{n_{12}+n_{21}}{n_1+n_2}=\frac{1+1}{6}=\frac{2}{6}=0.33
\]</span></p>
<p><strong>Procedimiento de Lanchenbruch</strong></p>
<p><strong>Para el primer grupo o población:</strong></p>
<p>Ahora se pasa a utilizar el procedimiento de <strong>Retención de Lachenbruch</strong>. Reteniendo la primera observación de <span class="math inline">\(\pi_1\)</span> dada por: <span class="math inline">\(\underline{\mathbf{x}}_{\ H}^t=[2 \ , \ 12]\)</span> de la muestra <span class="math inline">\(\mathbf{X}_{1}\)</span>, es decir <span class="math inline">\(n_{1\ H}=2\)</span>, se tiene que:
<span class="math display">\[
\mathbf{X}_{\ 1\ H}=\begin{bmatrix} 4&amp;10 \\ 3&amp;8  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 1\ H}=\begin{bmatrix} 3.5 \\ 9 \end{bmatrix}\ \ \ ; \ \ \ (n_{1\ H}-1)\mathbf{S}_{\ 1\ H}=1\ \mathbf{S}_{\ 1\ H}=\begin{bmatrix} 0.5 &amp; 1 \\ 1 &amp;  2 \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1\ H}-1)\ \mathbf{S}_{\ 1\ H}+(n_2-1)\ \mathbf{S}_{\ 2}}{n_{1\ H}+n_2-2}=\frac{1}{3}(1\ \mathbf{S}_{\ 1\ H} + 2 \ \mathbf{S}_{\ 2} )=\frac{1}{3}\begin{bmatrix} 2.5 &amp; -1 \\ -1 &amp;  10 \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{1}{8}\begin{bmatrix} 10 &amp; 1 \\ 1 &amp;  2.5 \end{bmatrix}
\]</span></p>
<p>Ahora se pasa a clasificar esta observación <span class="math inline">\(\underline{\mathbf{x}}_{\ H}\)</span> usando la regla de clasificación obtenida con dicha observación extraída de la población, es decir usando:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0\ H} \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_{\ 1\ H}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{H\ Polled}^{-1}\ \underline{\mathbf{x}}_{\ 0\ H} \geq \frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1\ H} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{H\ Polled}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1\ H} + \overline{\underline{\mathbf{x}}}_{\ 2} )
\]</span>
es decir:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0\ H} \in \pi_1: \ \text{Si}: \ \ \widehat{\ y\ }_{0\ H} \geq \widehat{\ m_{H}\ }
\]</span></p>
<p>Ahora, es computacionalmente más rápido clasificar la observación reservada <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}\)</span> sobre la base de su distancia al cuadrado a las medias <span class="math inline">\(\overline{\underline{\mathbf{x}}}_{\ 1\ H}\)</span> y <span class="math inline">\(\overline{\underline{\mathbf{x}}}_{\ 2}\)</span> de cada grupo, es decir, calcularlas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span>
y asignar a <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}\)</span> al grupo cuya distancia sea más pequeña.</p>
<p>Este procedimiento es equivalente para calcular el valor de la función lineal
<span class="math display">\[
\widehat{\ y\ }_{0\ H} = (\overline{\underline{\mathbf{x}}}_{\ 1\ H}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{H\ Polled}^{-1}\ \underline{\mathbf{x}}_{\ 0\ H}
\]</span></p>
<p>y compararlo con el punto medio dado por:
<span class="math display">\[
\widehat{\ m_{H}\ } = \frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1\ H} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{H\ Polled}^{-1}(\overline{\underline{\mathbf{x}}}
_{\ 1\ H} + \overline{\underline{\mathbf{x}}}_{\ 2} )
\]</span></p>
<ul>
<li><strong>Para la Primera Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}^t=\underline{\mathbf{x}}_{H}^t=[2\ , \ 12]\)</span></strong></li>
</ul>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[2\ , \ 12]\)</span> de <span class="math inline">\(\mathbf{X}_1\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 2-3.5 &amp; 12-9 \end{bmatrix} \frac{1}{8}\begin{bmatrix}10 &amp; 1 \\ 1 &amp; 2.5  \end{bmatrix} \begin{bmatrix}2-3.5 \\ 12-9 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=4.5
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 2-4 &amp; 12-7 \end{bmatrix} \frac{1}{8}\begin{bmatrix}10 &amp; 1 \\ 1 &amp; 2.5  \end{bmatrix} \begin{bmatrix}2-4 \\ 12-7 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2}) &amp; = 10.3
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) = 4.5 &lt; 10.3 = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[2\ , \ 12]\)</span> se clasifica en la población <span class="math inline">\(\pi_1\)</span>, <em>es decir es una clasificación correcta</em>.</p>
<ul>
<li><strong>Para la Segunda Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=[4\ , \ 10]\)</span></strong></li>
</ul>
<p>Primero debemos hallar las nuevas matrices de datos:
<span class="math display">\[
\mathbf{X}_{\ 1\ H}=\begin{bmatrix} 2&amp;12 \\ 3&amp;8  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 1\ H}=\begin{bmatrix} 2.5 \\ 10 \end{bmatrix}\ \ \ ; \ \ \ (n_{1\ H}-1)\mathbf{S}_{\ 1\ H}=1\ \mathbf{S}_{\ 1\ H}=\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1\ H}-1)\ \mathbf{S}_{\ 1\ H}+(n_2-1)\ \mathbf{S}_{\ 2}}{n_{1\ H}+n_2-2}=\frac{1}{3}(1\ \mathbf{S}_{\ 1\ H} + 2 \ \mathbf{S}_{\ 2} )=\frac{1}{3}\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{1}{8}\begin{bmatrix} 16 &amp; 4 \\ 4 &amp;  2.5 \end{bmatrix}
\]</span></p>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[4\ , \ 10]\)</span> de <span class="math inline">\(\mathbf{X}_1\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 4-2.5 &amp; 10-10 \end{bmatrix} \frac{1}{8}\begin{bmatrix}16 &amp; 4 \\ 4 &amp; 2.5  \end{bmatrix} \begin{bmatrix}4-2.5 \\ 10-10 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=4.5
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 4-4 &amp; 10-7 \end{bmatrix} \frac{1}{8}\begin{bmatrix}16 &amp; 4 \\ 4 &amp; 2.5  \end{bmatrix} \begin{bmatrix}4-4 \\ 10-7 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2}) &amp; = 2.8
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) = 4.5 &gt; 2.8 = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[4\ , \ 10]\)</span> se clasifica en la población <span class="math inline">\(\pi_2\)</span>, <em>es decir es una clasificación incorrecta</em>.</p>
<ul>
<li><strong>Para la Tercera Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=[3\ , \ 8]\)</span></strong></li>
</ul>
<p>Primero debemos hallar las nuevas matrices de datos:
<span class="math display">\[
\mathbf{X}_{\ 1\ H}=\begin{bmatrix} 2&amp;12 \\ 4&amp;10  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 1\ H}=\begin{bmatrix} 3 \\ 11 \end{bmatrix}\ \ \ ; \ \ \ (n_{1\ H}-1)\mathbf{S}_{\ 1\ H}=1\ \mathbf{S}_{\ 1\ H}=\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1\ H}-1)\ \mathbf{S}_{\ 1\ H}+(n_2-1)\ \mathbf{S}_{\ 2}}{n_{1\ H}+n_2-2}=\frac{1}{3}(1\ \mathbf{S}_{\ 1\ H} + 2 \ \mathbf{S}_{\ 2} )=\frac{1}{3}\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{1}{8}\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[3\ , \ 8]\)</span> de <span class="math inline">\(\mathbf{X}_1\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 3-2 &amp; 8-11 \end{bmatrix} \frac{1}{8}\begin{bmatrix}x &amp; y \\ z &amp; w  \end{bmatrix} \begin{bmatrix}3-2 \\ 8-11 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp; = xxx
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1\ H})\\
&amp;= \begin{bmatrix} 3-4 &amp; 8-7 \end{bmatrix} \frac{1}{8}\begin{bmatrix}x &amp; y \\ z &amp; w  \end{bmatrix} \begin{bmatrix}3-4 \\ 8-7 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2}) &amp; = yyy
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H}) = xxx &gt; yyy = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[3\ , \ 8]\)</span> se clasifica en la población <span class="math inline">\(\pi_2\)</span>, <em>es decir es una clasificación incorrecta</em>.</p>
<p><strong>Resúmen:</strong></p>
<p>Con esto se concluye que usando el método de Retención de Lachenbruch se tiene que del grupo o población uno se clasifican incorrectamente a dos observaciones, la segunda y la tercera, es decir que: <span class="math inline">\(n_{1\ M}^{(H)}=2\)</span>.</p>
<p><strong>Para el Segundo Grupo o Población:</strong></p>
<p>Ahora se pasa a utilizar el procedimiento de <strong>Retención de Lachenbruch</strong>. Reteniendo la primera observación de <span class="math inline">\(\pi_2\)</span> dada por: <span class="math inline">\(\underline{\mathbf{x}}_{\ H}^t=[5 \ , \ 7]\)</span> de la muestra <span class="math inline">\(\mathbf{X}_{2}\)</span>, es decir <span class="math inline">\(n_{2\ H}=2\)</span>, se tiene que:
<span class="math display">\[
\mathbf{X}_{\ 2\ H}=\begin{bmatrix} 3&amp;9 \\ 4&amp;5  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 2\ H}=\begin{bmatrix} 3.5 \\ 7 \end{bmatrix}\ \ \ ; \ \ \ (n_{2\ H}-1)\mathbf{S}_{\ 2\ H}=1\ \mathbf{S}_{\ 2\ H}=\begin{bmatrix} 0.5 &amp; -2 \\ -2 &amp;  8 \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1}-1)\ \mathbf{S}_{\ 1}+(n_{2\ H}-1)\ \mathbf{S}_{\ 2\ H}}{n_{1}+n_{2\ H}-2}=\frac{1}{3}(2\ \mathbf{S}_{\ 1} + 1 \ \mathbf{S}_{\ 2\ H} )=\frac{1}{3}\begin{bmatrix} 2.5 &amp; -4 \\ -4 &amp;  16 \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{3}{24}\begin{bmatrix} 16 &amp; 4 \\ 4 &amp;  2.5 \end{bmatrix}
\]</span></p>
<p>Ahora se pasa a clasificar esta observación <span class="math inline">\(\underline{\mathbf{x}}_{\ H}\)</span> usando la regla de clasificación obtenida con dicha observación extraída de la población, es decir usando:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0\ H} \in \pi_1: \ \text{Si}: \ \ (\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})^t
\mathbf{S}_{H\ Polled}^{-1}\ \underline{\mathbf{x}}_{\ 0\ H} \geq \frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2\ H} )^t
\mathbf{S}_{H\ Polled}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2\ H} )
\]</span>
es decir:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0\ H} \in \pi_1: \ \text{Si}: \ \ \widehat{\ y\ }_{0\ H} \geq \widehat{\ m_{H}\ }
\]</span></p>
<p>Ahora, es computacionalmente más rápido clasificar la observación reservada <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}\)</span> sobre la base de su distancia al cuadrado a las medias <span class="math inline">\(\overline{\underline{\mathbf{x}}}_{\ 1}\)</span> y <span class="math inline">\(\overline{\underline{\mathbf{x}}}_{\ 2\ H}\)</span> de cada grupo, es decir, calcularlas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span>
y asignar a <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}\)</span> al grupo cuya distancia sea más pequeña.</p>
<p>Este procedimiento es equivalente para calcular el valor de la función lineal
<span class="math display">\[
\widehat{\ y\ }_{0\ H} = (\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})^t
\mathbf{S}_{H\ Polled}^{-1}\ \underline{\mathbf{x}}_{\ 0\ H}
\]</span></p>
<p>y compararlo con el punto medio dado por:
<span class="math display">\[
\widehat{\ m_{H}\ } = \frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2\ H} )^t
\mathbf{S}_{H\ Polled}^{-1}(\overline{\underline{\mathbf{x}}}
_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2\ H} )
\]</span></p>
<ul>
<li><strong>Para la Primera Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}^t=\underline{\mathbf{x}}_{H}^t=[5\ , \ 7]\)</span></strong></li>
</ul>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[5\ , \ 7]\)</span> de <span class="math inline">\(\mathbf{X}_2\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})\\
&amp;= \begin{bmatrix} 5-3 &amp; 7-10 \end{bmatrix} \frac{3}{24}\begin{bmatrix} 16 &amp; 4 \\ 4 &amp; 2.5  \end{bmatrix} \begin{bmatrix} 5-3 \\ 7-10 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1\ H})&amp;=4.8
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})\\
&amp;= \begin{bmatrix} 5-3.5 &amp; 7-7 \end{bmatrix} \frac{3}{24}\begin{bmatrix} 16 &amp; 4 \\ 4 &amp; 2.5  \end{bmatrix} \begin{bmatrix} 5-3.5 \\ 7-7 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2}) &amp; = 4.5
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) = 4.8 &gt; 4.5 = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[5\ , \ 7]\)</span> se clasifica en la población <span class="math inline">\(\pi_2\)</span>, <em>es decir es una clasificación correcta</em>.</p>
<ul>
<li><strong>Para la Segunda Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=[3\ , \ 9]\)</span></strong></li>
</ul>
<p>Primero debemos hallar las nuevas matrices de datos:
<span class="math display">\[
\mathbf{X}_{\ 2\ H}=\begin{bmatrix} 5&amp;7 \\ 4&amp;5  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 2\ H}=\begin{bmatrix} 4.5 \\ 6 \end{bmatrix}\ \ \ ; \ \ \ (n_{2\ H}-1)\mathbf{S}_{\ 2\ H}=1\ \mathbf{S}_{\ 2\ H}=\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1}-1)\ \mathbf{S}_{\ 1}+(n_{2\ H}-1)\ \mathbf{S}_{\ 2\ H}}{n_{1}+n_{2\ H}-2}=\frac{1}{3}(2\ \mathbf{S}_{1} + 1 \ \mathbf{S}_{\ 2\ H} )=\frac{1}{3}\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{3}{24}\begin{bmatrix} 10 &amp; 1 \\ 1 &amp;  2.5 \end{bmatrix}
\]</span></p>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[3\ , \ 9]\)</span> de <span class="math inline">\(\mathbf{X}_2\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})\\
&amp;= \begin{bmatrix} 3-3 &amp; 9-10 \end{bmatrix} \frac{3}{24}\begin{bmatrix} 10 &amp; 1 \\ 1 &amp; 2.5  \end{bmatrix} \begin{bmatrix} 3-3 \\ 9-10 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) &amp; = 0.3
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})\\
&amp;= \begin{bmatrix} 3-4.5 &amp; 9-6 \end{bmatrix} \frac{3}{24}\begin{bmatrix} 10 &amp; 1 \\ 1 &amp; 2.5  \end{bmatrix} \begin{bmatrix} 3-4.5 \\ 9-6 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H}) &amp; = 4.5
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) = 0.3 &lt; 4.5 = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[3\ , \ 9]\)</span> se clasifica en la población <span class="math inline">\(\pi_1\)</span>, <em>es decir es una clasificación incorrecta</em>.</p>
<ul>
<li><strong>Para la Tercera Observación: <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=[4\ , \ 5]\)</span></strong></li>
</ul>
<p>Primero debemos hallar las nuevas matrices de datos:
<span class="math display">\[
\mathbf{X}_{\ 2\ H}=\begin{bmatrix} 5&amp;7 \\ 3&amp;9  \end{bmatrix} \ \ \ ; \ \ \ \underline{\overline{\mathbf{x}}}_{\ 2\ H}=\begin{bmatrix} 4 \\ 8 \end{bmatrix}\ \ \ ; \ \ \ (n_{2\ H}-1)\mathbf{S}_{\ 2\ H}=1\ \mathbf{S}_{\ 2\ H}=\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>La nueva Matriz de varianzas covarianzas ponderada sería:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}=\frac{(n_{1}-1)\ \mathbf{S}_{\ 1}+(n_{2\ H}-1)\ \mathbf{S}_{\ 2\ H}}{n_{1}+n_{2\ H}-2}=\frac{1}{3}(2\ \mathbf{S}_{1} + 1 \ \mathbf{S}_{\ 2\ H} )=\frac{1}{3}\begin{bmatrix} x &amp; y \\ z &amp;  w \end{bmatrix}
\]</span></p>
<p>con inversa dada por:
<span class="math display">\[
\mathbf{S}_{\ H,\ pooled}^{-1}=\frac{3}{24}\begin{bmatrix} x &amp; y \\ z &amp; w \end{bmatrix}
\]</span></p>
<p>Para esta observación <span class="math inline">\(\underline{\mathbf{x}}_{H}^t=[4\ , \ 5]\)</span> de <span class="math inline">\(\mathbf{X}_2\)</span> se pasan a calcular las respectivas distancias al cuadrado dadas por:
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) \ \ \ \ \ \text{y} \ \ \ \ \ \ d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p><span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 1})\\
&amp;= \begin{bmatrix} 4-3 &amp; 5-10 \end{bmatrix} \frac{3}{24}\begin{bmatrix} x &amp; y \\ z &amp; w  \end{bmatrix} \begin{bmatrix} 4-3 \\ 5-10 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) &amp; = xxx
\end{align*}
\]</span></p>
<p>Similarmente,
<span class="math display">\[
\begin{align*}
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})&amp;=(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})^t \ \mathbf{S}_{H\ polled}^{-1}(\underline{\mathbf{x}}_{\ 0\ H}-\overline{\underline{\mathbf{x}}}_{\ 2\ H})\\
&amp;= \begin{bmatrix} 4-4 &amp; 5-8 \end{bmatrix} \frac{3}{24}\begin{bmatrix} x &amp; y \\ z &amp; w  \end{bmatrix} \begin{bmatrix} 4-4 \\ 5-8 \end{bmatrix}\\
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H}) &amp; = yyy
\end{align*}
\]</span></p>
<p>Ahora como la distancia
<span class="math display">\[
d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 1}) = xxx &lt; yyy = d^2(\underline{\mathbf{x}}_{\ 0\ H}\ , \ \overline{\underline{\mathbf{x}}}_{\ 2\ H})
\]</span></p>
<p>luego la observación <span class="math inline">\(\underline{\mathbf{x}}_{\ 0\ H}=\underline{\mathbf{x}}_{\ H}=[4\ , \ 5]\)</span> se clasifica en la población <span class="math inline">\(\pi_2\)</span>, <em>es decir es una clasificación correcta</em>.</p>
<p><strong>Resúmen:</strong></p>
<p>Con esto se concluye que usando el método de Retención de Lachenbruch se tiene que del grupo o población dos se clasifican incorrectamente una observaciones, la segunda, es decir que: <span class="math inline">\(n_{2\ M}^{(H)}=1\)</span>.</p>
<p><em>En conclusión de todos los cálculos anteriores se tiene que</em> <strong>La Tasa de Error Actual Esperada (<span class="math inline">\(\widehat{\ E\ }(TEA)\)</span>) es:</strong>
<span class="math display">\[
\widehat{\ E\ }(TEA)=\frac{n_{1M}^{(H)}+n_{2M}^{(H)}}{n_1+n_2}=\frac{2+1}{3+3}=\frac{3}{6}=0.5
\]</span></p>
<p>Por lo tanto, vemos que la <em>Tasa de Error Aparente</em> <span class="math inline">\(TEAP= 0.33\)</span> es una medida optimista del desempeño de la Regla de Clasificación. Por supuesto, en la práctica, los tamaños de muestra son más grandes que los que tenemos considerados en este ejemplo y la diferencia entre la Tasa de Error Aparente, <span class="math inline">\(TEAP\)</span> y la Tasa de Error Actual Esperada, <span class="math inline">\(\widehat{\ E\ }(TEA)\)</span> puede no ser tan grande. Si está interesado en seguir los enfoques para estimar las Tasas de Error de Clasificación en Análisis Discriminante, ver. <span class="citation">(<a href="#ref-lachenbruch1975">Lachenbruch 1975</a>)</span>.</p>
<div class="example">
<p><span id="exm:ejemplo2-tasa-de-error-actual-esperada" class="example"><strong>Ejemplo 9.8  (Ejemplo: Clasificación del salmón de Alaska y del Canadá) </strong></span>La pesca del salmón es una un recurso valioso tanto para Estados Unidos como para Canadá. Debido a que éste es un recurso limitado, se debe gestionar eficientemente. Además, dado que más de un país está involucrado en dicha pesca, los problemas se deben resolver de manera equitativa. Es decir, los pescadores comerciales de Alaska no pueden pescar demasiado salmón canadiense y viceversa.</p>
<p>Estos peces tienen un ciclo de vida notable. Ellos nacen en arroyos de agua dulce
y después de uno o dos años nadan hacia el océano. Después de un par de años en aguas saladas, regresan a su lugar de nacimiento para desovar y morir. En el momento en que están a punto de regresar como peces maduros son capturados mientras aún están en el océano. Para ayudar a regular las capturas, las muestras de pescado tomadas durante las capturas deben ser identificadas como procedentes de aguas de Alaska o de Canadá. Los peces llevan cierta información sobre su lugar de nacimiento en los anillos de crecimiento de sus escamas. Típicamente, los anillos asociados con el crecimiento en agua dulce son menores para los nacidos en Alaska que para los nacidos en Canadá. Los datos de la siguiente tabla muestran los diámetros de las regiones de los anillos de crecimiento, magnificados 100 veces, donde:</p>
</div>
<p><span class="math display">\[
X_1 = \text{Diámetro de los anillos para el crecimiento en agua dulce del primer año (en cientos de una pulgada)}
\]</span></p>
<p><span class="math display">\[
X_2 = \text{Diámetro de los anillos para el crecimiento en agua marina o salada del primer año (en cientos de una pulgada)}
\]</span></p>
<p>Además se tiene la información del género, donde los machos son codificados con el número-1 y las hembras con el número-2.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico2-ejemplo2-discriminante"></span>
<img src="bookdown-iam_files/figure-html/grafico2-ejemplo2-discriminante-1.png" alt="Grafico de Dispersión de Datos (Dos Poblaciones)" width="80%" />
<p class="caption">
Figura 9.7: Grafico de Dispersión de Datos (Dos Poblaciones)
</p>
</div>
<p>Los datos se presentan en la siguiente tabla.</p>
<table class="kable_wrapper table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-306">Tabla 9.2: </span>Datos del Ejemplo Grupos-1,2
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:right;">
Región
</th>
<th style="text-align:right;">
Género
</th>
<th style="text-align:right;">
Agua_Dulce
</th>
<th style="text-align:right;">
Agua_Marina
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
108
</td>
<td style="text-align:right;">
368
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
131
</td>
<td style="text-align:right;">
355
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
469
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
86
</td>
<td style="text-align:right;">
506
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
402
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
87
</td>
<td style="text-align:right;">
423
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
440
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
117
</td>
<td style="text-align:right;">
489
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
432
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
403
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:right;">
428
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
372
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
372
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
420
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
112
</td>
<td style="text-align:right;">
394
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
104
</td>
<td style="text-align:right;">
407
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
111
</td>
<td style="text-align:right;">
422
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
126
</td>
<td style="text-align:right;">
423
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
434
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
119
</td>
<td style="text-align:right;">
474
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:right;">
396
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
100
</td>
<td style="text-align:right;">
470
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:right;">
399
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
102
</td>
<td style="text-align:right;">
429
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
469
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
85
</td>
<td style="text-align:right;">
444
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
397
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
106
</td>
<td style="text-align:right;">
442
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
82
</td>
<td style="text-align:right;">
431
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
118
</td>
<td style="text-align:right;">
381
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
388
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
403
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
85
</td>
<td style="text-align:right;">
451
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
83
</td>
<td style="text-align:right;">
453
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
427
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
95
</td>
<td style="text-align:right;">
411
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
76
</td>
<td style="text-align:right;">
442
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
95
</td>
<td style="text-align:right;">
426
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
87
</td>
<td style="text-align:right;">
402
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
397
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:right;">
511
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
91
</td>
<td style="text-align:right;">
469
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
74
</td>
<td style="text-align:right;">
451
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
474
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
80
</td>
<td style="text-align:right;">
398
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
95
</td>
<td style="text-align:right;">
433
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
92
</td>
<td style="text-align:right;">
404
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
481
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
491
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
87
</td>
<td style="text-align:right;">
480
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:right;">
Región
</th>
<th style="text-align:right;">
Género
</th>
<th style="text-align:right;">
Agua_Dulce
</th>
<th style="text-align:right;">
Agua_Marina
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
129
</td>
<td style="text-align:right;">
420
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
148
</td>
<td style="text-align:right;">
371
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
179
</td>
<td style="text-align:right;">
407
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
152
</td>
<td style="text-align:right;">
381
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
166
</td>
<td style="text-align:right;">
377
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
389
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
156
</td>
<td style="text-align:right;">
419
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
131
</td>
<td style="text-align:right;">
345
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
140
</td>
<td style="text-align:right;">
362
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
345
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
393
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
108
</td>
<td style="text-align:right;">
330
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
135
</td>
<td style="text-align:right;">
355
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
170
</td>
<td style="text-align:right;">
386
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
152
</td>
<td style="text-align:right;">
301
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
153
</td>
<td style="text-align:right;">
397
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
152
</td>
<td style="text-align:right;">
301
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
136
</td>
<td style="text-align:right;">
438
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
122
</td>
<td style="text-align:right;">
306
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
148
</td>
<td style="text-align:right;">
383
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
90
</td>
<td style="text-align:right;">
385
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
337
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
364
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
376
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
115
</td>
<td style="text-align:right;">
354
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
134
</td>
<td style="text-align:right;">
383
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
117
</td>
<td style="text-align:right;">
355
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
126
</td>
<td style="text-align:right;">
345
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
118
</td>
<td style="text-align:right;">
379
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
120
</td>
<td style="text-align:right;">
369
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
153
</td>
<td style="text-align:right;">
403
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
150
</td>
<td style="text-align:right;">
354
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
154
</td>
<td style="text-align:right;">
390
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
155
</td>
<td style="text-align:right;">
349
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
325
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
117
</td>
<td style="text-align:right;">
344
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
128
</td>
<td style="text-align:right;">
400
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
403
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
370
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
355
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
133
</td>
<td style="text-align:right;">
375
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
128
</td>
<td style="text-align:right;">
383
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
349
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;">
373
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
140
</td>
<td style="text-align:right;">
388
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
150
</td>
<td style="text-align:right;">
339
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
341
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
346
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
153
</td>
<td style="text-align:right;">
352
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
108
</td>
<td style="text-align:right;">
339
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>Las muestras de entrenamiento de tamaños <span class="math inline">\(n_1=50\)</span>-nacidos en Alaska y de <span class="math inline">\(n_2=50\)</span>-nacidos en Canadá, producen los siguientes resúmenes estadísticos:</p>
<p><span class="math display">\[
\underline{\overline{\mathbf{x}}}_{\ 1}= \begin{bmatrix} 98.38 \\ 429.66 \end{bmatrix}\ \ \ \ ; \ \ \ \ \ \mathbf{S}_{1}=\begin{bmatrix} 260.608 &amp; -188.093 \\ -188.093 &amp; 1399.086 \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\underline{\overline{\mathbf{x}}}_{\ 2}= \begin{bmatrix} 137.46 \\ 366.62 \end{bmatrix}\ \ \ \ ; \ \ \ \ \ \mathbf{S}_{2}=\begin{bmatrix} 326.09 &amp; 133.505 \\ 133.505 &amp; 893.261 \end{bmatrix}
\]</span></p>
<p>además,
<span class="math display">\[
\mathbf{S}_{pooled}=\begin{bmatrix} 293.349 &amp; -27.2939 \\ -27.2939 &amp; 1146.1735 \end{bmatrix} \ \ \ \ ; \ \ \ \ \ \mathbf{S}_{pooled}^{-1}=\begin{bmatrix} 0.0034 &amp; 8\times 10^{-5} \\ 8\times 10^{-5} &amp; 8.7\times 10^{-4} \end{bmatrix}
\]</span></p>
<p>Los datos parecen satisfacer el supuesto de distribuciones normales bivariadas, pero las matrices de covarianza pueden diferir. Sin embargo, para ilustrar un punto concerniente a las probabilidades de mal clasificación, se utiliza el procedimiento de clasificación lineal.</p>
<p>Las tasas de Mal Clasificación se calculan basados en la función discriminante lineal dadas en las ecuaciones <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm2">(9.19)</a> y <a href="dos-pob-nm.html#eq:regla-clasificacion-estimada-pob-nm3">(9.20)</a>, la cual compara a:</p>
<p><span class="math display">\[
\hat{y}=(\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \underline{\mathbf{x}}=\hat{\mathbf{a}}^t\ \underline{\mathbf{x}}, \ \ \text{evaluada en} \ \  \underline{\mathbf{x}}_0
\]</span>
donde:
<span class="math display">\[
\hat{\mathbf{a}}=\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})=\begin{bmatrix} 0.0034 &amp; 8\times 10^{-5} \\ 8\times 10^{-5} &amp; 8.7\times 10^{-4} \end{bmatrix} \begin{bmatrix}
-39.08 \\ 63.04 \end{bmatrix}=\begin{bmatrix} -0.1284 \\ 0.0519 \end{bmatrix}
\]</span></p>
<p>es decir,
<span class="math display">\[
\hat{y}=\hat{\mathbf{a}}^t\ \underline{\mathbf{x}}=\begin{bmatrix} -0.12839 &amp; 0.05194 \end{bmatrix} \begin{bmatrix} X_1  \\ X_2 \end{bmatrix} = -0.12839X_1+0.05194X_2
\]</span></p>
<p><strong>con el número</strong>:
<span class="math display">\[
\widehat{m}=\frac{1}{2} (\overline{\underline{\mathbf{x}}}_{\ 1} -\overline{\underline{\mathbf{x}}}_{\ 2} )^t
\mathbf{S}_{Pol}^{-1}(\overline{\underline{\mathbf{x}}}_{\ 1} + \overline{\underline{\mathbf{x}}}_{\ 2} )=\frac{1}{2}(\overline{y}_1+\overline{y}_2)
\]</span></p>
<p>donde,
<span class="math display">\[
\overline{y}_1=(\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \overline{\underline{\mathbf{x}}}_{\ 1}=\hat{\mathbf{a}}^t\ \overline{\underline{\mathbf{x}}}_{\ 1}=\begin{bmatrix} -0.12839 &amp; 0.05194 \end{bmatrix}
\begin{bmatrix}
98.38 \\ 429.66 \end{bmatrix}=
9.6871
\]</span></p>
<p><span class="math display">\[
\overline{y}_2=(\overline{\underline{\mathbf{x}}}_{\ 1}-\overline{\underline{\mathbf{x}}}_{\ 2})^t
\mathbf{S}_{Pol}^{-1}\ \overline{\underline{\mathbf{x}}}_{\ 2}=\hat{\mathbf{a}}^t\ \overline{\underline{\mathbf{x}}}_{\ 2}=\begin{bmatrix} -0.12839 &amp; 0.05194 \end{bmatrix}
\begin{bmatrix}
137.46 \\ 366.62 \end{bmatrix}= 1.3953
\]</span></p>
<p>es decir con:
<span class="math display">\[
\widehat{m}=\frac{1}{2}(\overline{y}_1+\overline{y}_2)=\frac{1}{2}(9.6871 + 1.3953) =
5.5412
\]</span></p>
<p><strong>La Regla de Clasificación se Reduce a</strong>:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0} \in R_1 \ \text{si} \ \ \ \widehat{y\ }_0 \geq \widehat{m}, \ \ \ \ \text{y} \ \ \  
\underline{\mathbf{x}}_{ 0} \in R_2 \ \ \text{si} \ \ \ \widehat{y\ }_0 &lt; \widehat{m}.
\]</span></p>
<p>o equivalentemente:
<span class="math display">\[
\underline{\mathbf{x}}_{\ 0} \in R_1 \ \text{si} \ \ \ \widehat{w}=\widehat{y\ }_0 - \widehat{m} \geq 0, \ \ \ \ \text{y} \ \ \  
\underline{\mathbf{x}}_{ 0} \in R_2 \ \ \text{si} \ \ \ \widehat{w}=\widehat{y\ }_0 - \widehat{m} &lt;0.
\]</span>
Es decir:
<span class="math display">\[
\widehat{w}=\widehat{y\ }_0 - \widehat{m} = -0.12839X_{10}+0.05194X_{20} -  5.5412
\]</span></p>
<p>El proceso de clasificación, utilizando la función discriminante lineal anterior, costos iguales y probabilidades aprioris iguales, produce las siguientes tasas de error estimadas:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Población</th>
<th>Asignada</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th><span class="math inline">\(\pi_1\)</span>:Alaska</th>
<th><span class="math inline">\(\pi_2\)</span>:Canadá</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Población</td>
<td><span class="math inline">\(\pi_1\)</span>:Alaska</td>
<td>44</td>
<td>6</td>
</tr>
<tr class="even">
<td>Actual</td>
<td><span class="math inline">\(\pi_1\)</span>:Canadá</td>
<td>1</td>
<td>49</td>
</tr>
</tbody>
</table>
<p>Las Observaciones Mal Clasificadas son:</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-308">Tabla 9.3: </span>Observaciones de Alaskas Clasificadas como de Canadá
</caption>
<thead>
<tr>
<th style="text-align:right;">
Región
</th>
<th style="text-align:right;">
Género
</th>
<th style="text-align:right;">
Agua_Dulce
</th>
<th style="text-align:right;">
Agua_Marina
</th>
<th style="text-align:right;">
w
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
108
</td>
<td style="text-align:right;">
368
</td>
<td style="text-align:right;">
-0.2920
</td>
<td style="text-align:right;">
5.249
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
131
</td>
<td style="text-align:right;">
355
</td>
<td style="text-align:right;">
-3.9201
</td>
<td style="text-align:right;">
1.621
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
372
</td>
<td style="text-align:right;">
-2.0100
</td>
<td style="text-align:right;">
3.531
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
372
</td>
<td style="text-align:right;">
-2.0100
</td>
<td style="text-align:right;">
3.531
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
118
</td>
<td style="text-align:right;">
381
</td>
<td style="text-align:right;">
-0.9006
</td>
<td style="text-align:right;">
4.641
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:right;">
403
</td>
<td style="text-align:right;">
-0.1430
</td>
<td style="text-align:right;">
5.398
</td>
</tr>
</tbody>
</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-309">Tabla 9.4: </span>Observaciones de Canadá Clasificadas como de Alaska
</caption>
<thead>
<tr>
<th style="text-align:right;">
Región
</th>
<th style="text-align:right;">
Género
</th>
<th style="text-align:right;">
Agua_Dulce
</th>
<th style="text-align:right;">
Agua_Marina
</th>
<th style="text-align:right;">
w
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
90
</td>
<td style="text-align:right;">
385
</td>
<td style="text-align:right;">
2.902
</td>
<td style="text-align:right;">
8.443
</td>
</tr>
</tbody>
</table>
<p>Luego de evaluar la función discriminante lineal anterior, <span class="math inline">\(\hat{w}\)</span>, en todos las datos muestrales se tienen algunas diferencias en las desviaciones estándar muestrales de <span class="math inline">\(\hat{w}\)</span> para los dos poblaciones consideradas.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Media</th>
<th>Desv_Estándar</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>n</th>
<th>Muestral</th>
<th>Muestral</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alaska</td>
<td>50</td>
<td>4.146</td>
<td>3.253</td>
</tr>
<tr class="even">
<td>Canadá</td>
<td>50</td>
<td>-4.146</td>
<td>2.45</td>
</tr>
</tbody>
</table>
<p>Aunque la tasa de error global de mal clasificación (7/100, ie. <span class="math inline">\(7\%\)</span>), es bastante baja, existe aquí una
injusticia. Es menos probable que un salmón nacido en Canadá sea clasificado erróneamente como como nacido en Alaska, y no al revés. La Figura <a href="evaluación-de-las-funciones-de-clasificación.html#fig:grafico-ejemplo2-funcion-discriminante-lineal">9.8</a>, muestra las dos densidades normales para la función discriminante lineal dada por:
<span class="math display">\[
\widehat{y\ } = -0.12839X_1+0.05194X_2
\]</span></p>
<p>lo cual explica este fenómeno.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico-ejemplo2-funcion-discriminante-lineal"></span>
<img src="bookdown-iam_files/figure-html/grafico-ejemplo2-funcion-discriminante-lineal-1.png" alt="Densidades de los Datos (Dos  Poblaciones de Salmon)" width="80%" />
<p class="caption">
Figura 9.8: Densidades de los Datos (Dos Poblaciones de Salmon)
</p>
</div>
<p>Para esta función discriminante lineal <span class="math inline">\(\hat{y}\)</span> se tiene que:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Media</th>
<th>Desv_Estándar</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th></th>
<th>n</th>
<th>Muestral</th>
<th>Muestral</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alaska</td>
<td>50</td>
<td>9.6871</td>
<td>1.8035</td>
</tr>
<tr class="even">
<td>Canadá</td>
<td>50</td>
<td>1.3953</td>
<td>1.5654</td>
</tr>
</tbody>
</table>
<p>Usar el punto medio entre las medias de las dos muestras no justifica que las dos probabilidaddes de mal clasificación sean iguales. Ésto claramente penaliza a la población con mayor varianza.
Por lo tanto, la adherencia ciega al procedimiento de clasificación lineal puede ser imprudente.</p>
<p>Debería quedar intuitivamente claro que una buena clasificación (bajas tasas de error) depende de la separación de las poblaciones. Cuanto más separados estén los grupos, es más probable que se pueda desarrollar un método de clasificación útil.</p>
<p>Como veremos, las reglas de asignación apropiadas para el caso de probabilidades aprioris iguales y costos de clasificación errónea iguales corresponden a funciones diseñadas para poblaciones máximamente separadas. Es en esta situación que empezamos a perder la distinción entre clasificación y separación.</p>
</div>
</div>
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-johnson2007applied" class="csl-entry">
Johnson, Richard A, and Dean W Wichern. 2007. <em>Applied Multivariate Statistical Analysis. 6th</em>. <em>New Jersey, US: Pearson Prentice Hall</em>.
</div>
<div id="ref-lachenbruch1975" class="csl-entry">
Lachenbruch, Peter A. 1975. <em>Discriminant Analysis, New York: Hafner</em>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dos-pob-nm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-iam.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
