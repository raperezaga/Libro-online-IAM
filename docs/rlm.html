<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 Modelo de Regresión Lineal Múltiple (RLM) | Chapter 10</title>
  <meta name="description" content="Este libro contine ……" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 Modelo de Regresión Lineal Múltiple (RLM) | Chapter 10" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/imagenes/imagen1.png" />
  <meta property="og:description" content="Este libro contine ……" />
  <meta name="github-repo" content="raperezaga/iam" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 Modelo de Regresión Lineal Múltiple (RLM) | Chapter 10" />
  
  <meta name="twitter:description" content="Este libro contine ……" />
  <meta name="twitter:image" content="/imagenes/imagen1.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducción-2.html"/>
<link rel="next" href="mrl-multiv.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preámbulo</a>
<ul>
<li class="chapter" data-level="" data-path="descripción.html"><a href="descripción.html"><i class="fa fa-check"></i>Descripción</a></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="" data-path="dedicación.html"><a href="dedicación.html"><i class="fa fa-check"></i>Dedicación</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="paquetes-usados-en-el-libro.html"><a href="paquetes-usados-en-el-libro.html"><i class="fa fa-check"></i>Paquetes usados en el libro</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="rep-al.html"><a href="rep-al.html"><i class="fa fa-check"></i><b>1</b> Repaso de álgebra lineal</a>
<ul>
<li class="chapter" data-level="1.1" data-path="acb-al.html"><a href="acb-al.html"><i class="fa fa-check"></i><b>1.1</b> Algunos conceptos básicos</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="acb-al.html"><a href="acb-al.html#matrices"><i class="fa fa-check"></i><b>1.1.1</b> Matrices</a></li>
<li class="chapter" data-level="1.1.2" data-path="acb-al.html"><a href="acb-al.html#vectores"><i class="fa fa-check"></i><b>1.1.2</b> Vectores</a></li>
<li class="chapter" data-level="1.1.3" data-path="acb-al.html"><a href="acb-al.html#operaciones-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Operaciones Matriciales</a></li>
<li class="chapter" data-level="1.1.4" data-path="acb-al.html"><a href="acb-al.html#matrices-especiales"><i class="fa fa-check"></i><b>1.1.4</b> Matrices Especiales</a></li>
<li class="chapter" data-level="1.1.5" data-path="acb-al.html"><a href="acb-al.html#descomp-espectral"><i class="fa fa-check"></i><b>1.1.5</b> Descomposición Espectral de una Matriz (eigen-descomposición)</a></li>
<li class="chapter" data-level="1.1.6" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-una-matriz"><i class="fa fa-check"></i><b>1.1.6</b> Diagonalización de una Matriz</a></li>
<li class="chapter" data-level="1.1.7" data-path="acb-al.html"><a href="acb-al.html#diagonalización-ortogonal"><i class="fa fa-check"></i><b>1.1.7</b> Diagonalización Ortogonal</a></li>
<li class="chapter" data-level="1.1.8" data-path="acb-al.html"><a href="acb-al.html#diagonalizacion-inversa"><i class="fa fa-check"></i><b>1.1.8</b> Diagonalización de la Inversa de una Matriz</a></li>
<li class="chapter" data-level="1.1.9" data-path="acb-al.html"><a href="acb-al.html#diagonalización-de-la-matriz-raíz-cuadrada"><i class="fa fa-check"></i><b>1.1.9</b> Diagonalización de la Matriz Raíz Cuadrada</a></li>
<li class="chapter" data-level="1.1.10" data-path="acb-al.html"><a href="acb-al.html#traza-determinante-y-rango-de-una-matriz"><i class="fa fa-check"></i><b>1.1.10</b> Traza, Determinante y Rango de una Matriz</a></li>
<li class="chapter" data-level="1.1.11" data-path="acb-al.html"><a href="acb-al.html#formas-cuadraticas"><i class="fa fa-check"></i><b>1.1.11</b> Formas Cuadráticas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><a href="algunas-propiedades-estadísticas-de-la-descomposición-de-una-matriz-en-valores-y-vectores-propios.html"><i class="fa fa-check"></i><b>1.2</b> Algunas Propiedades Estadísticas de la Descomposición de una Matriz en Valores y Vectores Propios</a></li>
<li class="chapter" data-level="1.3" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html"><i class="fa fa-check"></i><b>1.3</b> Diferenciación con Vectores y Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="diferenc_vectores.html"><a href="diferenc_vectores.html#vector-gradiente-para-diferentes-definiciones-de-funderlinemathbfx"><i class="fa fa-check"></i><b>1.3.1</b> Vector-Gradiente para diferentes definiciones de <span class="math inline">\(f(\underline{\mathbf{x}})\)</span>:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="org_pres_dat.html"><a href="org_pres_dat.html"><i class="fa fa-check"></i><b>2</b> Organización y Presentación de Datos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html"><i class="fa fa-check"></i><b>2.1</b> Resumenes Descriptivos</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#matriz-de-datos"><i class="fa fa-check"></i><b>2.1.1</b> Matriz de Datos</a></li>
<li class="chapter" data-level="2.1.2" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#estadísticos-descriptivos"><i class="fa fa-check"></i><b>2.1.2</b> Estadísticos descriptivos</a></li>
<li class="chapter" data-level="2.1.3" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#algunas-notaciones"><i class="fa fa-check"></i><b>2.1.3</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="2.1.4" data-path="resumenes-descriptivos.html"><a href="resumenes-descriptivos.html#representación-gráfica-de-observaciones-multivariadas"><i class="fa fa-check"></i><b>2.1.4</b> Representación Gráfica de Observaciones multivariadas</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vectores-y-matrices-aleatorias.html"><a href="vectores-y-matrices-aleatorias.html"><i class="fa fa-check"></i><b>2.2</b> Vectores y Matrices Aleatorias</a></li>
<li class="chapter" data-level="2.3" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html"><i class="fa fa-check"></i><b>2.3</b> Vectores y Matrices Poblacionales Particionados</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-del-vector-de-medias-poblacionales"><i class="fa fa-check"></i><b>2.3.1</b> Particionamiento del Vector de Medias-Poblacionales</a></li>
<li class="chapter" data-level="2.3.2" data-path="vectores-y-matrices-poblacionales-particionados.html"><a href="vectores-y-matrices-poblacionales-particionados.html#particionamiento-de-la-matriz-de-var-cov-poblacional-mathbfsigma"><i class="fa fa-check"></i><b>2.3.2</b> Particionamiento de la Matriz de Var-Cov Poblacional <span class="math inline">\(\mathbf{\Sigma}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.4</b> Propiedades Sobre la Media y Varianza de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.5" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html"><i class="fa fa-check"></i><b>2.5</b> Vectores y Matrices Muestrales Particionadas</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-del-vector-de-medias-muestrales"><i class="fa fa-check"></i><b>2.5.1</b> Particionamiento del Vector de Medias Muestrales</a></li>
<li class="chapter" data-level="2.5.2" data-path="vectores-y-matrices-muestrales-particionadas.html"><a href="vectores-y-matrices-muestrales-particionadas.html#particionamiento-de-la-matriz-de-var-cov-muestrales"><i class="fa fa-check"></i><b>2.5.2</b> Particionamiento de la Matriz de Var-Cov Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="algunas-formas-matriciales-eficientes.html"><a href="algunas-formas-matriciales-eficientes.html"><i class="fa fa-check"></i><b>2.6</b> Algunas formas matriciales Eficientes</a></li>
<li class="chapter" data-level="2.7" data-path="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><a href="propiedades-sobre-la-media-y-varianza-muestral-de-combinaciones-lineales.html"><i class="fa fa-check"></i><b>2.7</b> Propiedades Sobre la Media y Varianza Muestral de Combinaciones Lineales</a></li>
<li class="chapter" data-level="2.8" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><i class="fa fa-check"></i><b>2.8</b> Varianza Generalizada Muestral y su Interpretación Geométrica</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-1-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.1</b> Interpretación Geométrica-1 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.2" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#interpretación-geométrica-2-de-la-varianza-generalizada"><i class="fa fa-check"></i><b>2.8.2</b> Interpretación Geométrica-2 de la Varianza Generalizada</a></li>
<li class="chapter" data-level="2.8.3" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#varianza-generalizada-determinada-por-la-matriz-de-correlación-muestral-mathbfr"><i class="fa fa-check"></i><b>2.8.3</b> Varianza Generalizada Determinada por la Matriz de Correlación Muestral <span class="math inline">\(\mathbf{R}\)</span></a></li>
<li class="chapter" data-level="2.8.4" data-path="varianza-generalizada-muestral-y-su-interpretación-geométrica.html"><a href="varianza-generalizada-muestral-y-su-interpretación-geométrica.html#relación-entre-mathbfs-y-mathbfr"><i class="fa fa-check"></i><b>2.8.4</b> Relación entre <span class="math inline">\(|\mathbf{S}|\)</span> y <span class="math inline">\(|\mathbf{R}|\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="varianza-total-muestral.html"><a href="varianza-total-muestral.html"><i class="fa fa-check"></i><b>2.9</b> Varianza Total Muestral</a></li>
<li class="chapter" data-level="2.10" data-path="muestra-aleatoria-de-distribuciones-p-variadas.html"><a href="muestra-aleatoria-de-distribuciones-p-variadas.html"><i class="fa fa-check"></i><b>2.10</b> Muestra Aleatoria de Distribuciones <span class="math inline">\(p\)</span>-Variadas</a></li>
<li class="chapter" data-level="2.11" data-path="distancias.html"><a href="distancias.html"><i class="fa fa-check"></i><b>2.11</b> Distancias</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="distancias.html"><a href="distancias.html#dist_euclidea"><i class="fa fa-check"></i><b>2.11.1</b> Definición de Algunas Distancias</a></li>
<li class="chapter" data-level="2.11.2" data-path="distancias.html"><a href="distancias.html#relación-de-la-distancia-de-mahalanobis-con-la-distribución-chi-cuadrado"><i class="fa fa-check"></i><b>2.11.2</b> Relación de la Distancia de Mahalanobis con la Distribución chi-Cuadrado</a></li>
<li class="chapter" data-level="2.11.3" data-path="distancias.html"><a href="distancias.html#ejemplo"><i class="fa fa-check"></i><b>2.11.3</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Normal-Multiv.html"><a href="Normal-Multiv.html"><i class="fa fa-check"></i><b>3</b> Distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="geometria-NM.html"><a href="geometria-NM.html"><i class="fa fa-check"></i><b>3.1</b> Geometría y propiedades de la NM</a></li>
<li class="chapter" data-level="3.2" data-path="normal-univariada.html"><a href="normal-univariada.html"><i class="fa fa-check"></i><b>3.2</b> Normal Univariada</a></li>
<li class="chapter" data-level="3.3" data-path="normal-multivariada.html"><a href="normal-multivariada.html"><i class="fa fa-check"></i><b>3.3</b> Normal Multivariada</a></li>
<li class="chapter" data-level="3.4" data-path="algunos-aspectos-geométricos-de-la-nm.html"><a href="algunos-aspectos-geométricos-de-la-nm.html"><i class="fa fa-check"></i><b>3.4</b> Algunos Aspectos Geométricos de la NM</a></li>
<li class="chapter" data-level="3.5" data-path="prop-nm.html"><a href="prop-nm.html"><i class="fa fa-check"></i><b>3.5</b> Propiedades de la distribución Normal Multivariada</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="prop-nm.html"><a href="prop-nm.html#prop1"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Propiedad-1:</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="prop-nm.html"><a href="prop-nm.html#prop2"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Propiedad-2:</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="prop-nm.html"><a href="prop-nm.html#prop3"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Propiedad-3:</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="prop-nm.html"><a href="prop-nm.html#prop4"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Propiedad-4:</strong></a></li>
<li class="chapter" data-level="3.5.5" data-path="prop-nm.html"><a href="prop-nm.html#prop5"><i class="fa fa-check"></i><b>3.5.5</b> <strong>Propiedad-5:</strong></a></li>
<li class="chapter" data-level="3.5.6" data-path="prop-nm.html"><a href="prop-nm.html#prop6"><i class="fa fa-check"></i><b>3.5.6</b> <strong>Propiedad-6:</strong></a></li>
<li class="chapter" data-level="3.5.7" data-path="prop-nm.html"><a href="prop-nm.html#prop7"><i class="fa fa-check"></i><b>3.5.7</b> <strong>Propiedad-7:</strong></a></li>
<li class="chapter" data-level="3.5.8" data-path="prop-nm.html"><a href="prop-nm.html#prop8"><i class="fa fa-check"></i><b>3.5.8</b> <strong>Propiedad-8:</strong></a></li>
<li class="chapter" data-level="3.5.9" data-path="prop-nm.html"><a href="prop-nm.html#prop9"><i class="fa fa-check"></i><b>3.5.9</b> <strong>Propiedad-9:</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.6</b> Evaluación del Supuesto de Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-a-nivel-marginal-ie.-normalidad-univariada"><i class="fa fa-check"></i><b>3.6.1</b> Evaluación a nivel marginal (ie. Normalidad Univariada)</a></li>
<li class="chapter" data-level="3.6.2" data-path="evaluación-del-supuesto-de-normalidad-multivariada.html"><a href="evaluación-del-supuesto-de-normalidad-multivariada.html#evaluación-de-la-normalidad-bi-variada"><i class="fa fa-check"></i><b>3.6.2</b> Evaluación de la Normalidad Bi-variada</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="detección-de-observaciones-atípicas.html"><a href="detección-de-observaciones-atípicas.html"><i class="fa fa-check"></i><b>3.7</b> Detección de Observaciones Atípicas</a></li>
<li class="chapter" data-level="3.8" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><i class="fa fa-check"></i><b>3.8</b> Transformaciones para Acercar a la Normalidad Multivariada</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#familia-de-transformaciones-de-potencia-de-box-y-cox-1964"><i class="fa fa-check"></i><b>3.8.1</b> Familia de Transformaciones de Potencia de Box y Cox (1964)</a></li>
<li class="chapter" data-level="3.8.2" data-path="transformaciones-para-acercar-a-la-normalidad-multivariada.html"><a href="transformaciones-para-acercar-a-la-normalidad-multivariada.html#transformaciones-para-el-caso-multivariado"><i class="fa fa-check"></i><b>3.8.2</b> Transformaciones para el Caso Multivariado:</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html"><i class="fa fa-check"></i><b>3.9</b> Muestra Aleatoria Normal <span class="math inline">\(p\)</span>-Variada</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#estimadores-de-máx-ver-de-una-normal-multivariada"><i class="fa fa-check"></i><b>3.9.1</b> Estimadores de Máx-Ver de una Normal-Multivariada</a></li>
<li class="chapter" data-level="3.9.2" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#distribución-muestral-de-overlineunderlinemathbfx-y-mathbfs_n"><i class="fa fa-check"></i><b>3.9.2</b> Distribución Muestral de <span class="math inline">\(\overline{\underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\mathbf{S}_n\)</span></a></li>
<li class="chapter" data-level="3.9.3" data-path="muestra-aleatoria-normal-p-variada.html"><a href="muestra-aleatoria-normal-p-variada.html#comportamiento-de-underlineoverlinemathbfx_p-y-mathbfs-para-tamaños-muestrales-grandes"><i class="fa fa-check"></i><b>3.9.3</b> Comportamiento de <span class="math inline">\(\underline{\overline{\mathbf{x}}}_p\)</span> y <span class="math inline">\(\mathbf{S}\)</span> para Tamaños Muestrales Grandes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferen-estad.html"><a href="inferen-estad.html"><i class="fa fa-check"></i><b>4</b> Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i><b>4.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2" data-path="inferencia-estadística-para-la-media-mu-caso-univariado.html"><a href="inferencia-estadística-para-la-media-mu-caso-univariado.html"><i class="fa fa-check"></i><b>4.2</b> Inferencia Estadística para la Media (<span class="math inline">\(\mu\)</span>)-caso univariado</a></li>
<li class="chapter" data-level="4.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><i class="fa fa-check"></i><b>4.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-desconocida-normal"><i class="fa fa-check"></i><b>4.3.1</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Desconocida (Normal)</a></li>
<li class="chapter" data-level="4.3.2" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-cuando-mathbfsigma-es-conocida-población-normal"><i class="fa fa-check"></i><b>4.3.2</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> cuando <span class="math inline">\(\mathbf{\Sigma}\)</span> es Conocida (Población: Normal)</a></li>
<li class="chapter" data-level="4.3.3" data-path="pruebas-de-hipótesis-para-underlineboldsymbolmu.html"><a href="pruebas-de-hipótesis-para-underlineboldsymbolmu.html#pruebas-de-hipótesis-para-underlineboldsymbolmu-en-muestra-grande"><i class="fa fa-check"></i><b>4.3.3</b> Pruebas de Hipótesis para <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span> en Muestra Grande</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><i class="fa fa-check"></i><b>4.4</b> PH Acerca de Contrastes del Vector de Medias Poblacional <span class="math inline">\(\underline{\boldsymbol{\mu}}\)</span>, de una <span class="math inline">\(N_p(\underline{\boldsymbol{\mu}} \ , \ \mathbf{\Sigma} )\)</span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-para-el-caso-de-pnm"><i class="fa fa-check"></i><b>4.4.1</b> PH de Contrastes para el Caso de PNM</a></li>
<li class="chapter" data-level="4.4.2" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#ph-de-contrastes-en-en-caso-de-n-grande"><i class="fa fa-check"></i><b>4.4.2</b> PH de Contrastes en en caso de <span class="math inline">\(n\)</span>-Grande</a></li>
<li class="chapter" data-level="4.4.3" data-path="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html"><a href="ph-acerca-de-contrastes-del-vector-de-medias-poblacional-underlineboldsymbolmu-de-una-n_punderlineboldsymbolmu-mathbfsigma.html#prueba-de-hipótesis-para-igualdad-de-medias"><i class="fa fa-check"></i><b>4.4.3</b> Prueba de hipótesis para igualdad de medias</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><i class="fa fa-check"></i><b>4.5</b> PH para Igualdad de Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}=\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida"><i class="fa fa-check"></i><b>4.5.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.5.2" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida"><i class="fa fa-check"></i><b>4.5.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.5.3" data-path="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html"><a href="ph-para-igualdad-de-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfxunderlineboldsymbolmu_-underlinemathbfy.html#caso-3.-mathbfsigma_-underlinemathbfxneq-mathbfsigma_-underlinemathbfy-desconocida"><i class="fa fa-check"></i><b>4.5.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}\neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocida</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><i class="fa fa-check"></i><b>4.6</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,   para muestras grandes</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-1.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-conocida-1"><i class="fa fa-check"></i><b>4.6.1</b> Caso-1. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Conocida</a></li>
<li class="chapter" data-level="4.6.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-2.-mathbfsigma_-underlinemathbfxmathbfsigma_-underlinemathbfymathbfsigma-desconocida-1"><i class="fa fa-check"></i><b>4.6.2</b> Caso-2. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}}=\mathbf{\Sigma}_{\ \underline{\mathbf{y}}}=\mathbf{\Sigma}\)</span>-Desconocida</a></li>
<li class="chapter" data-level="4.6.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-para-muestras-grandes.html#caso-3.-mathbfsigma_-underlinemathbfx-neq-mathbfsigma_-underlinemathbfy-desconocidas"><i class="fa fa-check"></i><b>4.6.3</b> Caso-3. <span class="math inline">\(\mathbf{\Sigma}_{\ \underline{\mathbf{x}}} \neq \mathbf{\Sigma}_{\ \underline{\mathbf{y}}}\)</span>-Desconocidas</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><i class="fa fa-check"></i><b>4.7</b> Pruebas de Hipótesis Acerca de dos Vectores de Medias Poblacionales <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{x}}}\)</span> y <span class="math inline">\(\underline{\boldsymbol{\mu}}_{\ \underline{\mathbf{y}}}\)</span>,      Observaciones Pareadas</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-univariado"><i class="fa fa-check"></i><b>4.7.1</b> Caso Univariado</a></li>
<li class="chapter" data-level="4.7.2" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#caso-multivariado"><i class="fa fa-check"></i><b>4.7.2</b> Caso Multivariado</a></li>
<li class="chapter" data-level="4.7.3" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#uso-de-contrastes-para-la-comparación-de-medias-en-muestras-pareadas"><i class="fa fa-check"></i><b>4.7.3</b> Uso de Contrastes para la Comparación de Medias en Muestras Pareadas</a></li>
<li class="chapter" data-level="4.7.4" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#un-diseño-de-medidas-repetidas-para-comparar-tratamientos"><i class="fa fa-check"></i><b>4.7.4</b> Un Diseño de Medidas Repetidas para Comparar Tratamientos</a></li>
<li class="chapter" data-level="4.7.5" data-path="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html"><a href="pruebas-de-hipótesis-acerca-de-dos-vectores-de-medias-poblacionales-underlineboldsymbolmu_-underlinemathbfx-y-underlineboldsymbolmu_-underlinemathbfy-observaciones-pareadas.html#análisis-de-perfiles"><i class="fa fa-check"></i><b>4.7.5</b> Análisis de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html"><i class="fa fa-check"></i><b>4.8</b> Inferencia para la Matriz de Varianzas Covarianza</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueva-de-rv-sigma"><i class="fa fa-check"></i><b>4.8.1</b> Pruba de Razón de Verosimilitud</a></li>
<li class="chapter" data-level="4.8.2" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#prueba-de-bartlet"><i class="fa fa-check"></i><b>4.8.2</b> Prueba de Bartlet</a></li>
<li class="chapter" data-level="4.8.3" data-path="inferencia-para-la-matriz-de-varianzas-covarianza.html"><a href="inferencia-para-la-matriz-de-varianzas-covarianza.html#dos-o-más-matrices-de-varianzas-covarianzas"><i class="fa fa-check"></i><b>4.8.3</b> Dos o más Matrices de Varianzas Covarianzas</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><i class="fa fa-check"></i><b>4.9</b> Regiones de Confianza y Comparaciones Simultáneas entre las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#construcción-de-una-región-de-confianza-para-underlineboldsymbol-mu-cuando-la-población-tiene-distribución-n_punderlineboldsymbol-mumathbfsigma-con-underlineboldsymbol-mu-y-mathbfsigma-desconocidos"><i class="fa fa-check"></i><b>4.9.1</b> Construcción de una Región de Confianza para <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> Cuando la Población tiene Distribución <span class="math inline">\(N_p(\underline{\boldsymbol \mu},\mathbf{\Sigma})\)</span>, con <span class="math inline">\(\underline{\boldsymbol \mu}\)</span> y <span class="math inline">\(\mathbf{\Sigma}\)</span> desconocidos</a></li>
<li class="chapter" data-level="4.9.2" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#región-de-confianza-para-el-caso-de-p2-elipse-de-confianza"><i class="fa fa-check"></i><b>4.9.2</b> Región de Confianza para el Caso de <span class="math inline">\(p=2\)</span> (Elipse de Confianza)</a></li>
<li class="chapter" data-level="4.9.3" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-para-las-componentes-del-vector-de-medias-underlineboldsymbol-mu"><i class="fa fa-check"></i><b>4.9.3</b> Intervalos de Confianza Simultáneos para las Componentes del Vector de Medias <span class="math inline">\(\underline{\boldsymbol \mu}\)</span></a></li>
<li class="chapter" data-level="4.9.4" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-t2-simultáneos-para-diferencias-de-medias"><i class="fa fa-check"></i><b>4.9.4</b> IC <span class="math inline">\(T^2\)</span> Simultáneos para Diferencias de Medias</a></li>
<li class="chapter" data-level="4.9.5" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#IC-Bonferroni"><i class="fa fa-check"></i><b>4.9.5</b> Método de Bonferroni para Comparaciones Múltiples</a></li>
<li class="chapter" data-level="4.9.6" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#intervalos-de-confianza-simultáneos-chi2-caso-n-grande"><i class="fa fa-check"></i><b>4.9.6</b> Intervalos de Confianza Simultáneos <span class="math inline">\(\chi^2\)</span> (caso n-Grande)</a></li>
<li class="chapter" data-level="4.9.7" data-path="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html"><a href="regiones-de-confianza-y-comparaciones-simultáneas-entre-las-componentes-del-vector-de-medias-underlineboldsymbol-mu.html#ic-simultáneos-para-n-grande-usando-la-normal-estándar-z_alpha"><i class="fa fa-check"></i><b>4.9.7</b> IC Simultáneos para n-Grande Usando la Normal Estándar <span class="math inline">\(Z_\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mrlm.html"><a href="mrlm.html"><i class="fa fa-check"></i><b>5</b> Modelos de Regresión Lineal Multivariados</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introducción-2.html"><a href="introducción-2.html"><i class="fa fa-check"></i><b>5.1</b> Introducción</a></li>
<li class="chapter" data-level="5.2" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>5.2</b> Modelo de Regresión Lineal Múltiple (RLM)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="rlm.html"><a href="rlm.html#estimación-de-mínimos-cuadrados"><i class="fa fa-check"></i><b>5.2.1</b> Estimación de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="5.2.2" data-path="rlm.html"><a href="rlm.html#inferencias-acerca-del-modelo-de-regresión-lineal-múltiple"><i class="fa fa-check"></i><b>5.2.2</b> Inferencias Acerca del Modelo de Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="5.2.3" data-path="rlm.html"><a href="rlm.html#inferencias-a-partir-de-la-función-de-regresión-estimada"><i class="fa fa-check"></i><b>5.2.3</b> Inferencias A partir de la Función de Regresión Estimada</a></li>
<li class="chapter" data-level="5.2.4" data-path="rlm.html"><a href="rlm.html#validación-de-los-supuestos-del-modelo-y-otros-aspectos-del-la-regresión"><i class="fa fa-check"></i><b>5.2.4</b> Validación de los Supuestos del Modelo y Otros Aspectos del la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html"><i class="fa fa-check"></i><b>5.3</b> Modelo de Regresión Lineal Multivariado (RL-Multivariado)</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mrl-multiv.html"><a href="mrl-multiv.html#algunas-notaciones-1"><i class="fa fa-check"></i><b>5.3.1</b> Algunas Notaciones</a></li>
<li class="chapter" data-level="5.3.2" data-path="mrl-multiv.html"><a href="mrl-multiv.html#mrl-multivariado-en-forma-matricial"><i class="fa fa-check"></i><b>5.3.2</b> MRL-Multivariado en forma Matricial</a></li>
<li class="chapter" data-level="5.3.3" data-path="mrl-multiv.html"><a href="mrl-multiv.html#m-mrl-múltiples"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(m\)</span>-MRL-Múltiples</a></li>
<li class="chapter" data-level="5.3.4" data-path="mrl-multiv.html"><a href="mrl-multiv.html#estimador-de-mínimos-cuadrados-de-los-parámetros"><i class="fa fa-check"></i><b>5.3.4</b> Estimador de Mínimos Cuadrados de los Parámetros</a></li>
<li class="chapter" data-level="5.3.5" data-path="mrl-multiv.html"><a href="mrl-multiv.html#propiedades-de-estimadores-de-mínimos-cuadrados-del-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.5</b> Propiedades de Estimadores de Mínimos Cuadrados del MRL-Multivariado</a></li>
<li class="chapter" data-level="5.3.6" data-path="mrl-multiv.html"><a href="mrl-multiv.html#prv-mrl-multivariado"><i class="fa fa-check"></i><b>5.3.6</b> Prueba de Razón de Verosimilitud para Parámetros de Regresión en MRL-Multivariados</a></li>
<li class="chapter" data-level="5.3.7" data-path="mrl-multiv.html"><a href="mrl-multiv.html#otras-pruebas-estadísticas-multivariadas"><i class="fa fa-check"></i><b>5.3.7</b> Otras Pruebas Estadísticas Multivariadas</a></li>
<li class="chapter" data-level="5.3.8" data-path="mrl-multiv.html"><a href="mrl-multiv.html#predicciones-a-partir-de-un-modelo-de-regresión-múltiple-multivariado"><i class="fa fa-check"></i><b>5.3.8</b> Predicciones A Partir de un Modelo de Regresión Múltiple Multivariado</a></li>
<li class="chapter" data-level="5.3.9" data-path="mrl-multiv.html"><a href="mrl-multiv.html#el-concepto-de-regresión-lineal"><i class="fa fa-check"></i><b>5.3.9</b> El Concepto de Regresión Lineal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ACP.html"><a href="ACP.html"><i class="fa fa-check"></i><b>6</b> Análisis de Componentes Principales (ACP)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducción-3.html"><a href="introducción-3.html"><i class="fa fa-check"></i><b>6.1</b> Introducción</a></li>
<li class="chapter" data-level="6.2" data-path="interpretaciones-geométricas-y-algebraícas-del-acp.html"><a href="interpretaciones-geométricas-y-algebraícas-del-acp.html"><i class="fa fa-check"></i><b>6.2</b> Interpretaciones Geométricas y Algebraícas del ACP</a></li>
<li class="chapter" data-level="6.3" data-path="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><a href="interpretación-geométrica-del-acp-mediante-un-ejemplo-simple-p2.html"><i class="fa fa-check"></i><b>6.3</b> Interpretación Geométrica del ACP Mediante un Ejemplo Simple <span class="math inline">\(p=2\)</span></a></li>
<li class="chapter" data-level="6.4" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html"><i class="fa fa-check"></i><b>6.4</b> Mas de dos Ejes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#min-cuadrados"><i class="fa fa-check"></i><b>6.4.1</b> Ajuste de Mínimos Cuadrados</a></li>
<li class="chapter" data-level="6.4.2" data-path="mas-de-dos-ejes.html"><a href="mas-de-dos-ejes.html#relación-entre-los-sub-espacios-de-mathbbrp-y-de-mathbbrn"><i class="fa fa-check"></i><b>6.4.2</b> Relación entre los Sub-espacios de <span class="math inline">\(\mathbb{R}^p\)</span> y de <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="componentes-principales-en-análisis-de-regresión.html"><a href="componentes-principales-en-análisis-de-regresión.html"><i class="fa fa-check"></i><b>6.5</b> Componentes Principales en Análisis de Regresión</a></li>
<li class="chapter" data-level="6.6" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html"><i class="fa fa-check"></i><b>6.6</b> Componentes Principales Poblacionales</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#definicón-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.1</b> Definicón de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.2" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#determinación-de-las-cps-poblacionales"><i class="fa fa-check"></i><b>6.6.2</b> Determinación de las CPs Poblacionales</a></li>
<li class="chapter" data-level="6.6.3" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-derivadas-de-una-normal-multivariada"><i class="fa fa-check"></i><b>6.6.3</b> Componentes Principales Derivadas de una Normal Multivariada</a></li>
<li class="chapter" data-level="6.6.4" data-path="componentes-principales-poblacionales.html"><a href="componentes-principales-poblacionales.html#componentes-principales-usando-variables-estandarizadas"><i class="fa fa-check"></i><b>6.6.4</b> Componentes Principales usando Variables Estandarizadas</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><i class="fa fa-check"></i><b>6.7</b> Componentes Principales para Matrices de Var-Cov <span class="math inline">\(\mathbf{\Sigma}\)</span> con Estructuras Especiales</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#variables-no-correlacionadas"><i class="fa fa-check"></i><b>6.7.1</b> Variables No-Correlacionadas</a></li>
<li class="chapter" data-level="6.7.2" data-path="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html"><a href="componentes-principales-para-matrices-de-var-cov-mathbfsigma-con-estructuras-especiales.html#var-correlacionadas"><i class="fa fa-check"></i><b>6.7.2</b> Variables Altamente Correlacionadas</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="componentes-principales-muestrales.html"><a href="componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.8</b> Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.9" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html"><i class="fa fa-check"></i><b>6.9</b> Algunos Ejemplos de ACP</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo1"><i class="fa fa-check"></i><b>6.9.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.9.2" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-2"><i class="fa fa-check"></i><b>6.9.2</b> Ejemplo-2</a></li>
<li class="chapter" data-level="6.9.3" data-path="algunos-ejemplos-de-acp.html"><a href="algunos-ejemplos-de-acp.html#ejemplo-3"><i class="fa fa-check"></i><b>6.9.3</b> Ejemplo-3</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="cómo-elegir-el-número-de-componentes-principales.html"><a href="cómo-elegir-el-número-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.10</b> Cómo elegir el número de Componentes Principales?</a></li>
<li class="chapter" data-level="6.11" data-path="interpretación-de-las-componentes-principales-muestrales.html"><a href="interpretación-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.11</b> Interpretación de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.12" data-path="estandarización-de-las-componentes-principales-muestrales.html"><a href="estandarización-de-las-componentes-principales-muestrales.html"><i class="fa fa-check"></i><b>6.12</b> Estandarización de las Componentes Principales Muestrales</a></li>
<li class="chapter" data-level="6.13" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html"><i class="fa fa-check"></i><b>6.13</b> Gráficas en un Análisis de Componentes Principales</a>
<ul>
<li class="chapter" data-level="6.13.1" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#graficos-de-las-cps"><i class="fa fa-check"></i><b>6.13.1</b> Graficos de las CPS</a></li>
<li class="chapter" data-level="6.13.2" data-path="gráficas-en-un-análisis-de-componentes-principales.html"><a href="gráficas-en-un-análisis-de-componentes-principales.html#el-gráfico-biplot"><i class="fa fa-check"></i><b>6.13.2</b> El gráfico biplot:</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><a href="propiedades-de-hatlambda_i-y-underlinehatmathbfe_i-en-muestras-grandes.html"><i class="fa fa-check"></i><b>6.14</b> Propiedades de <span class="math inline">\(\hat{\lambda}_i\)</span> y <span class="math inline">\(\underline{\hat{\mathbf{e}}}_i\)</span> en Muestras Grandes</a></li>
<li class="chapter" data-level="6.15" data-path="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><a href="prueba-de-hipótesis-para-estructura-de-correlación-igual.html"><i class="fa fa-check"></i><b>6.15</b> Prueba de Hipótesis para Estructura de Correlación Igual</a></li>
<li class="chapter" data-level="6.16" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html"><i class="fa fa-check"></i><b>6.16</b> Ejemplos Finales</a>
<ul>
<li class="chapter" data-level="6.16.1" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-1"><i class="fa fa-check"></i><b>6.16.1</b> Ejemplo-1</a></li>
<li class="chapter" data-level="6.16.2" data-path="ejemplos-finales.html"><a href="ejemplos-finales.html#ejemplo-acp-con-individuos-y-variables-suplementarias"><i class="fa fa-check"></i><b>6.16.2</b> Ejemplo ACP con Individuos y Variables Suplementarias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="affc.html"><a href="affc.html"><i class="fa fa-check"></i><b>7</b> Análisis Factorial de Factores Comunes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="introducción-4.html"><a href="introducción-4.html"><i class="fa fa-check"></i><b>7.1</b> Introducción</a></li>
<li class="chapter" data-level="7.2" data-path="tipos-de-factores.html"><a href="tipos-de-factores.html"><i class="fa fa-check"></i><b>7.2</b> Tipos de Factores</a></li>
<li class="chapter" data-level="7.3" data-path="motivación-del-análisis-factorial.html"><a href="motivación-del-análisis-factorial.html"><i class="fa fa-check"></i><b>7.3</b> Motivación del Análisis Factorial</a></li>
<li class="chapter" data-level="7.4" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html"><i class="fa fa-check"></i><b>7.4</b> Enfoques del AF</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#métodos-para-realizar-la-extracción-de-los-factores"><i class="fa fa-check"></i><b>7.4.1</b> Métodos para realizar la extracción de los factores</a></li>
<li class="chapter" data-level="7.4.2" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#rotación-de-ejes"><i class="fa fa-check"></i><b>7.4.2</b> Rotación de Ejes</a></li>
<li class="chapter" data-level="7.4.3" data-path="enfoques-del-af.html"><a href="enfoques-del-af.html#puntuaciones-o-scores-de-los-sujetos-en-los-factores-extraídos"><i class="fa fa-check"></i><b>7.4.3</b> Puntuaciones o Scores de los Sujetos en los Factores Extraídos</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html"><i class="fa fa-check"></i><b>7.5</b> El Modelo de Factor Ortogonal</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#estructura-de-covarianzas-del-modelo-de-factor-ortogonal"><i class="fa fa-check"></i><b>7.5.1</b> Estructura de Covarianzas del Modelo de Factor Ortogonal</a></li>
<li class="chapter" data-level="7.5.2" data-path="el-modelo-de-factor-ortogonal.html"><a href="el-modelo-de-factor-ortogonal.html#ejemplos"><i class="fa fa-check"></i><b>7.5.2</b> Ejemplos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html"><i class="fa fa-check"></i><b>7.6</b> Métodos de Estimación</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-cp"><i class="fa fa-check"></i><b>7.6.1</b> El Método de la Componente Principal</a></li>
<li class="chapter" data-level="7.6.2" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-pa"><i class="fa fa-check"></i><b>7.6.2</b> Una Aproximación Modificada (La Solución del Factor Principal o de las CP-Iteradas o de Ejes Principales-PA)</a></li>
<li class="chapter" data-level="7.6.3" data-path="métodos-de-estimación.html"><a href="métodos-de-estimación.html#metodo-mle"><i class="fa fa-check"></i><b>7.6.3</b> El Método de Máxima Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="prueba-para-el-número-de-factores-muestra-grande..html"><a href="prueba-para-el-número-de-factores-muestra-grande..html"><i class="fa fa-check"></i><b>7.7</b> Prueba para el Número de Factores (Muestra Grande).</a></li>
<li class="chapter" data-level="7.8" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html"><i class="fa fa-check"></i><b>7.8</b> Rotación de Factores</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-cuando-m2-factores"><i class="fa fa-check"></i><b>7.8.1</b> Rotaciones cuando <span class="math inline">\(m=2\)</span>-Factores</a></li>
<li class="chapter" data-level="7.8.2" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#criterio-varimáx"><i class="fa fa-check"></i><b>7.8.2</b> Criterio Varimáx:</a></li>
<li class="chapter" data-level="7.8.3" data-path="rotación-de-factores.html"><a href="rotación-de-factores.html#rotaciones-oblicuas"><i class="fa fa-check"></i><b>7.8.3</b> Rotaciones Oblicuas</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html"><i class="fa fa-check"></i><b>7.9</b> Factores-Scores (o Puntuaciones) de los Factores</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#método-de-los-mínimos-cuadrados-ponderados"><i class="fa fa-check"></i><b>7.9.1</b> Método de los Mínimos Cuadrados Ponderados</a></li>
<li class="chapter" data-level="7.9.2" data-path="factores-scores-o-puntuaciones-de-los-factores.html"><a href="factores-scores-o-puntuaciones-de-los-factores.html#el-método-de-la-regresión"><i class="fa fa-check"></i><b>7.9.2</b> El Método de la Regresión</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><a href="perspectivas-y-estrategías-para-el-análisis-de-factor.html"><i class="fa fa-check"></i><b>7.10</b> Perspectivas y Estrategías para el Análisis de Factor</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="avm.html"><a href="avm.html"><i class="fa fa-check"></i><b>8</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introducción-5.html"><a href="introducción-5.html"><i class="fa fa-check"></i><b>8.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="introducción-5.html"><a href="introducción-5.html#suposiciones-del-manova"><i class="fa fa-check"></i><b>8.1.1</b> Suposiciones del MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>8.2</b> Análisis de Varianza Univariado (ANOVA)</a></li>
<li class="chapter" data-level="8.3" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>8.3</b> Análisis de Varianza Multivariado (MANOVA)</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="manova.html"><a href="manova.html#modelo-manova-para-comparar-g-vectores-de-medias-poblacionales"><i class="fa fa-check"></i><b>8.3.1</b> Modelo MANOVA para comparar <span class="math inline">\(g\)</span>-Vectores de Medias Poblacionales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="adiscriminante.html"><a href="adiscriminante.html"><i class="fa fa-check"></i><b>9</b> Análisis Discriminante y Clasificación</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introducción-6.html"><a href="introducción-6.html"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.2</b> Separación y Clasificación para el caso de dos poblaciones</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html"><a href="separación-y-clasificación-para-el-caso-de-dos-poblaciones.html#clasificación-de-dos-poblaciones"><i class="fa fa-check"></i><b>9.2.1</b> Clasificación de dos Poblaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.3</b> Regla de Discriminación para dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#costo-esperado-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.1</b> Costo Esperado de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.2" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#probabilidad-total-de-mal-clasificación"><i class="fa fa-check"></i><b>9.3.2</b> Probabilidad Total de Mal Clasificación</a></li>
<li class="chapter" data-level="9.3.3" data-path="regla-de-discriminación-para-dos-poblaciones.html"><a href="regla-de-discriminación-para-dos-poblaciones.html#regla-de-clasificación-de-bayes"><i class="fa fa-check"></i><b>9.3.3</b> Regla de Clasificación de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html"><i class="fa fa-check"></i><b>9.4</b> Clasificación con Dos Poblaciones Normales Multivariadas</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1mathbfsigma_2mathbfsigma"><i class="fa fa-check"></i><b>9.4.1</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1=\mathbf{\Sigma}_2=\mathbf{\Sigma}\)</span></a></li>
<li class="chapter" data-level="9.4.2" data-path="dos-pob-nm.html"><a href="dos-pob-nm.html#clasificación-con-dos-poblaciones-normales-donde-mathbfsigma_1neq-mathbfsigma_2"><i class="fa fa-check"></i><b>9.4.2</b> Clasificación con dos Poblaciones Normales donde <span class="math inline">\(\mathbf{\Sigma}_1\neq \mathbf{\Sigma}_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html"><i class="fa fa-check"></i><b>9.5</b> Evaluación de las Funciones de Clasificación</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-óptimo-teo"><i class="fa fa-check"></i><b>9.5.1</b> Tasa de Error Óptimo (TEO)</a></li>
<li class="chapter" data-level="9.5.2" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual"><i class="fa fa-check"></i><b>9.5.2</b> Tasa de Error Actual</a></li>
<li class="chapter" data-level="9.5.3" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-aparente-teap"><i class="fa fa-check"></i><b>9.5.3</b> Tasa de Error Aparente (TEAP)</a></li>
<li class="chapter" data-level="9.5.4" data-path="evaluación-de-las-funciones-de-clasificación.html"><a href="evaluación-de-las-funciones-de-clasificación.html#tasa-de-error-actual-esperada"><i class="fa fa-check"></i><b>9.5.4</b> Tasa de Error Actual Esperada</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.6</b> Clasificación con Varias Poblaciones (Es decir Más de dos Poblaciones)</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html"><a href="clasificación-con-varias-poblaciones-es-decir-más-de-dos-poblaciones.html#costo-esperado-de-mal-clasificación-ecm"><i class="fa fa-check"></i><b>9.6.1</b> Costo Esperado de Mal Clasificación (ECM)</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><i class="fa fa-check"></i><b>9.7</b> Clasificación con Poblaciones Normales y más de dos Poblaciones</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#un-clasificador-equivalente-para-el-caso-de-matrices-de-var-cov-iguales"><i class="fa fa-check"></i><b>9.7.1</b> Un Clasificador Equivalente para el Caso de Matrices de Var-Cov Iguales</a></li>
<li class="chapter" data-level="9.7.2" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#comparación-dos-a-dos-de-los-scores-de-la-función-de-clasificación-lineal"><i class="fa fa-check"></i><b>9.7.2</b> Comparación Dos a Dos de los Scores de la Función de Clasificación Lineal</a></li>
<li class="chapter" data-level="9.7.3" data-path="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html"><a href="clasificación-con-poblaciones-normales-y-más-de-dos-poblaciones.html#tasa-de-error-actual-esperada-aer-estimada"><i class="fa fa-check"></i><b>9.7.3</b> Tasa de Error Actual Esperada (AER) Estimada</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><i class="fa fa-check"></i><b>9.8</b> Método de Fisher Para Discriminar Entre Varias Poblaciones</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#uso-de-la-función-de-discriminación-de-fisher-para-clasificación"><i class="fa fa-check"></i><b>9.8.1</b> Uso de la Función de Discriminación de Fisher Para Clasificación</a></li>
<li class="chapter" data-level="9.8.2" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#relación-entre-la-regla-de-clasificación-de-fisher-y-las-funciones-de-discriminación-de-la-teoría-normal"><i class="fa fa-check"></i><b>9.8.2</b> Relación entre la Regla de Clasificación de Fisher y las Funciones de Discriminación de la Teoría Normal</a></li>
<li class="chapter" data-level="9.8.3" data-path="método-de-fisher-para-discriminar-entre-varias-poblaciones.html"><a href="método-de-fisher-para-discriminar-entre-varias-poblaciones.html#regla-de-clasificación-de-fisher-basado-en-discriminantes-muestrales"><i class="fa fa-check"></i><b>9.8.3</b> Regla de Clasificación de Fisher Basado en Discriminantes Muestrales</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html"><i class="fa fa-check"></i><b>9.9</b> Regresión Logística y Clasificación</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#el-modelo-logit"><i class="fa fa-check"></i><b>9.9.1</b> El Modelo Logit</a></li>
<li class="chapter" data-level="9.9.2" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#análisis-de-regresión-logística"><i class="fa fa-check"></i><b>9.9.2</b> Análisis de Regresión Logística</a></li>
<li class="chapter" data-level="9.9.3" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#regresión-logística-con-respuestas-binomiales"><i class="fa fa-check"></i><b>9.9.3</b> Regresión Logística con Respuestas Binomiales</a></li>
<li class="chapter" data-level="9.9.4" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#chequeo-del-modelo"><i class="fa fa-check"></i><b>9.9.4</b> Chequeo del Modelo</a></li>
<li class="chapter" data-level="9.9.5" data-path="regresión-logística-y-clasificación.html"><a href="regresión-logística-y-clasificación.html#prueba-de-residuales-y-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>9.9.5</b> Prueba de Residuales y de Bondad de Ajuste</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="acluster.html"><a href="acluster.html"><i class="fa fa-check"></i><b>10</b> Análisis de Agrupamiento o Cluster</a>
<ul>
<li class="chapter" data-level="10.1" data-path="introducción-7.html"><a href="introducción-7.html"><i class="fa fa-check"></i><b>10.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2" data-path="consideraciones-iniciales.html"><a href="consideraciones-iniciales.html"><i class="fa fa-check"></i><b>10.2</b> Consideraciones Iniciales</a></li>
<li class="chapter" data-level="10.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html"><i class="fa fa-check"></i><b>10.3</b> Medidas de Similaridad entre Pares de Observaciones</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#medidas-de-distancia"><i class="fa fa-check"></i><b>10.3.1</b> Medidas de Distancia</a></li>
<li class="chapter" data-level="10.3.2" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-de-correlación-entre-casos"><i class="fa fa-check"></i><b>10.3.2</b> Coeficientes de Correlación (entre casos)</a></li>
<li class="chapter" data-level="10.3.3" data-path="medidas-de-similaridad-entre-pares-de-observaciones.html"><a href="medidas-de-similaridad-entre-pares-de-observaciones.html#coeficientes-binarios-de-asociación-o-similaridad-entre-observaciones"><i class="fa fa-check"></i><b>10.3.3</b> Coeficientes Binarios de Asociación o Similaridad Entre Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><a href="medidas-de-similaridad-o-asociación-para-pares-de-variables.html"><i class="fa fa-check"></i><b>10.4</b> Medidas de Similaridad o Asociación para Pares de Variables</a></li>
<li class="chapter" data-level="10.5" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html"><i class="fa fa-check"></i><b>10.5</b> Métodos Jerárquicos de Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-enlaces-para-agupamientos-aglomerativos"><i class="fa fa-check"></i><b>10.5.1</b> Métodos Jerarquicos de Enlaces para Agupamientos Aglomerativos</a></li>
<li class="chapter" data-level="10.5.2" data-path="métodos-jerárquicos-de-agrupamiento.html"><a href="métodos-jerárquicos-de-agrupamiento.html#métodos-jerarquicos-de-agrupamientos-desaglomerativos"><i class="fa fa-check"></i><b>10.5.2</b> Métodos Jerarquicos de Agrupamientos Desaglomerativos</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><i class="fa fa-check"></i><b>10.6</b> Métodos NO-Jerárquicos de Partición o Agrupamiento</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#pasos-o-etapas-de-un-método-de-clasificación-no-jararquico"><i class="fa fa-check"></i><b>10.6.1</b> Pasos o etapas de un Método de Clasificación No-Jararquico</a></li>
<li class="chapter" data-level="10.6.2" data-path="métodos-no-jerárquicos-de-partición-o-agrupamiento.html"><a href="métodos-no-jerárquicos-de-partición-o-agrupamiento.html#método-de-las-k-medias-o-k-means"><i class="fa fa-check"></i><b>10.6.2</b> Método de las <span class="math inline">\(k\)</span>-Medias o <span class="math inline">\(k\)</span>-means</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="cómo-determinar-el-número-apropiado-de-conglomerados.html"><a href="cómo-determinar-el-número-apropiado-de-conglomerados.html"><i class="fa fa-check"></i><b>10.7</b> Cómo determinar el número apropiado de conglomerados?</a></li>
<li class="chapter" data-level="10.8" data-path="agrupamientos-basados-en-modelos-estadísticos.html"><a href="agrupamientos-basados-en-modelos-estadísticos.html"><i class="fa fa-check"></i><b>10.8</b> Agrupamientos Basados en Modelos Estadísticos</a></li>
<li class="chapter" data-level="10.9" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html"><i class="fa fa-check"></i><b>10.9</b> Escalamiento Multidimensional</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="escalamiento-multidimensional.html"><a href="escalamiento-multidimensional.html#el-algoritmo-básico"><i class="fa fa-check"></i><b>10.9.1</b> El Algoritmo Básico</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html"><i class="fa fa-check"></i><b>10.10</b> Análisis de Correspondencia</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#desarrollo-algebraíco-del-análsisi-de-correspondencia"><i class="fa fa-check"></i><b>10.10.1</b> Desarrollo Algebraíco del Análsisi de Correspondencia</a></li>
<li class="chapter" data-level="10.10.2" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-como-un-problema-de-mínimos-cuadrados-ponderado"><i class="fa fa-check"></i><b>10.10.2</b> Análisis de Correspondencia como un Problema de Mínimos Cuadrados Ponderado</a></li>
<li class="chapter" data-level="10.10.3" data-path="análisis-de-correspondencia.html"><a href="análisis-de-correspondencia.html#análisis-de-correspondencia-medinate-el-método-de-aproximación-de-perfiles"><i class="fa fa-check"></i><b>10.10.3</b> Análisis de Correspondencia Medinate el Método de Aproximación de Perfiles</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><i class="fa fa-check"></i><b>10.11</b> Biplots para la Visualización de Unidades Muestrales y Variables</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html"><a href="biplots-para-la-visualización-de-unidades-muestrales-y-variables.html#construcción-de-un-biplot"><i class="fa fa-check"></i><b>10.11.1</b> Construcción de un Biplot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Chapter 10</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rlm" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Modelo de Regresión Lineal Múltiple (RLM)<a href="rlm.html#rlm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sean <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span> las <span class="math inline">\(r\)</span>-variables predictoras (o regresoras) que se piensa están relacionadas con la variables respuesta <span class="math inline">\(Y\)</span>.</p>
<p>El modelo de regresión lineal clásico establece que la variables <span class="math inline">\(Y\)</span> se compone de una media, la cual depende de manera continua de las variables <span class="math inline">\(Z´s\)</span> y de un error aleatorio <span class="math inline">\(\varepsilon\)</span>, que da cuenta del error de medición y los efectos de otras variables no consideradas explícitamente en el modelo. Los valores de las variables predictoras registradas a partir de un experimento o establecidas por parte del investigador se tratan como valores fijos no aleatorios. El error (y por lo tanto la respuesta) se ven como una variable aleatoria cuyo comportamiento se caracteriza por un conjunto de suposiciones distribucionales.</p>
<p>Específicamente el Modelo de Regresión Lineal Múltiple toma la forma dada por:
<span class="math display" id="eq:mrlm1">\[
\begin{equation}
\underset{\text{Respuesta}}{Y} =\underset{\text{Media que depende de las}\  Z´s}{ \underbrace{ \beta_0 + \beta_1\ Z_1 + \beta_2\ Z_2 + \cdots + \beta_r}\ Z_r } +  \underset{Error}{\varepsilon }
\end{equation}
\tag{5.1}
\]</span></p>
<p>El término <em>Lineal</em> se refiere al hecho de que la media es una función lineal de los parámetros desconocidos <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_r\)</span>. Las variables predictoras pueden estar o no estar en el modelo como términos de primer orden.</p>
<p>Con <span class="math inline">\(n\)</span>-observaciones independientes de la variables respuesta <span class="math inline">\(Y\)</span> y los respectivos valores asociados de las variables predictoras <span class="math inline">\(Z_i´s\)</span>, el modelo de regresión lineal <a href="rlm.html#eq:mrlm1">(5.1)</a> se convierte en:
<span class="math display" id="eq:mrlm-muestral">\[
\begin{equation}
y_1 = \beta_0 + \beta_1\ z_{11} + \beta_2\ z_{12} + \cdots + \beta_r\ z_{1r} + \varepsilon_1 \\
y_2 = \beta_0 + \beta_1\ z_{21} + \beta_2\ z_{22} + \cdots + \beta_r\ z_{2r} + \varepsilon_2 \\
\vdots \\
y_n = \beta_0 + \beta_1\ z_{n1} + \beta_2\ z_{n2} + \cdots + \beta_r\ z_{nr} + \varepsilon_n \\
\end{equation}
\tag{5.2}
\]</span></p>
<p>con las siguientes suposiciones sobre los términos de error <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p><span class="math display" id="eq:supuestos-mrlm">\[
\begin{equation}
E[\varepsilon_i]=0 \\
Var[\varepsilon_i]= \sigma^2 \ \ (\text{ie. constante}) \\
Cov[\varepsilon_i \ ,\ \varepsilon_j]=0 \ \ , \ \ i\neq j
\end{equation}
\tag{5.3}
\]</span></p>
<p>El modelo <a href="rlm.html#eq:mrlm-muestral">(5.2)</a> en <strong>Forma Matricial</strong> se expresa como:
<span class="math display">\[
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix}=\begin{bmatrix} 1 &amp; z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1r} \\
1 &amp; z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2r} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{nr} \end{bmatrix}\ \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_r \end{bmatrix} + \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}
\]</span></p>
<p>o equivalentemente,
<span class="math display" id="eq:mrlm-matricial">\[
\begin{equation}
\underset{n\times 1}{\underline{\mathbf{y}}}= \underset{n \times (r+1)}{ \underset{}{\mathbf{Z}} } \ \  \underset{(n+r)\times 1}{\underline{\boldsymbol \beta}} + \ \underset{n\times 1}{\underline{\boldsymbol \varepsilon}}
\end{equation}
\tag{5.4}
\]</span></p>
<p>y los supuestos de <a href="rlm.html#eq:supuestos-mrlm">(5.3)</a> se convierten en:
<span class="math display" id="eq:supuestos-mrlm-matricial">\[
\begin{equation}
E[\ \underline{\boldsymbol \varepsilon}\ ]= \underline{\mathbf{0}} \\
Var[\ \underline{\boldsymbol \varepsilon} \ ] = \sigma^2\ \mathbf{I}_n
\end{equation}
\tag{5.5}
\]</span></p>
<p>A la matriz <span class="math inline">\(\mathbf{Z}\)</span>-se le llama <strong>Matriz Diseño</strong>, al vector <span class="math inline">\(\underline{\boldsymbol \beta}\)</span>-vector de parámetros del modelo y al vector <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>-vector de errores del modelo. Cada columna de <span class="math inline">\(\mathbf{Z}\)</span> contiene los <span class="math inline">\(n\)</span>-valores de las variables regresoras <span class="math inline">\(Z´s\)</span>, mientras que cada fila de <span class="math inline">\(\mathbf{Z}\)</span> contiene los valores de las <span class="math inline">\(r\)</span>-variables regresoras sobre el individuo <span class="math inline">\(i\)</span>-ésimo <span class="math inline">\(i=1,2,\ldots,n\)</span>.</p>
<div id="estimación-de-mínimos-cuadrados" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Estimación de Mínimos Cuadrados<a href="rlm.html#estimación-de-mínimos-cuadrados" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Uno de los objetivos del análisis de regresión es desarrollar una ecuación que permita predecir la respuesta para valores dados de las variables predictoras. Por lo tanto, es necesario “ajustar” el modelo dado en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a> a los valores observados <span class="math inline">\(y_j\)</span> correspondientes a los valores conocidos <span class="math inline">\(1,z_{i1},z_{i2},\ldots,z_{ir}\)</span>. Es decir, se deben determinar los valores para los <em>Coeficientes de Regresión</em> <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> y la varianza de error <span class="math inline">\(\sigma^2\)</span> consistente con los datos disponibles.</p>
<p>El método de Mínimos Cuadrados consiste en hallar el vector <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> que minimice la suma de cuadrados de errores dada por:
<span class="math display" id="eq:suma-cuadratica-de-errores">\[
\begin{equation}
S(\underline{\boldsymbol \beta})= \sum_{i=1}^n\ (y_i-\beta_0 - \beta_1\ z_{i1} - \beta_2\ z_{i2} - \cdots - \beta_r\ z_{ir})^2 \\
= (\underline{\mathbf{y}}-\mathbf{Z}\ \underline{\boldsymbol \beta})^{\ t }(\underline{\mathbf{y}}-\mathbf{Z}\ \underline{\boldsymbol \beta})
\end{equation}
\tag{5.6}
\]</span></p>
<p>La solución <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> a la ecuación anterior se dice que es consistente con los datos en el sentido de que ella produce la respuesta media estimada (o ajustada) <span class="math inline">\(\widehat{y_i}=\hat{\beta}_0+\hat{\beta}_1 \ z_{i1}+ \hat{\beta}_2\ z_{i2}+ \cdots + \hat{\beta}_r\ z_{ir}\)</span> cuya suma de cuadrados de las diferencias desde el valor observado <span class="math inline">\(y_i\)</span> al valor estimado <span class="math inline">\(\hat{y}_i\)</span> sea tan pequeña como sea posible. A las deviaciones dadas por
<span class="math display">\[
\hat{\varepsilon}_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0+\hat{\beta}_1 \ z_{i1}+ \hat{\beta}_2\ z_{i2}+ \cdots + \hat{\beta}_r\ z_{ir}) \ \ \ , \ \ \ i=1,2,\ldots,n
\]</span></p>
<p>se les llaman <em>Residuales</em>. El vector de residuales
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}= \underline{\mathbf{y}} - \widehat{\underline{\mathbf{y}}}=\underline{\mathbf{y}} - \mathbf{Z} \ \widehat{\underline{\boldsymbol \beta}}
\]</span></p>
<p>contiene información acerca del parámetro restante desconocido <span class="math inline">\(\sigma^2\)</span>, como se verá más adelante.</p>
<div class="theorem">
<p><span id="thm:teorema-estimadores-ms" class="theorem"><strong>Teorema 5.1  (Teorema Estimadores de Mínimos Cuadrados en el MRLM) </strong></span>Sea <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\((r+1) \leq n\)</span>. El Estimador de Mínimos Cuadrados de <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> obtenido de <a href="rlm.html#eq:suma-cuadratica-de-errores">(5.6)</a> está dado por:</p>
</div>
<p><span class="math display" id="eq:estimador-ms">\[
\begin{equation}
\widehat{\underline{\boldsymbol \beta}}= (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \mathbf{Z}^{\ t}\ \underline{\mathbf{y}}
\end{equation}
\tag{5.7}
\]</span></p>
<p>Además, si
<span class="math display">\[
\widehat{\underline{\mathbf{y}}}= \mathbf{Z} \ \widehat{\underline{\boldsymbol \beta}}= \mathbf{Z}\biggl((\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \mathbf{Z}^{\ t}\ \underline{\mathbf{y}}\biggl)= \mathbf{H}\ \underline{\mathbf{y}} \ \ \ \ , \ \  \text{con:} \ \ \ \underset{n\times n}{\mathbf{H} }=\mathbf{Z}\ (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \mathbf{Z}^{\ t}-\text{Matriz Sombrero}
\]</span></p>
<p>El vector de residuales
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}= \underline{\mathbf{y}} - \widehat{\underline{\mathbf{y}}}=\underline{\mathbf{y}} - \mathbf{H}\ \underline{\mathbf{y}} = (\mathbf{I}-\mathbf{H})\ \underline{\mathbf{y}}
\]</span></p>
<p>cumple que:
<span class="math display">\[
\mathbf{Z}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}= \underline{\mathbf{0}} \ \ \ \ \ y \ \ \ \ \ \ \  \underline{\mathbf{y}}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}= 0
\]</span></p>
<p>Además, la suma cuadrática de residuales
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}= \sum_{i=1}^n\ (y_i - \hat{y}_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 \ z_{i1} - \hat{\beta}_2\ z_{i2} - \cdots - \hat{\beta}_r\ z_{ir})^2 \\
= \underline{\mathbf{y}}^{\ t} \bigl[\ \mathbf{I} - \mathbf{Z}(\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \mathbf{Z}^{\ t}\ \bigr]\  \underline{\mathbf{y}} \\
\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}= \underline{\mathbf{y}}^{\ t}\ \underline{\mathbf{y}} - \underline{\mathbf{y}}^{\ t}\ \mathbf{Z}\ \widehat{ \underline{\boldsymbol \beta} }
\]</span></p>
<p>Por otro lado, también se tiene que:
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}= \biggl( (\mathbf{I}-\mathbf{H})\ \underline{\mathbf{y}}   \biggr)^{t} \biggl( (\mathbf{I}-\mathbf{H})\ \underline{\mathbf{y}}   \biggr) \\
= \underline{\mathbf{y}}^t\ (\mathbf{I}-\mathbf{H})^t      (\mathbf{I}-\mathbf{H})\ \underline{\mathbf{y}}    \\
= \underline{\mathbf{y}}^t\ (\mathbf{I}-\mathbf{H})^2\ \underline{\mathbf{y}}    \\
\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}} = \underline{\mathbf{y}}^{\ t}\ \bigl[\mathbf{I}-\mathbf{H}\bigr] \ \underline{\mathbf{y}}
\]</span></p>
<div class="example">
<p><span id="exm:ejemplo1-mrlm" class="example"><strong>Ejemplo 5.1  (Estimador de Mínimos Cuadrados de un MRLM) </strong></span>Calcular el Estimador de Mínimos Cuadrados <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span>, el vecor de residuales <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}\)</span> y la suma de cuadrados de residuales <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}^t\widehat{\underline{\boldsymbol \varepsilon}}\)</span> para el MRLM:</p>
</div>
<p><span class="math display">\[
Y_j= \beta_0  + \beta_1\ z_{j1}+\varepsilon_j
\]</span></p>
<p>ajustado a los datos:
<span class="math display">\[
\begin{array}{c|ccccc}
z_1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\hline
Y &amp; 1 &amp; 4 &amp; 4 &amp; 8 &amp; 9
\end{array}
\]</span></p>
<p>En este caso se tienen las siguientes matrices involucradas en el MRLM:
<span class="math display">\[
\mathbf{Z}= \begin{bmatrix} 1 &amp;0\\ 1 &amp;1 \\ 1&amp;2 \\ 1&amp;3 \\ 1&amp;4  \end{bmatrix} \ \ \ , \ \ \ \ \mathbf{Z}^t= \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4  \end{bmatrix} \ \ \ , \ \ \ \ \underline{\mathbf{y}}= \begin{bmatrix} 1 \\ 4  \\ 3 \\ 8 \\ 9  \end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{Z}^t\mathbf{Z}= \begin{bmatrix} 5 &amp; 10 \\ 10 &amp; 30 \end{bmatrix} \ \ \ , \ \ \ \ (\mathbf{Z}^t\mathbf{Z})^{-1}= \begin{bmatrix} 0.6 &amp; -0.2 \\ -0. &amp; 0.1 \end{bmatrix} \ \ \ , \ \ \ \ \mathbf{Z}^t\underline{\mathbf{y}}= \begin{bmatrix} 25 \\ 70   \end{bmatrix}
\]</span></p>
<p>Luego, el Estimador de Mínimos Cuadrados esta dado por:
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}}=\mathbf{Z}^t\underline{\mathbf{y}}= \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1   \end{bmatrix}=(\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\underline{\mathbf{y}}=\begin{bmatrix} 0.6 &amp; -0.2 \\ -0. &amp; 0.1 \end{bmatrix}\begin{bmatrix} 25 \\ 70   \end{bmatrix}=\begin{bmatrix} 1 \\ 2   \end{bmatrix}
\]</span></p>
<p>de donde el MRLM-Ajustado es:
<span class="math display">\[
\hat{y}=\hat{\beta}_0 + \hat{\beta}_1\ z
\]</span></p>
<p>es decir:
<span class="math display">\[
\hat{y}=1 + 2\ z
\]</span></p>
<p>Ahora, el vector de valores predichos es:
<span class="math display">\[
\underline{\hat{\mathbf{y}}}= \mathbf{Z}\ \widehat{\underline{\boldsymbol \beta}}=\begin{bmatrix} 1 &amp;0\\ 1 &amp;1 \\ 1&amp;2 \\ 1&amp;3 \\ 1&amp;4  \end{bmatrix}\begin{bmatrix} 1 \\ 2   \end{bmatrix}=\begin{bmatrix} 1 \\ 3 \\ 5 \\ 7 \\ 9   \end{bmatrix}
\]</span></p>
<p>de donde, el vector de residuales es:
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}=\underline{\mathbf{y}} - \underline{\hat{\mathbf{y}}}= \begin{bmatrix} 1 \\ 4  \\ 3 \\ 8 \\ 9  \end{bmatrix}-\begin{bmatrix} 1 \\ 3 \\ 5 \\ 7 \\ 9   \end{bmatrix}=\begin{bmatrix} 0 \\ 1 \\ -2 \\ 1 \\ 0   \end{bmatrix}
\]</span></p>
<p>y la suma cuadrática de residuales es:
<span class="math display">\[
\widehat{\underline{\boldsymbol \varepsilon}}^t\widehat{\underline{\boldsymbol \varepsilon}}=\begin{bmatrix} 0 &amp; 1 &amp; -2 &amp; 1 &amp; 0   \end{bmatrix}\begin{bmatrix} 0 \\ 1 \\ -2 \\ 1 \\ 0   \end{bmatrix}= 6
\]</span></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model             40  1          40      20 0.0208352
## Error              6  3           2</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ z)
## 
## Residuals:
##  1  2  3  4  5 
##  0  1 -2  1  0 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)    1.000      1.095    0.91    0.429  
## z              2.000      0.447    4.47    0.021 *
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.41 on 3 degrees of freedom
## Multiple R-squared:  0.87,   Adjusted R-squared:  0.826 
## F-statistic:   20 on 1 and 3 DF,  p-value: 0.0208</code></pre>
<div id="descomposición-de-la-suma-total-de-cuadrados" class="section level4 hasAnchor" number="5.2.1.1">
<h4><span class="header-section-number">5.2.1.1</span> Descomposición de la Suma Total de Cuadrados<a href="rlm.html#descomposición-de-la-suma-total-de-cuadrados" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A partir de la suma de cuadrados total dada por <span class="math inline">\(\underline{\mathbf{y}}^t \underline{\mathbf{y}}=\sum_{i=1}^n\ y_i^2\)</span> y usando el hecho de que <span class="math inline">\(\underline{\mathbf{y}}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}= 0\)</span> se tiene que:
<span class="math display">\[
\underline{\mathbf{y}}^t  \underline{\mathbf{y}} =  ( \hat{\underline{\mathbf{y}}} + \underline{\mathbf{y}} - \hat{\underline{\mathbf{y}}}  )^{\ t}\ ( \hat{\underline{\mathbf{y}}} + \underline{\mathbf{y}} - \hat{\underline{\mathbf{y}}} ) \\
= ( \hat{\underline{\mathbf{y}}} + \hat{\underline{\boldsymbol \varepsilon }} )^{\ t}\ ( \hat{\underline{\mathbf{y}}} + \hat{\underline{\boldsymbol \varepsilon }} ) \\
\underline{\mathbf{y}}^t  \underline{\mathbf{y}}  = \hat{\underline{\mathbf{y}}}^{\ t} \ \hat{\underline{\mathbf{y}}} + \hat{\underline{\boldsymbol \varepsilon }}^{\ t}\ \hat{\underline{\boldsymbol \varepsilon }}
\]</span>
<span class="math display" id="eq:suma-cuadratica1">\[
\begin{equation}
\underline{\mathbf{y}}^t  \underline{\mathbf{y}}  = \hat{\underline{\mathbf{y}}}^{\ t} \ \hat{\underline{\mathbf{y}}} + \hat{\underline{\boldsymbol \varepsilon }}^{\ t}\ \hat{\underline{\boldsymbol \varepsilon }}
\end{equation}
\tag{5.8}
\]</span></p>
<p>Ahora de la condición de que <span class="math inline">\(\mathbf{Z}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}=\underline{\mathbf{0}}\)</span>, se tiene que:
<span class="math display">\[
0 = \underline{\mathbf{1}}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}} = \sum_{i=1}^n \ \hat{\varepsilon}_i = \sum_{i=1}^n (y_i- \hat{y}_i) = \sum_{i=1}^n y_i - \sum_{i=1}^n\ \hat{y}_i \ \ ,\ \ \text{es decir:} \ \ \overline{y}= \overline{\hat{y}}
\]</span></p>
<p>de donde, restando a ambos lados de <a href="rlm.html#eq:suma-cuadratica1">(5.8)</a> la cantidad <span class="math inline">\(n\ \overline{y} =n\ \overline{\hat{y}}\)</span>, se tiene que:
<span class="math display">\[
( \underline{\mathbf{y}}^t  \underline{\mathbf{y}} - n\ \overline{y} )= (\hat{\underline{\mathbf{y}}}^{\ t} \ \hat{\underline{\mathbf{y}}} - n\ \overline{\hat{y}} )  + \hat{\underline{\boldsymbol \varepsilon }}^{\ t}\ \hat{\underline{\boldsymbol \varepsilon }}
\]</span></p>
<p>o equivalentemente
<span class="math display" id="eq:suma-cuadratica2">\[
\begin{equation}
\sum_{i=1}^n\ ( y_i - \overline{y} )^2 = \sum_{i=1}^n\ ( \hat{y}_i -  \overline{y} )  + \sum_{i=1}^n \ \varepsilon_i^2
\end{equation}
\tag{5.9}
\]</span></p>
<p>obteniendo:
<span class="math display">\[
SST = SSR + SSE
\]</span></p>
<p>La descomposición anterior de la Suma de Cuadrados Total, sugiere que la calidad de los Modelos Ajustados puede ser medida porel <em>Coeficiente de Determinación Múltiple</em> dado por:
<span class="math display" id="eq:R2a">\[
\begin{equation}
R^2 = 1- \frac{\sum_{i=1}^n \ \varepsilon_i^2}{\sum_{i=1}^n\ ( y_i - \overline{y} )^2} = 1 - \frac{SSE}{SST}
\end{equation}
\tag{5.10}
\]</span></p>
<p>o equivalentemente,
<span class="math display" id="eq:R2a">\[
\begin{equation}
R^2= \frac{ \sum_{i=1}^n\ ( \hat{y}_i -  \overline{y} )   }{\sum_{i=1}^n\ ( y_i - \overline{y} )^2} = \frac{SSR}{SST}
\end{equation}
\tag{5.10}
\]</span></p>
<p>La cantidad <span class="math inline">\(R^2\)</span>-nos da la proporción de variación total de las respuestas <span class="math inline">\(y_i´s\)</span>, explicada por (o atribuibles a) las variables predictoras <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span>. Aquí, <span class="math inline">\(R^2\)</span> (o el Coeficiente de Correlación Múltiple <span class="math inline">\(R=\sqrt{R^2}\)</span>) es igual a 1, si la ecuación ajustada pasa a través de todos los puntos o datos observados, de modo que <span class="math inline">\(\hat{\varepsilon}_i=0 \ \ \ \forall i\)</span>. En el otro extremo <span class="math inline">\(R^2=0\)</span> si <span class="math inline">\(\hat{\beta}_0 = \overline{y}\)</span> y <span class="math inline">\(\hat{\beta}_1=\hat{\beta}_2=\cdots=\hat{\beta}_r=0\)</span>. En este caso las variables predictoras <span class="math inline">\(Z_1,Z_2,\ldots,Z_r\)</span> no tienen influencia sobre la variable respuesta <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="geometría-de-los-mínimos-cuadrados" class="section level4 hasAnchor" number="5.2.1.2">
<h4><span class="header-section-number">5.2.1.2</span> Geometría de los Mínimos Cuadrados<a href="rlm.html#geometría-de-los-mínimos-cuadrados" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una interpretación geométrica de la técnica de mínimos cuadrados ilustra la naturaleza del concepto. De acuerdo al Modelo de Regresión Lineal Clásico,
<span class="math display">\[
E[\underline{\mathbf{y}}]=\mathbf{Z} \ \underline{\boldsymbol \beta} = \beta_0 \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1  \end{bmatrix} + \beta_1 \begin{bmatrix} z_{11} \\ z_{21} \\ \vdots \\ z_{n1}  \end{bmatrix} +  \beta_2 \begin{bmatrix} z_{12} \\ z_{22} \\ \vdots \\ z_{n2}  \end{bmatrix} + \cdots + \beta_r \begin{bmatrix} z_{1r} \\ z_{2r} \\ \vdots \\ z_{nr}  \end{bmatrix}
\]</span></p>
<p>es decir que, el vector de respuesta media <span class="math inline">\(E[\ \underline{\mathbf{y}}\ ]\)</span> es una combinación lineal de las columnas de <span class="math inline">\(\mathbf{Z}\)</span>. A medida que <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> varía, <span class="math inline">\(\mathbf{Z}\ \underline{\boldsymbol \beta}\)</span> genera el plano-modelo de todas las combinaciones lineales. Usualmente, el vector de observaciones <span class="math inline">\(\underline{\mathbf{y}}\)</span> no caé en el plano-modelo, debido al error aleatorio <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>, es decir que, <span class="math inline">\(\underline{\mathbf{y}}\)</span>-no es (exactamente) una combinación lineal de las columnas de <span class="math inline">\(\mathbf{Z}\)</span>.</p>
<p>Recordemos que:
<span class="math display">\[
\underset{Vector-Respuesta}{\underline{\mathbf{y}} } = \underset{Vector-en-Plano-Modelos}{  \mathbf{Z} \ \underline{\boldsymbol \beta} } + \underset{Vector-de-Error}{ \underline{\boldsymbol \varepsilon} }
\]</span>
Una vez que las observaciones están disponibles, se deriva la solución de mínimos cuadrados del vector de desviación:
<span class="math display">\[
\underline{\mathbf{y}}-\mathbf{Z}\ \underline{\boldsymbol b} = (\text{Vector de Observación}) -(\text{Vector en el Plano-Modelo})
\]</span></p>
<p>la longitud al cuadrado <span class="math inline">\((\underline{\mathbf{y}}-\mathbf{Z}\ \underline{\boldsymbol b})^{\ t}\  (\underline{\mathbf{y}}-\mathbf{Z}\ \underline{\boldsymbol b})\)</span> es la suma de cuadrados <span class="math inline">\(S(\ \underline{\boldsymbol b}\ )\)</span>. Como se ilustra en la figura, <span class="math inline">\(S(\ \underline{\boldsymbol b}\ )\)</span> es lo más pequeño posible, cuando se selecciona <span class="math inline">\(\underline{\boldsymbol b}\)</span> tal que <span class="math inline">\(\mathbf{Z}\ \underline{\boldsymbol b}\)</span> es el punto en el Plano-Modelo lo mas cercano a <span class="math inline">\(\underline{\mathbf{y}}\)</span>. Este punto ocurre en la punta de la proyección perpendicular de <span class="math inline">\(\underline{\mathbf{y}}\)</span> sobre el plano. Es decir, para la elección de <span class="math inline">\(\underline{\boldsymbol b}=\widehat{\underline{\boldsymbol \beta}}\)</span>, <span class="math inline">\(\widehat{\underline{\mathbf{y}}}=\mathbf{Z}\ \widehat{\underline{\boldsymbol \beta}}\)</span> es la proyección de <span class="math inline">\(\underline{\mathbf{y}}\)</span> sobre el plano que consiste de todas las combinaciones lineales de las columnas de <span class="math inline">\(\mathbf{Z}\)</span>. El vector de residuales <span class="math inline">\(\widehat{ \underline{\boldsymbol \varepsilon}}=\underline{\mathbf{y}} - \widehat{\underline{\mathbf{y}}}\)</span> es perpendicular a ese plano.</p>
</div>
<div id="propiedades-muestrales-de-los-estimadores-mínimo-cuadráticos" class="section level4 hasAnchor" number="5.2.1.3">
<h4><span class="header-section-number">5.2.1.3</span> Propiedades Muestrales de los Estimadores Mínimo Cuadráticos<a href="rlm.html#propiedades-muestrales-de-los-estimadores-mínimo-cuadráticos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El vector de estimadores de mínimos cuadrados <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> y el vector de residuales <span class="math inline">\(\widehat{ \underline{\boldsymbol \varepsilon}}\)</span> tienen las propiedades muestrales que se enuncian en le siguiente resultado.</p>
<div class="theorem">
<p><span id="thm:teorema-propiedades-minimos-cuadrados" class="theorem"><strong>Teorema 5.2  (Propiedades del Estimador de Mínimos Cuadrados) </strong></span>Bajo el Modelo de Regresión Lineal General dado en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a> y <a href="rlm.html#eq:supuestos-mrlm-matricial">(5.5)</a>, el estimador de Mínimos Cuadrados <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}=(\ \mathbf{Z}^{\ t}\ \mathbf{Z}\ )^{-1}\ \mathbf{Z}^t\ \underline{\mathbf{y}}\)</span> cumple que:</p>
</div>
<p><span class="math display" id="eq:propiedade-minimos-cuadrados">\[
\begin{equation}
E[\ \widehat{\underline{\boldsymbol \beta}}\ ] =  \underline{\boldsymbol \beta} \ \ \ \ \ \ , \ \ \ \ \  Var[\ \widehat{\underline{\boldsymbol \beta}}\ ]=\sigma^2\ (\ \mathbf{Z}^{\ t}\ \mathbf{Z}\ )^{-1}
\end{equation}
\tag{5.11}
\]</span></p>
<p>además, el vector de residuales <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}\)</span> cumple que:
<span class="math display" id="eq:propiedade-minimos-cuadrados2">\[
\begin{equation}
E[\ \widehat{\underline{\boldsymbol \varepsilon}}\ ] =  \underline{\boldsymbol 0} \ \ \ \ \ \ , \ \ \ \ \  Var[\ \widehat{\underline{\boldsymbol \varepsilon}}\ ]=\sigma^2\ [\ \mathbf{I}- \mathbf{H}\ ]
\end{equation}
\tag{5.12}
\]</span></p>
<p>también se cumple que: <span class="math inline">\(E[\ \widehat{\underline{\boldsymbol \varepsilon}}^{\  t}\ \widehat{\underline{\boldsymbol \varepsilon}}\ ]=(n-(r+1))\sigma^2\)</span>.</p>
<p>Definiendo a
<span class="math display">\[
S^2= \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\  t}\ \widehat{\underline{\boldsymbol \varepsilon}}}{n-(r+1)}= \frac{\underline{\mathbf{y}}\ ^{\ t}[\ \mathbf{I}- \mathbf{H}\ ]\ \underline{\mathbf{y}}}{n-r-1}
\]</span></p>
<p>se tiene que:
<span class="math display">\[
E[\ S^2\ ]= \sigma^2.
\]</span></p>
<p>Además, <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> y <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}\)</span>-son No-Correlacionados o independientes.</p>
<div class="theorem">
<p><span id="thm:teorema-propiedades-minimos-cuadrados2" class="theorem"><strong>Teorema 5.3  (Propiedades de MC-Teorema de Gauss) </strong></span>Sea el Modelo de Regresión Lineal General dado en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a> y <a href="rlm.html#eq:supuestos-mrlm-matricial">(5.5)</a> y <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\(r+1\)</span>. Para cualquier vector de constantes <span class="math inline">\(\underline{\mathbf{c}}\)</span>, el estimador</p>
</div>
<p><span class="math display">\[
\underline{\mathbf{c}}^{\ t}\ \widehat{\underline{\boldsymbol \beta}}= c_0\ \hat{\beta}_0 + c_1\ \hat{\beta}_1 + \cdots + c_r\ \hat{\beta}_r
\]</span></p>
<p>de <span class="math inline">\(\underline{\mathbf{c}}^{\ t}\ \underline{\boldsymbol \beta}\)</span>-tiene la varianza más pequeña posible entre todos los estimadores lineales insesgados de <span class="math inline">\(\underline{\mathbf{c}}^{\ t}\ \underline{\boldsymbol \beta}\)</span> de la forma
<span class="math display">\[
\underline{\mathbf{a}}^{\ t}\ \underline{\mathbf{y}}= a_1\ y_1 + a_2\ y_2 + \cdots + a_n\ y_n.
\]</span></p>
</div>
</div>
<div id="inferencias-acerca-del-modelo-de-regresión-lineal-múltiple" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Inferencias Acerca del Modelo de Regresión Lineal Múltiple<a href="rlm.html#inferencias-acerca-del-modelo-de-regresión-lineal-múltiple" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ahora se describen procedimientos de inferencia basados en el modelo clásico de regresión lineal dado en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a> y <a href="rlm.html#eq:supuestos-mrlm-matricial">(5.5)</a> con el supuesto adicional (tentativo) de que los errores <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span> tienen una distribución normal. También se consideran métodos para comprobar la adecuación de dicho modelo</p>
<div id="inferencias-concernientes-a-los-parámetros-de-regresión" class="section level4 hasAnchor" number="5.2.2.1">
<h4><span class="header-section-number">5.2.2.1</span> Inferencias Concernientes a los Parámetros de Regresión<a href="rlm.html#inferencias-concernientes-a-los-parámetros-de-regresión" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sea el Modelo de Regresión Linela Multiple dado por
<span class="math display">\[
\underline{\mathbf{y}}= \mathbf{Z}\ \underline{\boldsymbol \beta} + \underline{\boldsymbol \varepsilon}
\]</span></p>
<p>con los supuestos dados por:
<span class="math display" id="eq:supuestos-mrlm-normal">\[
\begin{equation}
\underline{\boldsymbol \varepsilon} \sim N_n \bigl(\ \underline{\mathbf{0}}\ , \ \sigma^2\mathbf{I}_n \bigr),  \ \ ie. \ \  \varepsilon_i \underset{i.i.d}{\sim} N(\ 0\ ,\ \sigma^2\ )
\end{equation}
\tag{5.13}
\]</span></p>
<p>Antes de evaluar la importancia de algunas variables regresoras particulares en la función de regresión dada por
<span class="math display">\[
E[Y]=\beta_0 + \beta_1\ Z_1 + \beta_2\ Z_2 + \cdots + \beta_r\ Z_r
\]</span></p>
<p>se debe determinar la distribución muestral de <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> y de la suma de cuadrados de residuales <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}}\)</span>. Se utilizarán los supuestos dados en <a href="rlm.html#eq:supuestos-mrlm-normal">(5.13)</a>.</p>
<div class="theorem">
<p><span id="thm:teorema-distribucipn-de-beta-gorro" class="theorem"><strong>Teorema 5.4  (Distribución Muestral del Beta-Gorro) </strong></span>Para el modelo dado por <span class="math inline">\(\underline{\mathbf{y}}= \mathbf{Z}\ \underline{\boldsymbol \beta} + \underline{\boldsymbol \varepsilon}\)</span> con <span class="math inline">\(\mathbf{Z}\)</span>-de rango completo <span class="math inline">\(r+1\)</span> y <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span> con los supuestos dados en <a href="rlm.html#eq:supuestos-mrlm-normal">(5.13)</a>, ie. <span class="math inline">\(\underline{\boldsymbol \varepsilon} \sim N_n \bigl(\ \underline{\mathbf{0}}\ , \ \sigma^2\mathbf{I}_n \bigr)\)</span>.</p>
</div>
<p>El Estimador de Máxima Verosimilitud de <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> es el mismo estimador <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> de mínimos cuadrados. Además,
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}} = (\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\underline{\mathbf{y}} \sim N_{r+1} \biggl(\ \underline{\boldsymbol \beta}\ , \ \sigma^2\ (\mathbf{Z}^t\mathbf{Z})^{-1}  \biggr)
\]</span></p>
<p>y es distribuído independientemente de los residuales <span class="math inline">\(\widehat{\underline{\boldsymbol \varepsilon}}=\underline{\mathbf{y}}-\mathbf{Z}\ \widehat{\underline{\boldsymbol \beta}}\)</span>, es decir, <span class="math inline">\(Cov[\ \widehat{\underline{\boldsymbol \beta}} \ , \ \widehat{\underline{\boldsymbol \varepsilon}} \ ]=\underline{\mathbf{0}}\)</span>.</p>
<p>Además,
<span class="math display">\[
n\ \widehat{\sigma}^2 = \widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon}} \sim \sigma^2\ \chi_{n-(r+1)}^2
\]</span></p>
<p>donde <span class="math inline">\(\widehat{\sigma}^2\)</span>-es el MLE de <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="theorem">
<p><span id="thm:teorema-region-de-cpnfianza-para-beta-gorro" class="theorem"><strong>Teorema 5.5  (Región de Confianza para Beta) </strong></span>Sea el modelo dado por <span class="math inline">\(\underline{\mathbf{y}}= \mathbf{Z}\ \underline{\boldsymbol \beta} + \underline{\boldsymbol \varepsilon}\)</span> con <span class="math inline">\(\mathbf{Z}\)</span>-de rango completo <span class="math inline">\(r+1\)</span> y <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span> con los supuestos dados en <a href="rlm.html#eq:supuestos-mrlm-normal">(5.13)</a>, ie. <span class="math inline">\(\underline{\boldsymbol \varepsilon} \sim N_n \bigl(\ \underline{\mathbf{0}}\ , \ \sigma^2\mathbf{I}_n \bigr)\)</span> entonces, una Región de Confianza del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> está dada por:</p>
</div>
<p><span class="math display" id="eq:region-de-confianza-beta">\[
\begin{equation}
(\ \underline{\boldsymbol \beta} - \widehat{\underline{\boldsymbol \beta}} \ ) \ \mathbf{Z}^t \ \mathbf{Z}\ (\ \underline{\boldsymbol \beta} - \widehat{\underline{\boldsymbol \beta}} \ ) \leq (r+1)\ S^2 F_{1-\alpha \ ; \ r+1 \ , \ n-(r+1)}
\end{equation}
\tag{5.14}
\]</span></p>
<p>donde <span class="math inline">\(F_{1-\alpha \ ; \ r+1 \ , \ n-(r+1)}\)</span>-es el percentil <span class="math inline">\((1-\alpha)100\%\)</span> superior de la distribución <span class="math inline">\(F\)</span>-con <span class="math inline">\(r+1\)</span> y <span class="math inline">\(n-r-1\)</span>-grados de libertad.</p>
<p>También IC-Simultáneos del <span class="math inline">\((1-\alpha)100\%\)</span> para los <span class="math inline">\(\beta_i\)</span> están dados por:
<span class="math display" id="eq:ic-simultaneos-para-betas">\[
\begin{equation}
\hat{\beta}_i \ \pm \ \sqrt{\widehat{Var}( \ \hat{\beta_i}  \ )}\ \sqrt{(r+1)\ F_{1-\alpha\ ; \ r+1\ , \ n-(r+1)}} \ \ , \ \ \ i=0,1,2,\ldots,r
\end{equation}
\tag{5.15}
\]</span></p>
<p>donde <span class="math inline">\(\widehat{Var}( \ \hat{\beta_i} \ )\)</span>-es el <span class="math inline">\(i\)</span>-ésimo elemento de la diagonal de la matriz <span class="math inline">\(S^2 (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1}\)</span> que corresponde a <span class="math inline">\(\hat{\beta}_i\)</span>.</p>
<p><strong>Nota:</strong> La elipse de confianza está centrado en la estimación de máxima verosimilitud <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span>, y su orientación y forma están determinadas por los valores y vectores propios de <span class="math inline">\(\mathbf{Z}^t\mathbf{Z}\)</span>. Si un valor propio es cercano a cero, la elipse de confianza será muy largo en la dirección del vector propio correspondiente.</p>
<p>Frecuentemente los practicantes ignoran la propiedad de confianza “simultánea” de los intervalos estimados en <a href="rlm.html#eq:ic-simultaneos-para-betas">(5.15)</a>. En su lugar, reemplazan a <span class="math inline">\((r+1)\ F_{1-\alpha\ ; \ r+1\ , \ n-(r+1)}\)</span> con el <span class="math inline">\(t\)</span>-valor de intervalos uno
a la vez, <span class="math inline">\(t_{1-\alpha/2\ ;\ n-(r+1)}\)</span> y usan los intervalos dados por:
<span class="math display" id="eq:ic-uno-a-la-vez-para-betas">\[
\begin{equation}
\hat{\beta}_i \ \pm \ t_{1-\alpha/2\ ;\ n-(r+1)}\ \sqrt{\widehat{Var}( \ \hat{\beta_i}  \ )} \ \ , \ \ \ i=0,1,2,\ldots,r
\end{equation}
\tag{5.16}
\]</span></p>
<p>cuando se buscan variables predictoras importantes.</p>
<div class="example">
<p><span id="exm:ejemplo-mrlm" class="example"><strong>Ejemplo 5.2  (Modelo de Regresión Lineal Múltiple) </strong></span>Los siguientes datos se obtuvieron de la evaluación de 20 hogares en un vecindario de Milwaukee, Wisconsin.</p>
</div>
<table>
<caption>
<span id="tab:unnamed-chunk-95">Tabla 5.1: </span>Datos del Ejemplo de MRLM
</caption>
<thead>
<tr>
<th style="text-align:center;">
Z1
</th>
<th style="text-align:center;">
Z2
</th>
<th style="text-align:center;">
Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
15.31
</td>
<td style="text-align:center;">
57.3
</td>
<td style="text-align:center;">
74.8
</td>
</tr>
<tr>
<td style="text-align:center;">
15.20
</td>
<td style="text-align:center;">
63.8
</td>
<td style="text-align:center;">
74.0
</td>
</tr>
<tr>
<td style="text-align:center;">
16.25
</td>
<td style="text-align:center;">
65.4
</td>
<td style="text-align:center;">
72.9
</td>
</tr>
<tr>
<td style="text-align:center;">
14.33
</td>
<td style="text-align:center;">
57.0
</td>
<td style="text-align:center;">
70.0
</td>
</tr>
<tr>
<td style="text-align:center;">
14.57
</td>
<td style="text-align:center;">
63.8
</td>
<td style="text-align:center;">
74.9
</td>
</tr>
<tr>
<td style="text-align:center;">
17.33
</td>
<td style="text-align:center;">
63.2
</td>
<td style="text-align:center;">
76.0
</td>
</tr>
<tr>
<td style="text-align:center;">
14.48
</td>
<td style="text-align:center;">
60.2
</td>
<td style="text-align:center;">
72.0
</td>
</tr>
<tr>
<td style="text-align:center;">
14.91
</td>
<td style="text-align:center;">
57.7
</td>
<td style="text-align:center;">
73.5
</td>
</tr>
<tr>
<td style="text-align:center;">
15.25
</td>
<td style="text-align:center;">
56.4
</td>
<td style="text-align:center;">
74.5
</td>
</tr>
<tr>
<td style="text-align:center;">
13.89
</td>
<td style="text-align:center;">
55.6
</td>
<td style="text-align:center;">
73.5
</td>
</tr>
<tr>
<td style="text-align:center;">
15.18
</td>
<td style="text-align:center;">
62.6
</td>
<td style="text-align:center;">
71.5
</td>
</tr>
<tr>
<td style="text-align:center;">
14.44
</td>
<td style="text-align:center;">
63.4
</td>
<td style="text-align:center;">
71.0
</td>
</tr>
<tr>
<td style="text-align:center;">
14.87
</td>
<td style="text-align:center;">
60.2
</td>
<td style="text-align:center;">
78.9
</td>
</tr>
<tr>
<td style="text-align:center;">
18.63
</td>
<td style="text-align:center;">
67.2
</td>
<td style="text-align:center;">
86.5
</td>
</tr>
<tr>
<td style="text-align:center;">
15.20
</td>
<td style="text-align:center;">
57.1
</td>
<td style="text-align:center;">
68.0
</td>
</tr>
<tr>
<td style="text-align:center;">
25.76
</td>
<td style="text-align:center;">
89.6
</td>
<td style="text-align:center;">
102.0
</td>
</tr>
<tr>
<td style="text-align:center;">
19.05
</td>
<td style="text-align:center;">
68.6
</td>
<td style="text-align:center;">
84.0
</td>
</tr>
<tr>
<td style="text-align:center;">
15.37
</td>
<td style="text-align:center;">
60.1
</td>
<td style="text-align:center;">
69.0
</td>
</tr>
<tr>
<td style="text-align:center;">
18.06
</td>
<td style="text-align:center;">
66.3
</td>
<td style="text-align:center;">
88.0
</td>
</tr>
<tr>
<td style="text-align:center;">
16.35
</td>
<td style="text-align:center;">
65.8
</td>
<td style="text-align:center;">
76.0
</td>
</tr>
</tbody>
</table>
<p>donde:</p>
<p><span class="math inline">\(Z_1\)</span>: Tamaño total de la Vivienda (en cientos de metros cuadrados)</p>
<p><span class="math inline">\(Z_2\)</span>: Valor pedido por la Vivienda (en miles de dólares)</p>
<p><span class="math inline">\(Y\)</span>: Precio de venta de la Vivienda (en miles de dólares)</p>
<p>Ajustar el MRLM</p>
<p><span class="math display">\[
y_i=\beta_0 + \beta_1 \ Z_{i1} + \beta_2\ Z_{i2} + \varepsilon_i
\]</span></p>
<p>usando mínimos cuadrados.</p>
<p>De los datos se tiene que:
<span class="math display">\[
(\mathbf{Z}^t\mathbf{Z})^{-1}= \begin{bmatrix} 5.1523 &amp; 0.2544 &amp; -0.1463 \\
&amp; 0.0512 &amp; -0.0172 \\
&amp;&amp; 0.0067
\end{bmatrix}
\]</span></p>
<p>y
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}} = (\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\ \underline{\mathbf{y}}=\begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \\\hat{\beta}_2  \end{bmatrix}=\begin{bmatrix} 30.967 \\ 2.634 \\ 0.045 \end{bmatrix}
\]</span></p>
<p>de donde la ecuación de regresión ajustada es:
<span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_0 \ Z_z + \hat{\beta}_2 \ Z_2
\]</span></p>
<p><span class="math display">\[
\hat{y}= 30.967 + 2.634 \ Z_1 + 0.045 \ Z_2
\]</span></p>
<p>De los resultados se tiene que
<span class="math display">\[
s^2= \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{n-(r+1)} = \frac{\underline{\mathbf{y}^t\ [\ \mathbf{I}_n-\mathbf{H}\ ]\ \underline{\mathbf{y}}}}{n-r-1} =  12.059
\]</span>
es decir <span class="math inline">\(s=\)</span> 3.473, y la matriz de varianzas-covarianzas de <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span> es:
<span class="math display">\[
Var[\ \widehat{\underline{\boldsymbol \beta}}\ ] = s^2\ (\mathbf{Z}^t\mathbf{Z})^{-1}=\begin{bmatrix} 62.131 &amp; 3.068 &amp; -1.764 \\  
&amp; 0.617 &amp; -0.207 \\
&amp;&amp; 0.081
\end{bmatrix}
\]</span></p>
<p>de donde las desviaciones estándar de los <span class="math inline">\(\hat{\beta}_i\)</span> son:
<span class="math display">\[
s_{\hat{\beta}_0}= \sqrt{Var[\hat{\beta}_0]} = 7.882 \\
s_{\hat{\beta}_1}= \sqrt{Var[\hat{\beta}_1]} = 0.785 \\
s_{\hat{\beta}_2}= \sqrt{Var[\hat{\beta}_2]} = 0.285
\]</span></p>
<p>Además,
<span class="math display">\[
R^2=1- \frac{SSE}{SST}= 1- \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{\underline{\mathbf{y}}^t\underline{\mathbf{y}}-n\ \overline{y}^{\ 2}}=1-\frac{205.0013}{1237.87} = 0.834
\]</span></p>
<p>lo cual indica que los datos exhiben una fuerte relación de regresión lineal entre las dos variables regresoras <span class="math inline">\(Z_1\)</span> y <span class="math inline">\(Z_2\)</span> y la variable respuesta <span class="math inline">\(Y\)</span>. Si los residuales <span class="math inline">\(\hat{\varepsilon}_i\)</span> verifican los supuestos del modelo, entonces se podría utilizar la ecuación ajustada para predecir el precio de venta de otras viviendas en el vecindario a partir de su precio de oferta y si tamaño.</p>
<p>Ahora, un IC del <span class="math inline">\(95\%\)</span> para <span class="math inline">\(\beta_2\)</span>-está dado por:
<span class="math display">\[
\hat{\beta}_2 \ \pm \ t_{1-\alpha/2\ ;\ n-(r+1)}\ \sqrt{\widehat{Var}( \ \hat{\beta_2}  \ )}
\]</span>
<span class="math display">\[
\hat{\beta}_2 \ \pm \ t_{0.975\ ;\ 20-(2+1)}\ \sqrt{\widehat{Var}( \ \hat{\beta_2}  \ )}
\]</span></p>
<p><span class="math display">\[
0.045 \ \pm \ (2.11)\ \times \  (0.285 )
\]</span>
<span class="math display">\[
0.045 \ \pm \ 0.6 \Longleftrightarrow  (\ -0.555 \ , \ 0.645 \ )
\]</span></p>
<p>Dado que el intervalo de confianza incluye a <span class="math inline">\(\beta_2=0\)</span>, la variable <span class="math inline">\(Z_2\)</span> podría ser eliminada del modelo de regresión y repetir el análisis con una sola variable predictora dada por <span class="math inline">\(Z_1\)</span> que denota el tamaño de la vivienda, es decir, el precio de oferta <span class="math inline">\(Z_2\)</span> parece agregar muy poco a la predicción del precio de venta <span class="math inline">\(Y\)</span>.</p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value
## Model       1032.875  2    516.4375 42.8276
## Error        204.995 17     12.0585        
##           P_value
## Model 2.30186e-07
## Error</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ z[, 1] + z[, 2])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -5.589 -1.541 -0.072  1.351  6.460 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  30.9666     7.8822    3.93   0.0011 **
## z[, 1]        2.6344     0.7856    3.35   0.0038 **
## z[, 2]        0.0452     0.2852    0.16   0.8760   
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.47 on 17 degrees of freedom
## Multiple R-squared:  0.834,  Adjusted R-squared:  0.815 
## F-statistic: 42.8 on 2 and 17 DF,  p-value: 2.3e-07</code></pre>
<pre><code>##               2.5 %  97.5 %
## (Intercept) 14.3366 47.5966
## z[, 1]       0.9769  4.2919
## z[, 2]      -0.5565  0.6469</code></pre>
</div>
<div id="prueba-de-razón-de-verosimilitud-para-los-parámetros-de-regresión" class="section level4 hasAnchor" number="5.2.2.2">
<h4><span class="header-section-number">5.2.2.2</span> Prueba de Razón de Verosimilitud para los Parámetros de Regresión<a href="rlm.html#prueba-de-razón-de-verosimilitud-para-los-parámetros-de-regresión" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Parte del análisis de regresión se centra en evaluar los efectos de determinadas variables predictoras sobre la variable de respuesta. Una hipótesis nula de interés establece que algunas de las variables regresoras <span class="math inline">\(Z´s\)</span> no influyen en la variable respuesta <span class="math inline">\(Y\)</span>. Estas predictoras son etiquetadas como <span class="math inline">\(Z_{q+q},Z_{q+2},\ldots, Z_r\)</span>.</p>
<p>La afirmación de que las variables <span class="math inline">\(Z_{q+1},Z_{q+2},\ldots, Z_r\)</span> no influyen en la respuesta <span class="math inline">\(Y\)</span> se traducen en la siguiente hipótesis estadística:
<span class="math display" id="eq:hipotesis-suma-extra-de-cuadrados">\[
\begin{equation}
\begin{cases}
H_0\ \ : \ \ \beta_{q+1}=\beta_{q+2}=\cdots=\beta_{r}=0 \\ \\
H_a \ \ : \ \ \beta_i \neq 0 \ \ , \ \ p.a \ \ i=q+1,q+2,\ldots,r
\end{cases} \Longleftrightarrow
\begin{cases}
H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \underline{\mathbf{0}} \\ \\
H_a \ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} \neq \underline{\mathbf{0}}
\end{cases}
\end{equation}
\tag{5.17}
\]</span></p>
<p>donde,
<span class="math display">\[
\underline{\boldsymbol \beta}_{\ (2)} = \begin{bmatrix} \beta_{q+1} \\ \beta_{q+2} \\ \vdots \\ \beta_r \end{bmatrix}_{(r-q) \times 1}
\]</span></p>
<p>haciendo
<span class="math display">\[
\underset{n \times (r+1)}{\mathbf{Z} }= \begin{bmatrix} \underset{n \times (q+1)}{\mathbf{Z}_1} &amp; | &amp;  \underset{n \times (r-q)}{\mathbf{Z}_1} \end{bmatrix} \ \ \ \ , \ \ \ \ \ \underset{(r+1) \times 1}{\underline{\boldsymbol \beta } }= \begin{bmatrix} \underset{(q+1) \times 1}{\underline{\boldsymbol \beta }_{\ (1)} } \\ --- \\   \underset{ (r-q)\times 1}{\underline{\boldsymbol \beta }_{\ (2)}} \end{bmatrix}
\]</span></p>
<p>el modelo de regresión lineal múltiple
<span class="math display">\[
\underline{\mathbf{y}}= \mathbf{Z}\ \underline{\boldsymbol \beta} + \underline{\boldsymbol \varepsilon}
\]</span></p>
<p>se puede reescribir como sigue:
<span class="math display">\[
\underline{\mathbf{y}}= \begin{bmatrix} \mathbf{Z}_1 &amp; | &amp;  \mathbf{Z}_1 \end{bmatrix} \  \begin{bmatrix} \underline{\boldsymbol \beta}_{\ (1)}  \\ --- \\   \underline{\boldsymbol \beta}_{\ (2)} \end{bmatrix} +  \underline{\boldsymbol \varepsilon}  =  \mathbf{Z}_1\ \beta_{\ (1)} + \mathbf{Z}_2\ \beta_{\ (2)} + \underline{\boldsymbol \varepsilon}
\]</span></p>
<p>es decir,
<span class="math display" id="eq:mrlm-particionado">\[
\begin{equation}
\underline{\mathbf{y}} = \mathbf{Z}_1\ \beta_{\ (1)} + \mathbf{Z}_2\ \beta_{\ (2)} + \underline{\boldsymbol \varepsilon}
\end{equation}
\tag{5.18}
\]</span></p>
<p>Bajo la hipótesis nula: <span class="math inline">\(H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \underline{\mathbf{0}}\)</span>, se tiene que:
<span class="math display" id="eq:mrlm-reducido">\[
\begin{equation}
\underline{\mathbf{y}} = \mathbf{Z}_1\ \beta_{\ (1)} + \underline{\boldsymbol \varepsilon}
\end{equation}
\tag{5.19}
\]</span></p>
<p>La prueba de Razón de Verosimilitud de <span class="math inline">\(H_0\)</span>-se basa en la Suma de Cuadrados Extra, dada por:
<span class="math display" id="eq:mrlm-ss-extra">\[
\begin{equation}
SS_{Extra}= SS_{res}(\mathbf{Z}_1) - SS_{res} (\mathbf{Z}) \\ \\  
SS_{Extra} = ( \underline{\mathbf{y}} - \mathbf{Z}_1\ \widehat{\underline{\beta}}_{\ (1)} )^{\ t}\ ( \underline{\mathbf{y}} - \mathbf{Z}_1\ \widehat{\underline{\beta}}_{\ (1)} ) - ( \underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )^{\ t}\ ( \underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )
\end{equation}
\tag{5.20}
\]</span></p>
<p>donde, <span class="math inline">\(\widehat{\underline{\beta}}_{\ (1)}=(\mathbf{Z}_1^{\ t} \mathbf{Z}_1)^{-1}\ \mathbf{Z}_1^{\ t}\ \underline{\mathbf{y}}\)</span>.</p>
<div class="theorem">
<p><span id="thm:teorema-ss-extra" class="theorem"><strong>Teorema 5.6  (Suma Extra de Cuadrados) </strong></span>Sea <span class="math inline">\(\mathbf{Z}\)</span> de rango completo <span class="math inline">\(r+1\)</span> y <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>-distribuído como una <span class="math inline">\(N_n(\ \underline{\mathbf{0}}\ , \ \sigma^2\mathbf{I}_n \ )\)</span>.</p>
</div>
<p>La prueba de Razón de Verosimilitud para
<span class="math display" id="eq:hipotesis-subconjunto-de-coeficientes">\[
\begin{equation}
H_0\ \ : \ \ \underline{\boldsymbol \beta}_{\ (2)} = \underline{\mathbf{0}}
\end{equation}
\tag{5.21}
\]</span></p>
<p>es equivalente a probar <span class="math inline">\(H_0\)</span> basado en la Suma de Cuadrados Extra <a href="rlm.html#eq:mrlm-ss-extra">(5.20)</a> y en
<span class="math display">\[
s^2=\frac{ \widehat{\underline{\boldsymbol \varepsilon }}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon }}  }{n-(r+1)} = \frac{(\underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )^{\ t}(\underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )}{n-r-1}.
\]</span></p>
<p>En particular, la prueba de razón de verosimilitud rechaza <span class="math inline">\(H_0\)</span> si:
<span class="math display" id="eq:estad-f-suma-extra-de-cuadrados">\[
\begin{equation}
F_0=\frac{\biggl[\ SS_{res}(\mathbf{Z}_1) - SS_{res} (\mathbf{Z})\ \biggr]\biggl/(r-q)}{s^2} &gt; F_{1-\alpha \ ; \ r-q \ , \ n-r-1 }   
\end{equation}
\tag{5.22}
\]</span></p>
<p>donde, <span class="math inline">\(F_{1-\alpha \ ; \ r-q \ , \ n-r-1 }\)</span>-es el percentil superior <span class="math inline">\((1-\alpha)100\%\)</span> de la distribución <span class="math inline">\(F\)</span>-con <span class="math inline">\(r-q\)</span> y <span class="math inline">\(n-r-1\)</span> grados de libertad.</p>
<p><strong>Comentario:</strong></p>
<p>La prueba de razón de verosimilitud se implementa de la siguiente manera. Para probar si todos los coeficientes en un subconjunto dado son cero, se ajuste el modelo de regresión con y sin los términos correspondientes a estos coeficientes de dicho subconjunto a evaluar. La mejora en la suma cuadrática de residuos (o en la suma extra de cuadrados) se compara con la suma cuadrática de residuos del modelo completo a través de la estadística <span class="math inline">\(F\)</span> definida en <a href="rlm.html#eq:estad-f-suma-extra-de-cuadrados">(5.22)</a>. El mismo procedimiento se aplica incluso en situaciones de análisis de varianza, donde la matriz <span class="math inline">\(\mathbf{Z}\)</span> no es de rango completo.</p>
<p>Mas generalmente, es posible formular hipótesis concernientes a <span class="math inline">\((r-q)\)</span>-combinaciones lineales de <span class="math inline">\(\underline{\boldsymbol \beta}\)</span>, de la forma:
<span class="math display">\[
H_0 \ \ : \ \ \mathbf{C} \ \underline{\boldsymbol \beta} = \underline{\mathbf{b}}_{\ 0}
\]</span>
Sean la matriz de rango completo <span class="math inline">\(\underset{(r-q)\times (r+1)}{\mathbf{C}}\)</span> y sea <span class="math inline">\(\underline{\mathbf{b}}_{\ 0}=\underline{\mathbf{0}}\)</span> y consideremos la hipótesis:
<span class="math display">\[
H_0 \ \ : \ \ \mathbf{C} \ \underline{\boldsymbol \beta} = \underline{\mathbf{0}}
\]</span></p>
<p>Ésta hipótesis se reduce a la planteada en <a href="rlm.html#eq:hipotesis-subconjunto-de-coeficientes">(5.21)</a>,
<span class="math display">\[
H_0 \ \ : \ \ \underline{\boldsymbol \beta }_{\ (2)} = \underline{\mathbf{0}}
\]</span></p>
<p>con <span class="math inline">\(\mathbf{C}\)</span> dada por:
<span class="math display">\[
\underset{(r-q) \times (r+1)}{\mathbf{C} }= \begin{bmatrix} \underset{(r-q)\times (q+1)}{\mathbf{O} } &amp; | &amp;  \underset{(r-q) \times (r-q)}{\mathbf{I}} \end{bmatrix}
\]</span></p>
<p>de donde,
<span class="math display">\[
\mathbf{C}\ \underline{\boldsymbol \beta } = \begin{bmatrix} \underset{(r-q)\times (q+1)}{\mathbf{O} } &amp; | &amp;  \underset{(r-q) \times (r-q)}{\mathbf{I}} \end{bmatrix}  \begin{bmatrix} \underset{(q+1) \times 1}{\underline{\boldsymbol \beta }_{\ (1)} } \\ --- \\   \underset{ (r-q)\times 1}{\underline{\boldsymbol \beta }_{\ (2)}} \end{bmatrix} \\
= \underset{(r-q)\times 1}{\mathbf{O} } +  \underset{ (r-q)\times 1}{\underline{\boldsymbol \beta }_{\ (2)}} \\
\mathbf{C}\ \underline{\boldsymbol \beta } = \underset{ (r-q)\times 1}{\underline{\boldsymbol \beta }_{\ (2)}}
\]</span></p>
<p>es decir que:
<span class="math display">\[
H_0 \ \ : \ \ \mathbf{C} \ \underline{\boldsymbol \beta} = \underline{\mathbf{0}} \Longleftrightarrow H_0 \ \ : \ \  \underline{\boldsymbol \beta}_{\ (2)} = \underline{\mathbf{0}}
\]</span></p>
<p>Bajo el modelo completo se tiene que:
<span class="math display">\[
\mathbf{C} \ \widehat{\underline{\boldsymbol \beta}} \sim N_{r-q} \biggl( \ \mathbf{C} \ \underline{\boldsymbol \beta} \ , \ \sigma^2 \mathbf{C} \ (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1}\ \mathbf{C}^{\ t}   \ \biggr).
\]</span></p>
<p>A un nivel de significancia de <span class="math inline">\(\alpha\)</span> se rechaza <span class="math inline">\(H_0 \ \ : \ \ \mathbf{C} \ \underline{\boldsymbol \beta} = \underline{\mathbf{0}}\)</span> si el vector nulo <span class="math inline">\(\underline{\mathbf{0}}\)</span> no cae en el Elipsoide de Confianza del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\(\mathbf{C} \ \underline{\boldsymbol \beta}\)</span>.</p>
<p>Equivalentemente, se rechaza <span class="math inline">\(H_0 \ \ : \ \ \mathbf{C} \ \underline{\boldsymbol \beta} = \underline{\mathbf{0}}\)</span> si se cumple que:
<span class="math display" id="eq:estadistico-suma-extra-combinaciones">\[
\begin{equation}
F_0 = \frac{ (\mathbf{C} \ \widehat{\underline{\boldsymbol \beta}})^{\ t }\  \biggl(\mathbf{C} \ (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1}\ \mathbf{C}^{\ t}\biggr)^{-1}\ (\mathbf{C} \ \widehat{\underline{\boldsymbol \beta}}) \ }{s^2} \sim (r-q)\ F_{1-\alpha\ ; \ r-q \ , \ n-(r+1)}
\end{equation}
\tag{5.23}
\]</span></p>
<p>donde
<span class="math display">\[
s^2=\frac{ \widehat{\underline{\boldsymbol \varepsilon }}^{\ t}\ \widehat{\underline{\boldsymbol \varepsilon }}  }{n-(r+1)} = \frac{(\underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )^{\ t}(\underline{\mathbf{y}} - \mathbf{Z}\ \widehat{\underline{\beta}} )}{n-r-1}.
\]</span>
y <span class="math inline">\(F_{1-\alpha\ ; \ r-q \ , \ n-(r+1)}\)</span>-es el percentil <span class="math inline">\((1-\alpha)100\%\)</span>-superior de la distribución <span class="math inline">\(F\)</span> con <span class="math inline">\(r-q\)</span> y <span class="math inline">\(n-r-1\)</span> grados de libertad. La prueba en <a href="rlm.html#eq:estadistico-suma-extra-combinaciones">(5.23)</a> es la prueba de razón de verosimilitud y el numerador de la estadística <span class="math inline">\(F\)</span>- en <a href="rlm.html#eq:estadistico-suma-extra-combinaciones">(5.23)</a>, es la suma de cuadrados extra residual ocurrida al ajustar el modelo sujeto a la restricción dada en
<span class="math inline">\(\mathbf{C}\ \underline{\boldsymbol \beta} = \underline{\mathbf{0}}\)</span>, es decir la suma extra de cuadrados debido a <span class="math inline">\(H_0\ \ : \ \ \mathbf{C}\ \underline{\boldsymbol \beta} = \underline{\mathbf{0}}\)</span>.</p>
<p>El siguiente ejemplo ilustra cómo los diseños experimentales desbalanceados son fácilmente tratados con la teoría general de regresión lineal descrita.</p>
<div class="example">
<p><span id="exm:ejemplo-mrlm-suma-extra-de-cuadrados" class="example"><strong>Ejemplo 5.3  (Modelo de Regresión Lineal Múltiple (Diseño Desbalanceado)) </strong></span>En este ejemplo se evaluá la importancia de variables predictoras adicionales usando el acercamiento de Suma Extra de Cuadrados.</p>
</div>
<p>Los clientes hombres y mujeres calificaron el servicio en tres establecimientos (o ubicaciones) de una gran cadena de restaurantes. Las calificaciones del servicio se convirtieron en un índice. La siguiente tabla contiene los datos de <span class="math inline">\(n = 18\)</span> clientes. Cada punto en esta la tabla se clasifica según la ubicación, (1, 2 o 3) y el según el género (hombres = 0 y mujeres = 1). Esta categorización tiene el formato de una tabla de frecuencias de doble entrada con diferente número de observaciones por celda. Por ejemplo, la combinación de la ubicación 1 y hombres tiene 5 respuestas, mientras que la combinación de la ubicación 2 y la mujer tiene 2 respuestas. Introducción tres variables dumys para tener en cuenta la ubicación y dos variables dumys para tener en cuenta el género, se puede desarrollar un modelo de regresión lineal múltiple que relaciona el índice que representa el servicio <span class="math inline">\(Y\)</span>, la ubicación, el género y la “interacción” entre ubicación y género, utilizando la matriz de diseño <span class="math inline">\(\mathbf{Z}\)</span> correspondiente.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-100">Tabla 5.2: </span>Datos del Ejemplo de MRLM (Diseño Desbalanceado)
</caption>
<thead>
<tr>
<th style="text-align:center;">
Ubicación
</th>
<th style="text-align:center;">
Género
</th>
<th style="text-align:center;">
Servicio-Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
15.2
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
21.2
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
27.3
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
21.2
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
21.2
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
36.4
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
92.4
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
27.3
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
15.2
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
9.1
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
18.2
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
50.0
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
44.0
</td>
</tr>
<tr>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
63.6
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
15.2
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
30.3
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
36.4
</td>
</tr>
<tr>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
40.9
</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\begin{array}{cc|c|c}
&amp; &amp; \text{Ubicación} &amp;  \\\hline
&amp; H &amp; 5 \ \ \  5 \ \ \  2 &amp; 12 \\
\text{Genero} &amp;&amp;&amp; \\
&amp; M &amp; 2 \ \ \  2 \ \ \ 2 &amp; 6 \\\hline
&amp;&amp; 7 \ \ \  7 \ \ \  4 &amp; 18
\end{array}
\]</span></p>
<p><span class="math display">\[
\begin{array}{cc|c|c}
&amp; &amp; \text{Ubicación} &amp;  \\
  &amp; &amp; \hspace{1cm} 1 \ \hspace{5cm} \  2 \ \hspace{4cm} \ 3 &amp; \\\hline
&amp; H &amp; 15.2 \ , 21.2 \ , 27.3 \ , 21.2 \ , 21.2  \ \  \ \  27.3 \ ,15.2 \ ,9.1 \ ,18.2 \ ,50.0  \ \  \  \ 15.2 \ ,30.3 &amp; 12 \\
\text{Genero} &amp;&amp;&amp; \\
&amp; M &amp; \hspace{1cm} 36.4 \ ,92.4 \  \hspace{3cm} \  44.0 \ ,63.6 \ \hspace{2cm} \ 36.4\ ,40.9 &amp; 6 \\\hline
&amp;&amp; \hspace{1cm} 7 \    \hspace{5cm} 7 \ \ \ \  \hspace{3cm} \  4 &amp; 18
\end{array}
\]</span></p>
<table>
<caption>
<span id="tab:unnamed-chunk-101">Tabla 5.3: </span>Matriz Diseño (MRLM con Variables Dumys)
</caption>
<thead>
<tr>
<th style="text-align:center;">
Z0
</th>
<th style="text-align:center;">
Z1
</th>
<th style="text-align:center;">
Z2
</th>
<th style="text-align:center;">
Z3
</th>
<th style="text-align:center;">
Z4
</th>
<th style="text-align:center;">
Z5
</th>
<th style="text-align:center;">
Z6
</th>
<th style="text-align:center;">
Z7
</th>
<th style="text-align:center;">
Z8
</th>
<th style="text-align:center;">
Z9
</th>
<th style="text-align:center;">
Z10
</th>
<th style="text-align:center;">
Z11
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
</tr>
<tr>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
1
</td>
</tr>
</tbody>
</table>
<p>El vector de coeficientes se puede denotar por:
<span class="math display">\[
\underline{\boldsymbol \beta}^t= \biggl[\ \beta_0, \beta_1, \beta_2, \beta_3,\tau_1,\tau_2, \gamma_{11},\gamma_{12},\gamma_{21},\gamma_{22},\gamma_{31},\gamma_{32} \ \biggr]
\]</span></p>
<p>donde los <span class="math inline">\(\beta_i\ &#39;s\)</span>-representan el efecto de las ubicaciones de los restaurantes en la determinación del valor del índice de servicio <span class="math inline">\(Y\)</span>, los <span class="math inline">\(\tau_i\ &#39;s\)</span>-representan el efecto del género de los clientes en la determinación del valor del índice de servicio <span class="math inline">\(Y\)</span> y los <span class="math inline">\(\gamma_{ij}\ &#39;s\)</span>-representan los efectos de interacción de la ubicación y género sobre la determinación del valor del índice de servicio <span class="math inline">\(Y\)</span>.</p>
<p><strong>Modelo Completo con <span class="math inline">\(\mathbf{Z}\)</span>:</strong></p>
<p>La matriz diseño <span class="math inline">\(\mathbf{Z}\)</span>-no es de rango completo, éste es de rango: <span class="math inline">\(Rango(\mathbf{Z})=\)</span> 6.</p>
<p>De los resultados se tiene que
<span class="math display">\[
s^2= \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{n-(r+1)} = \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{n-Rango(\mathbf{Z})} = \frac{\underline{\mathbf{y}^t\ [\ \mathbf{I}_n-\mathbf{H}\ ]\ \underline{\mathbf{y}}}}{18-6} = \frac{2977.39}{12}  = 248.116
\]</span>
es decir <span class="math inline">\(s=\)</span> 15.752.</p>
<p>Además,
<span class="math display">\[
SS_{res}(\mathbf{Z}) = \widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}} = 2977.39
\]</span></p>
<p><strong>Modelo Reducido con <span class="math inline">\(\mathbf{Z}_1\)</span>:</strong></p>
<p>El Modelo sin los términos de efectos de interacción tiene como matriz diseño a <span class="math inline">\(\mathbf{Z}_1\)</span>-que consiste de las primeras 6-columnas de <span class="math inline">\(\mathbf{Z}\)</span>. Para la matriz diseño <span class="math inline">\(\mathbf{Z}_1\)</span> se tienen los siguientes cálculos.</p>
<p>La matriz diseño <span class="math inline">\(\mathbf{Z}_1\)</span>-no es de rango completo, éste es de rango: <span class="math inline">\(Rango(\mathbf{Z}_1)=\)</span> 4.</p>
<p>De los resultados se tiene que
<span class="math display">\[
s^2= \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{n-(r+1)} = \frac{\widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}}}{n-Rango(\mathbf{Z}_1)} = \frac{\underline{\mathbf{y}^t\ [\ \mathbf{I}_n-\mathbf{H}\ ]\ \underline{\mathbf{y}}}}{18-4} = \frac{3419.1471}{14}  = 244.225
\]</span>
es decir <span class="math inline">\(s=\)</span> 15.628.</p>
<p>Además,
<span class="math display">\[
SS_{res}(\mathbf{Z}_1) = \widehat{\underline{\boldsymbol \varepsilon}}^{\ t}\widehat{\underline{\boldsymbol \varepsilon}} = 3419.1471
\]</span></p>
<p>Para probar la hipótesis sobre la no existencia de efectos de interacción entre la ubicación y el género, expresada como:
<span class="math display">\[
H_0\ \ : \ \ \gamma_{11}=\gamma_{12}=\gamma_{21}=\gamma_{22}=\gamma_{31}=\gamma_{32}=0
\]</span></p>
<p>se utiliza la Estadística de Prueba dada por:
<span class="math display">\[
F_0 = \frac{ \biggl[\ SS_{res}(\mathbf{Z}_1) - SS_{res}(\mathbf{Z}) \ \biggr]\biggl/ \biggl( \ (n-rango(\mathbf{Z}_1)-(n-rango(\mathbf{Z}) \ \biggr) }{SS_{res}(\mathbf{Z})/(n-rango(\mathbf{Z}))} \\
\frac{ \biggl[\ SS_{res}(\mathbf{Z}_1) - SS_{res}(\mathbf{Z}) \ \biggr]\biggl/ \biggl( \ rango(\mathbf{Z}) - rango(\mathbf{Z}_1) \ \biggr) }{SS_{res}(\mathbf{Z})/(18-6)} \\  
= \frac{ \biggl[\ SS_{res}(\mathbf{Z}_1) - SS_{res}(\mathbf{Z}) \ \biggr]\biggl/ ( \ 6-4 \ ) }{ s^2 } \\
= \frac{ \biggl[\ 3419.1471 - 2977.39 ) \ \biggr]/ ( \ 2 \ ) }{ 248.116 } \\
F_0 = 0.89
\]</span></p>
<p>El estadístico <span class="math inline">\(F_0\)</span> se compara con el valor <span class="math inline">\(F_{tabla}\)</span> de la distribución <span class="math inline">\(F\)</span> con 2 y 12 grados de libertad respectivamente. El valor <span class="math inline">\(F_{tabla}=\)</span> 3.8853. Como <span class="math inline">\(F_0 = 0.89 &lt; 3.8853\)</span> entonces no se rechaza <span class="math inline">\(H_0\)</span>, es decir, no hay significancia estadística para <span class="math inline">\(H_0\)</span>, ie. concluimos que el índice de servicio <span class="math inline">\(Y\)</span> no depende de la interacción entre la ubicación y el género y por lo tanto estos términos pueden eliminarse de la modelo.</p>
<p>Usando el enfoque de Suma Extra de Cuadrados, podemos verificar que no hay
diferencia entre las ubicaciones (no hay efecto de ubicación), pero que el género si es significativo; es decir, hombres y mujeres no otorgan las mismas calificaciones al índice de servicio <span class="math inline">\(Y\)</span>.</p>
<p>En situaciones de análisis de varianza donde los conteos de celdas son desiguales, la variación en la respuesta atribuible a diferentes variables predictoras y a sus interacciones normalmente no se pueden separar en cantidades independientes. Para evaluar las influencias relativas de las predictoras sobre la respuesta en estos caso, es necesario ajustar el modelo con y sin los términos en cuestión y calcular el Estadísticas apropiado de prueba <span class="math inline">\(F\)</span>.</p>
<p><strong>Anova del Modelo Completo:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        4209.22  5     841.844 3.39295 0.0384888
## Error        2977.39 12     248.116</code></pre>
<p><strong>Anova del Modelo Reducido:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        3767.46  3    1255.821 5.14207 0.0132329
## Error        3419.15 14     244.225</code></pre>
<p><strong>Anova de PH Sobre Efectos de Interacción Entre Ubicación y Género:</strong></p>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5]
## Model 2: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5] + z[, 6] + z[, 
##     7] + z[, 8] + z[, 9] + z[, 10] + z[, 11]
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)
## 1     14 3419                         
## 2     12 2977  2       442 0.89   0.44</code></pre>
<p><strong>Análisis de Efectos de la Ubicación del Restaurante:</strong></p>
<p>Se trata de probar la hipótesis:
<span class="math display">\[
\begin{cases}
H_0:\ \ \ \beta_1=\beta_2=\beta_3=0 \\ \\
H_a: \ \ \ \beta_i\neq 0\ \ \ ;\ \  p.a\ \ i=1,2,3
\end{cases}
\]</span></p>
<p><strong>Anova del Modelo Completo Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        3767.46  3    1255.821 5.14207 0.0132329
## Error        3419.15 14     244.225</code></pre>
<p><strong>Anova del Modelo Reducido (para Género, ie. Bajo <span class="math inline">\(H_0\)</span>) Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value  P_value
## Model        3520.44  1    3520.444  15.364 0.001222
## Error        3666.17 16     229.135</code></pre>
<p><strong>Anova de PH Sobre Efectos de la Ubicación:</strong></p>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ z[, 4] + z[, 5]
## Model 2: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5]
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)
## 1     16 3666                         
## 2     14 3419  2       247 0.51   0.61</code></pre>
<p><strong>Análisis de Efectos del Género de los Clientes:</strong></p>
<p>Se trata de probar la hipótesis:
<span class="math display">\[
\begin{cases}
H_0:\ \ \ \tau_1=\tau_2=0 \\ \\
H_a: \ \ \ \tau_i\neq 0\ \ \ ;\ \  p.a\ \ i=1,2
\end{cases}
\]</span></p>
<p><strong>Anova del Modelo Completo:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        3767.46  3    1255.821 5.14207 0.0132329
## Error        3419.15 14     244.225</code></pre>
<p><strong>Anova del Modelo Reducido:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value  P_value
## Model        3520.44  1    3520.444  15.364 0.001222
## Error        3666.17 16     229.135</code></pre>
<p><strong>Anova de PH Sobre Efectos de Interacción Entre Ubicación y Género:</strong></p>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ z[, 4] + z[, 5]
## Model 2: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5]
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)
## 1     16 3666                         
## 2     14 3419  2       247 0.51   0.61</code></pre>
<p><strong>Análisis de Efectos de la Ubicación del Restaurante:</strong></p>
<p>Se trata de probar la hipótesis:
<span class="math display">\[
\begin{cases}
H_0:\ \ \ \beta_1=\beta_2=\beta_3=0 \\ \\
H_a: \ \ \ \beta_i\neq 0\ \ \ ;\ \  p.a\ \ i=1,2,3
\end{cases}
\]</span></p>
<p><strong>Anova del Modelo Completo Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        3767.46  3    1255.821 5.14207 0.0132329
## Error        3419.15 14     244.225</code></pre>
<p><strong>Anova del Modelo Reducido (para Género, ie. Bajo <span class="math inline">\(H_0\)</span>) Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value  P_value
## Model        3520.44  1    3520.444  15.364 0.001222
## Error        3666.17 16     229.135</code></pre>
<p><strong>Anova de PH Sobre Efectos de la Ubicación:</strong></p>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ z[, 4] + z[, 5]
## Model 2: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5]
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)
## 1     16 3666                         
## 2     14 3419  2       247 0.51   0.61</code></pre>
<p><strong>Análisis de Efectos del Género de los Clientes:</strong></p>
<p>Se trata de probar la hipótesis:
<span class="math display">\[
\begin{cases}
H_0:\ \ \ \tau_1=\tau_2=0 \\ \\
H_a: \ \ \ \tau_i\neq 0\ \ \ ;\ \  p.a\ \ i=1,2
\end{cases}
\]</span></p>
<p><strong>Anova del Modelo Completo Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value   P_value
## Model        3767.46  3    1255.821 5.14207 0.0132329
## Error        3419.15 14     244.225</code></pre>
<p><strong>Anova del Modelo Reducido (para Ubicación, ie. Bajo <span class="math inline">\(H_0\)</span>) Sin Efectos de Interacción:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square  F_Value  P_value
## Model        20.7837  2     10.3919 0.021753 0.978513
## Error      7165.8257 15    477.7217</code></pre>
<p><strong>Anova de PH Sobre Efectos del Género de los Clientes:</strong></p>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ z[, 1] + z[, 2] + z[, 3]
## Model 2: y ~ z[, 1] + z[, 2] + z[, 3] + z[, 4] + z[, 5]
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)   
## 1     15 7166                            
## 2     14 3419  1      3747 15.3 0.0015 **
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="inferencias-a-partir-de-la-función-de-regresión-estimada" class="section level3 hasAnchor" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Inferencias A partir de la Función de Regresión Estimada<a href="rlm.html#inferencias-a-partir-de-la-función-de-regresión-estimada" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una vez que el investigador esta satisfecho con el modelo de regresión ajustado, éste modelo se puede usar para resolver dos problemas de predicción.</p>
<p>Sea el vector de valores de las variables regresoras dado por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}=\begin{bmatrix} 1 \\ z_{01} \\ z_{02} \\ \vdots \\ z_{0r} \end{bmatrix}.
\]</span></p>
<p><span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span> y <span class="math inline">\(\underline{\widehat{\boldsymbol \beta }}\)</span> se pueden usar para:</p>
<ol style="list-style-type: decimal">
<li><p>Estimar la función de regresión: <span class="math inline">\(\beta_0+\beta_1\ z_{01} + \cdots + \beta_r\ z_{or}\)</span> en el vector: <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>.</p></li>
<li><p>Estimar el valor de la respuesta <span class="math inline">\(Y\)</span> en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>.</p></li>
</ol>
<div id="estimación-de-la-función-de-regresión-en-underlinemathbfz_-0" class="section level4 hasAnchor" number="5.2.3.1">
<h4><span class="header-section-number">5.2.3.1</span> Estimación de la Función de regresión en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span><a href="rlm.html#estimación-de-la-función-de-regresión-en-underlinemathbfz_-0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sea <span class="math inline">\(Y_0\)</span>-el valor de la respuesta <span class="math inline">\(Y\)</span> cuando las variables predictoras toman los valores dados en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>. De acuerdo al modelo definido en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a>, el valor esperado de <span class="math inline">\(Y_0\)</span> es:
<span class="math display" id="eq:valor-esperado-y0">\[
\begin{equation}
E\bigl[\ Y_0 \ \bigl| \ \underline{\mathbf{z}}_{\ 0}  \ \bigr]= \beta_0+\beta_1\ z_{01} + \cdots + \beta_r\ z_{or} = \underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\boldsymbol \beta }
\end{equation}
\tag{5.24}
\]</span></p>
<p>Ahora, el EStimador de Mínimos CUadrados de <span class="math inline">\(E\bigl[\ Y_0 \ \bigl| \ \underline{\mathbf{z}}_{\ 0} \ \bigr]=\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\boldsymbol \beta }\)</span> está dado por: <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}\)</span>.</p>
<div class="theorem">
<p><span id="thm:teorema-ic-para-respuesta-media" class="theorem"><strong>Teorema 5.7  (Estimación de IC para la Respuesta Media) </strong></span>Para el Modelo de Regresión definido en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a>, <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}\)</span> es el Estimador Lineal Insesgado de <span class="math inline">\(E\bigl[\ Y_0 \ \bigl| \ \underline{\mathbf{z}}_{\ 0} \ \bigr]\)</span> con Mínima Varianza, dada por: <span class="math inline">\(Var[\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}]=\sigma^2\ \underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0}\)</span>.</p>
</div>
<p>Si además, los errores <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>-son normalmente distribuidos, entonces, un IC del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\(E\bigl[\ Y_0 \ \bigl| \ \underline{\mathbf{z}}_{\ 0} \ \bigr]=\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\boldsymbol \beta }\)</span> está dado por:
<span class="math display" id="eq:ic-de-respuesta-media">\[
\begin{equation}
\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }} \ \pm \ t_{\alpha/2\ , \ n-(r+1)}\ \sqrt{s^2\ (\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} )}
\end{equation}
\tag{5.25}
\]</span></p>
<p>donde, <span class="math inline">\(t_{\alpha/2\ , \ n-(r+1)}\)</span>-es el percentil <span class="math inline">\((1-\alpha)100\%\)</span>-superior de la distribución <span class="math inline">\(t\)</span>-Student con <span class="math inline">\(n-(r+1)\)</span>-grados de libertad.</p>
</div>
<div id="pronóstico-de-una-nueva-obervación-en-underlinemathbfz_-0" class="section level4 hasAnchor" number="5.2.3.2">
<h4><span class="header-section-number">5.2.3.2</span> Pronóstico de una Nueva Obervación en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span><a href="rlm.html#pronóstico-de-una-nueva-obervación-en-underlinemathbfz_-0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Predecir una nueva observación, tal como <span class="math inline">\(Y_0\)</span>, en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>, es más incierto que <em>estimar</em> el Valor Esperado de <span class="math inline">\(Y_0\)</span>. De acuerdo al modelo de regresión de <a href="rlm.html#eq:mrlm-matricial">(5.4)</a>, se tiene que:
<span class="math display">\[
Y_0=\underline{\mathbf{z}}_{\ 0}^{\ t}\  \underline{\boldsymbol \beta} + \varepsilon_0
\]</span></p>
<p>o equivalentemente,
<span class="math display">\[
(\text{la nueva respuesta}\ \ Y_0)= (\text{al valor esperado de}\ \  Y_0 \ \ \text{en} \ \ \underline{\mathbf{z}}_{\ 0}) + (\text{un nuevo error})
\]</span></p>
<p>donde, <span class="math inline">\(\varepsilon_0 \sim N(0\ , \ \sigma^2)\)</span> y es independiente de <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>, y por lo tanto es independiente de: <span class="math inline">\(\widehat{\underline{ \boldsymbol \beta }}\)</span> y de <span class="math inline">\(s^2\)</span>. El error <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span> influye para estimar a <span class="math inline">\(\widehat{\underline{ \boldsymbol \beta }}\)</span> y a <span class="math inline">\(s^2\)</span> a través de la respuesta <span class="math inline">\(\underline{\mathbf{y}}\)</span>, pero <span class="math inline">\(\varepsilon_0\)</span>-no influye.</p>
<div class="theorem">
<p><span id="thm:teorema-intervalo-de-predicción" class="theorem"><strong>Teorema 5.8  (Estimación de un IP para un Valor Futuro de la Respuesta) </strong></span>Para el Modelo de Regresión definido en <a href="rlm.html#eq:mrlm-matricial">(5.4)</a>, una nueva observación <span class="math inline">\(Y_0\)</span> tiene el <em>Predictor Insesgado</em> dado por:</p>
</div>
<p><span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}=\hat{\beta}_0 + \hat{\beta}_1\ z_{01} + \cdots + \hat{\beta}_r
\ z_{0r}.
\]</span></p>
<p>La Varianza del <em>Error de Pronóstico</em> <span class="math inline">\(Y_0-\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}\)</span> está dada por:
<span class="math display">\[
Var[\ Y_0-\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }}\ ]=\sigma^2\ \biggl[ 1+ \underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} \biggr].
\]</span></p>
<p>Si además, los errores <span class="math inline">\(\underline{\boldsymbol \varepsilon}\)</span>-son normalmente distribuidos, entonces, un <em>Intervalo de Predicción</em> del <span class="math inline">\((1-\alpha)100\%\)</span> para <span class="math inline">\(Y_0\)</span> está dado por:
<span class="math display" id="eq:intervalo-prediccion">\[
\begin{equation}
\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }} \ \pm \ t_{\alpha/2\ , \ n-(r+1)}\ \sqrt{s^2\ \biggl(1+\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} \biggr)}
\end{equation}
\tag{5.26}
\]</span></p>
<p>donde, <span class="math inline">\(t_{\alpha/2\ , \ n-(r+1)}\)</span>-es el percentil <span class="math inline">\((1-\alpha)100\%\)</span>-superior de la distribución <span class="math inline">\(t\)</span>-Student con <span class="math inline">\(n-(r+1)\)</span>-grados de libertad.</p>
<p><strong>Nota:</strong></p>
<p>El intervalo de predicción para <span class="math inline">\(Y_0\)</span> es más amplio que el intervalo de confianza para estimar el valor de la función de regresión <span class="math inline">\(E\bigl[\ Y_0 \ \bigl| \ \underline{\mathbf{z}}_{\ 0} \ \bigr]=\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\boldsymbol \beta }\)</span>. La incertidumbre adicional al pronosticar a <span class="math inline">\(Y_0\)</span>, la cual está representada por el término adicional <span class="math inline">\(s^2\)</span> en la expresión
<span class="math inline">\(s^2\ \biggl(1+\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} \biggr)\)</span>, proviene de la presencia del término de error desconocido, <span class="math inline">\(\varepsilon_0\)</span>.</p>
<div class="example">
<p><span id="exm:ejemplo2-mrlm" class="example"><strong>Ejemplo 5.4  (Intervalos para la Respuesta Media y para una Respuesta Futura) </strong></span>Las compañías que consideran la compra de una computadora primero evaluán sus necesidades futuras con el fin de determinar el equipo adecuado. Un ingenierio recopiló datos de siete sitios de empresas similares con el fin de desarrollar una ecuación para pronosticar los requisitos del hardware de la computadora para la gestión de inventario. Los datos se dan en la siguiente tabla.</p>
</div>
<p>Las variables medidas son:</p>
<p><span class="math inline">\(Z_1:\)</span> Pedidos de clientes (en miles)</p>
<p><span class="math inline">\(Z_2:\)</span> Recuento de elementos para agregar y/o eliminar (en miles)</p>
<p><span class="math inline">\(Y:\)</span> Tiempo de la CPU (unidad central de procesamiento) (en horas)</p>
<table>
<caption>
<span id="tab:unnamed-chunk-123">Tabla 5.4: </span>Datos del Ejemplo de IC e I-Predicción
</caption>
<thead>
<tr>
<th style="text-align:center;">
Z1-Pedidos
</th>
<th style="text-align:center;">
Z2-Elementos
</th>
<th style="text-align:center;">
Tiempo-Y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
123.5
</td>
<td style="text-align:center;">
2.108
</td>
<td style="text-align:center;">
141.5
</td>
</tr>
<tr>
<td style="text-align:center;">
146.1
</td>
<td style="text-align:center;">
9.213
</td>
<td style="text-align:center;">
168.9
</td>
</tr>
<tr>
<td style="text-align:center;">
133.9
</td>
<td style="text-align:center;">
1.905
</td>
<td style="text-align:center;">
154.8
</td>
</tr>
<tr>
<td style="text-align:center;">
128.5
</td>
<td style="text-align:center;">
0.815
</td>
<td style="text-align:center;">
146.5
</td>
</tr>
<tr>
<td style="text-align:center;">
151.5
</td>
<td style="text-align:center;">
1.061
</td>
<td style="text-align:center;">
172.8
</td>
</tr>
<tr>
<td style="text-align:center;">
136.2
</td>
<td style="text-align:center;">
8.603
</td>
<td style="text-align:center;">
160.1
</td>
</tr>
<tr>
<td style="text-align:center;">
92.0
</td>
<td style="text-align:center;">
1.125
</td>
<td style="text-align:center;">
108.5
</td>
</tr>
</tbody>
</table>
<p>Construir un IC del <span class="math inline">\(95\%\)</span> para la respuesta media (Tiempo promedio de la CPU),
<span class="math display">\[
E[\ y_0\ | \ \underline{\mathbf{z}}_{\ 0}  \ ]= \beta_0 + \beta_1 z_{01} + \beta_2 z_{02},
\]</span></p>
<p>para <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}=[1,130,7.5]\)</span>.</p>
<p>Además, hallar un I-Predicción del <span class="math inline">\(95\%\)</span> para un nuevo requerimiento de CPU correspondiente al mismo <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>.</p>
<p>A partir de los datos se tiene que:
<span class="math display">\[
\hat{y}=\hat{\beta_0} + \hat{\beta}_1z_1 + \hat{\beta}_2z_2
\]</span></p>
<p>es decir:
<span class="math display">\[
\hat{y}=8.42 + 1.08\ z_1 + 0.42\ z_2
\]</span></p>
<p>Además,
<span class="math display">\[
(\mathbf{Z}^t\mathbf{Z})^{-1}= \begin{bmatrix} 8.1797 &amp; -0.0641 &amp; 0.0883 \\ &amp; 5.2\times 10^{-4} &amp; -0.0011 \\ &amp;&amp; 0.0144 \end{bmatrix}
\]</span></p>
<p>y <span class="math inline">\(s=\)</span> 1.204.</p>
<p>Por lo tanto, la Respuesta media estimada en <span class="math inline">\(\underline{\mathbf{z}}_0\)</span>, ie. <span class="math inline">\(\widehat{Y}_0\)</span> es:
<span class="math display">\[
\widehat{\underline{\boldsymbol \mu}}_{\ {Y}_0}=\widehat{E}[\ Y\ |\ \underline{\mathbf{z}}_{\ 0}\ ]=\widehat{Y}_0=\underline{\mathbf{z}}_{\ 0}^t\ \widehat{\underline{\boldsymbol \beta}}=[1\ \ 130 \ \ 7.5]\begin{bmatrix} 8.42 \\ 1.08 \\ 0.42 \end{bmatrix}=151.8406
\]</span>
y el error estándar de dicha estimación es:
<span class="math display">\[
S_{\widehat{\underline{\boldsymbol \mu}}_{\ {Y}_0}}=\sqrt{ Var[\ \widehat{\underline{\boldsymbol \mu}}_{\ {Y}_0}\ ]}=s\sqrt{\underline{\mathbf{z}}_{\ 0}^t(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}}=1.204\sqrt{0.37}=1.204 (0.6082)\\
=0.7323
\]</span></p>
<p>Además, <span class="math inline">\(t_{n-r-1;1-\alpha/2}=t_{7-2-1;0.05/2}=t_{4;0.975}=2.776\)</span>.</p>
<p>De lo anterior se tiene que un IC del <span class="math inline">\(95\%\)</span> para la respuesta media en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>, ie. para: <span class="math inline">\(\underline{\boldsymbol \mu}_{\ {Y}_0}=E[\ Y\ |\ \underline{\mathbf{z}}_{\ 0}\ ]\)</span>, esta dado por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }} \ \pm \ t_{\alpha/2\ , \ n-(r+1)}\ \sqrt{s^2\ (\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} )} \Longleftrightarrow 151.8406\  \pm \ 2.776(0.7323)\\ \Longleftrightarrow (149.8073 \ , \ 153.8738).
\]</span></p>
<p>Ahora para calcular el Intervalo de Predicción en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>, primero se halla el error estándar de estimación de dicho valor futuro, dado por:
<span class="math display">\[
S_{\ \widehat{Y}_0}=\sqrt{ Var[\ \widehat{Y}_0\ ]}=s\ \sqrt{1+\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} }=1.204\sqrt{1+0.37}=1.204\sqrt{1.37}\\
=(1.204)(1.1704)
=1.4092
\]</span></p>
<p>de donde, un intervalo de predicción para un nuevo valor de <span class="math inline">\(Y\)</span> en <span class="math inline">\(\underline{\mathbf{z}}_{\ 0}\)</span>, ie. para <span class="math inline">\(Y_0\)</span>, esta dado por:
<span class="math display">\[
\underline{\mathbf{z}}_{\ 0}^{\ t}\ \underline{\widehat{ \boldsymbol \beta }} \ \pm \ t_{\alpha/2\ , \ n-(r+1)}\ \sqrt{s^2\ \biggl(1+\underline{\mathbf{z}}_{\ 0}^{\ t} (\mathbf{Z}^{\ t}\ \mathbf{Z})^{-1} \underline{\mathbf{z}}_{\ 0} \biggr)} \Longleftrightarrow 151.8406\ \pm \ 2.776(1.4092)\\
\Longleftrightarrow (147.928 \ , \ 155.7532).
\]</span></p>
<p><strong>Coeficiente Estimados del Modelo:</strong></p>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)     8.42       3.44    2.45     0.07
## z1              1.08       0.03   39.25     0.00
## z2              0.42       0.14    2.91     0.04</code></pre>
<p><strong>Tabla Anova:</strong></p>
<pre><code>##       Sum_of_Squares DF Mean_Square F_Value
## Model     2801.67929  2  1400.83964 966.454
## Error        5.79785  4     1.44946        
##           P_value
## Model 4.26483e-06
## Error</code></pre>
<p><strong>Valor de <span class="math inline">\(h_{00}=\underline{\mathbf{z}}_{\ 0}^{\ t}(\mathbf{Z}^t\mathbf{Z})^{-1}\underline{\mathbf{z}}_{\ 0}\)</span>:</strong></p>
<pre><code>##      [,1]
## [1,] 0.37</code></pre>
<p><strong>IC de Respuesta Media:</strong></p>
<pre><code>##     fit   lwr   upr
## 1 151.8 149.8 153.9</code></pre>
<p><strong>Intervalo de Predicción de una Observación Futura:</strong></p>
<pre><code>##     fit   lwr   upr
## 1 151.8 147.9 155.8</code></pre>
</div>
</div>
<div id="validación-de-los-supuestos-del-modelo-y-otros-aspectos-del-la-regresión" class="section level3 hasAnchor" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Validación de los Supuestos del Modelo y Otros Aspectos del la Regresión<a href="rlm.html#validación-de-los-supuestos-del-modelo-y-otros-aspectos-del-la-regresión" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suponiendo que el modelo es “correcto”, se ha utilizado la función de regresión estimada para hacer inferencias. Por supuesto, es imperativo examinar la idoneidad del modelo antes de que la función estimada se convierta en una parte permanente en la toma de decisiones.</p>
<p>Toda la información sobre falta de ajuste del Modelo está contenida en los residuales:
<span class="math display">\[
\hat{\varepsilon}_1 = y_1 -\hat{\beta}_0 - \hat{\beta_1}\ z_{11} - \cdots - \hat{\beta}_r\ z_{1r} \\
\hat{\varepsilon}_2 = y_2 -\hat{\beta}_0 - \hat{\beta_1}\ z_{21} - \cdots - \hat{\beta}_r\ z_{2r} \\
\vdots \\
\hat{\varepsilon}_n = y_n -\hat{\beta}_0 - \hat{\beta_1}\ z_{n1} - \cdots - \hat{\beta}_r\ z_{nr}
\]</span>
o equivalentemente,
<span class="math display" id="eq:residuales-matricial">\[
\begin{equation}
\underset{n \times 1}{\underline{\widehat{ \boldsymbol \varepsilon}} } = [\ \mathbf{I}-\mathbf{H}\ ]\ \underline{\mathbf{y}}
\end{equation}
\tag{5.27}
\]</span></p>
<p>recuerde que: <span class="math inline">\(\underset{n \times 1}{\underline{\widehat{ \boldsymbol \varepsilon}} }=\underline{\mathbf{y}}- \widehat{ \underline{\mathbf{y}}}=\underline{\mathbf{y}}- \mathbf{Z}\ \widehat{ \underline{\boldsymbol \beta }}=\underline{\mathbf{y}}- \mathbf{Z}\ (\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\ \underline{\mathbf{y}}=[\mathbf{I}-\mathbf{Z}\ (\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t]\underline{\mathbf{y}}=[\ \mathbf{I}-\mathbf{H}\ ]\ \underline{\mathbf{y}}\)</span>.</p>
<p>Si el Modelo es válido, cada <span class="math inline">\(\widehat{\varepsilon}_j\)</span>-es un estimador del error <span class="math inline">\(\varepsilon_j\)</span>-los cuales se asumen que son una variables aleatoria con media cero y varianza <span class="math inline">\(\sigma^2\)</span>. Aunque los residuales <span class="math inline">\(\widehat{\boldsymbol \varepsilon}\)</span>-tienen valor esperado <span class="math inline">\(\underline{\mathbf{0}}\)</span>, su matriz de varianzas-covarianzas no es diagonal, pues:
<span class="math display">\[
Var\bigl[ \ \underline{\widehat{\boldsymbol \varepsilon}}\ \bigr]=Var\bigl[ \  (\mathbf{I}-\mathbf{H})\ \underline{\mathbf{y}}\ \bigr]=\sigma^2\ [\ \mathbf{I}-\mathbf{H}\ ] \neq Diag(\mathbf{A})
\]</span></p>
<p>Con esto anterior, se observa que los residuales tienen varianzas-desiguales y correlaciones distintas de cero. Además, las correlaciones son frecuentemente pequeñas y las varianzas son muy cercanamente iguales.</p>
<p>Debido a que los residuales <span class="math inline">\(\underline{\widehat{\boldsymbol \varepsilon}}\)</span>-tienen como Matriz de Varianzas-Covarianzas a: <span class="math inline">\(\sigma^2\ [\ \mathbf{I}-\mathbf{H}\ ] \neq Diag(\mathbf{A})\)</span>, las varianzas de <span class="math inline">\(\varepsilon_j\)</span>, puede variar mucho, si los elementos diagonales de <span class="math inline">\(\mathbf{H}\)</span>, es decir, los apalancamientos <span class="math inline">\(h_{jj}\)</span>, son sustancialmente
diferente. En consecuencia, muchos estadísticos prefieren diagnósticos gráficos basados sobre <em>Residuales-Studentizados</em>.</p>
<p>Utilizando la media cuadrática residual <span class="math inline">\(s^2\)</span>-como un Estimador de <span class="math inline">\(\sigma^2\)</span>, tenemos que:
<span class="math display" id="eq:varianzas-de-los-residuales">\[
\begin{equation}
\widehat{Var}\big[\ \hat{\varepsilon}_j \ \bigr]=s^2 (1-h_{jj}) \ \ , \ \ j=1,2,\cdots,n
\end{equation}
\tag{5.28}
\]</span></p>
<p>y los <strong>Residuales Studentizaos</strong> son:
<span class="math display" id="eq:residuales-studentizados">\[
\begin{equation}
\widehat{\varepsilon}_j^{\star}=\frac{\widehat{\varepsilon}_j}{\sqrt{ s^2 (1-h_{jj})}} \ \ , \ \ j=1,2,\cdots,n.
\end{equation}
\tag{5.29}
\]</span></p>
<p>Se espera que los residuos Studentizados luzcan, aproximadamente, como lucen las observaciones independientes de una distribución <span class="math inline">\(N(0,1)\)</span>. Algunos paquetes de software estadísticos, van un paso más allá y Studentizan los <span class="math inline">\(\widehat{\varepsilon}_j\)</span> usando la varianza estimada eliminando una observación a la vez denotada por <span class="math inline">\(s^2_{(j)}\)</span>, la es la Media Cuadrática de Residuales cuando la <span class="math inline">\(j\)</span>-ésima observación se elimina del análisis.</p>
<div id="gráficas-de-residuales" class="section level4 hasAnchor" number="5.2.4.1">
<h4><span class="header-section-number">5.2.4.1</span> Gráficas de Residuales<a href="rlm.html#gráficas-de-residuales" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los residuales se deben graficar de varias formas, para detectar posibles anomalías del Modelo. Para propósitos generales de Diagnóstico, se tienen las siguientes gráficas útiles:</p>
<ol style="list-style-type: decimal">
<li><strong>Gráfico de Residuales vs. Valores Predichos</strong> En el gráfico de los residuales <span class="math inline">\(\hat{\varepsilon}_j\)</span> contra los valores predichos: <span class="math inline">\(\hat{y}_j=\hat{\beta}_0+\hat{\beta}_1\ z_{j1}+\hat{\beta}_2\ z_{2j}+\cdots+\hat{\beta}_r\ z_{rj}\)</span>, las desviaciones de los supuestos del Modelo suelen indicarse mediante dos tipos de fenómenos:</li>
</ol>
<p>(a). <em>Una dependencia de los residuales sobre los valores predichos</em>. Esto se ilustra en la figura xxxx. Los calculos numéricos son incorrectos, o el término <span class="math inline">\(\beta_0\)</span>-ha sido omitido del modelo.</p>
<p>(b). <em>La Varianza de los residuales no es Constante</em>. El patrón del gráfico de residuales puede ser en forma de embudo, como en la figura xxxx, en donde existe una gran variabilidad de los residuales, para valores grande de <span class="math inline">\(\hat{y}\)</span> y una variabilidad pequeña de los residuales, para valores pequeños de <span class="math inline">\(\hat{y}\)</span>. Si éste es el caso, la varianza de los errores no es constante, y algunas transformaciones, o un acercamiento de Mínimos Cuadrados Ponderados (o ambos), son requeridos. En la figura xxxx, los residuales forman una banda horizontal, éste es el gráfico ideal, donde indica varianzas iguales o constante y que no dependen de <span class="math inline">\(\hat{y}\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Gráfico de Residuales vs. Algunas Variables Predichas</strong></li>
</ol>
<p>Sien la gráfica de los residuales <span class="math inline">\(\hat{\varepsilon}_j\)</span> contra una variables predictora o regresora, tal como <span class="math inline">\(z_{1}\)</span>, o contra productos de predictoras, tales como <span class="math inline">\(z_1^2\)</span> o <span class="math inline">\(z_1z_2\)</span>, se observa un patrón sistemático, como por ejemplo el de la fgura xxxx, entonces sugieren que se necesitan mas términos en el modelo.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Gráficos qq-plot e Histogramas de Residuales</strong></li>
</ol>
<p>Con este tipo de gráficos se trata de responder a preguntas como por ejemplo: ¿Están los errores distribuidos normalmente? Para dar respuesta a dicha pregunta, se pueden examinar los residuales <span class="math inline">\(\hat{\varepsilon}_j\)</span> o <span class="math inline">\(\hat{\varepsilon}_j^\star\)</span> usando las técnicas vistas en capítulos anterirores sobre evaluación de la normalidad a nivel univariado. Los gráficos <span class="math inline">\(qq-plot\)</span>, los histogramas y los diagramas de puntos, ayudan a detectar la presencia de observaciones atípicas y alejamientos severos de la normalidad, los cuales pueden requerir especial atención en el análisis. Si el tamaño de muestra <span class="math inline">\(n\)</span>-es grande, los alejamientos menores a la normalidad podrían no afectar gravemente las inferencias acerca del vector de parámetros <span class="math inline">\(\underline{\boldsymbol \beta}\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Gráficos de Residuales vs. el Tiempo</strong></li>
</ol>
<p>El supuesto de independencia de los errores es crucial pero difícil de examinar. Si los datos son de naturaleza cronológica, un gráfico de los residuales contra el orden de recolección de dichos datos puede revelar un patrón sistemático. (Un gráfico de las posiciones de los residuales en el espacio, también puede revelar asociaciones entre los errores). Entre los posibles patrones comúnmente detectados que, los residuales que crecen con el tiempo indican una fuerte dependencia positiva entre ellos. Un Estadístico de Prueba de Independencia se puede construir a partir de la primera autocorrelación de residuales de periodods adjuntos, dada por:
<span class="math display">\[
r_1=\frac{\sum_{j=2}^n\ \hat{\varepsilon}_j\ \hat{\varepsilon}_{j - 1}}{\sum_{j=1}^n\ \hat{\varepsilon}_j^2}
\]</span></p>
<p>Una prueba popular basada sobre la Estadística:
<span class="math display">\[
DW= \frac{\sum_{j=2}^n\ (\hat{\varepsilon}_j-\hat{\varepsilon}_{j-1})^2}{\sum_{j=1}^n\ \hat{\varepsilon}_j^2} \doteq 2(1-r_1),
\]</span></p>
<p>es la Llamada Preuba de Durbin-Wastson, ver <span class="citation">(<a href="#ref-durbin1971">Durbin and Watson 1971</a>)</span>, para la descripción de dicha prueba y tablas de valores críticos.</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Prueba de Falta de Ajuste</strong></li>
</ol>
<p>Si existen varias observaciones de la variables respuesta <span class="math inline">\(Y\)</span> para los mismo valores de las variables predictoras, entonces existe una prueba formal de Falta de Ajuste que puede llevarse a cabo, ver <span class="citation">(<a href="#ref-draper1998">Draper and Smith 1998</a>)</span>, para una discusión de dicha prueba de Falta de Ajuste.</p>
<div class="example">
<p><span id="exm:ejemplo3-mrlm" class="example"><strong>Ejemplo 5.5  (Análisis de Residuales) </strong></span>Continaundo con el ejemplo <a href="rlm.html#exm:ejemplo2-mrlm">5.4</a>, se tienen las siguientes gráficas de residuales.</p>
</div>
<p><strong>Evaluación de Normalidad de Residuales:</strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico1-npp-residuales"></span>
<img src="bookdown-iam_files/figure-html/grafico1-npp-residuales-1.png" alt="Gráfico NPP de Residuales" width="600px" />
<p class="caption">
Figura 5.1: Gráfico NPP de Residuales
</p>
</div>
<p><strong>Evaluación de Varianza Constante e Independencia:</strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico1-residuales"></span>
<img src="bookdown-iam_files/figure-html/grafico1-residuales-1.png" alt="Gráfico de Residuales vs. Valores Predichos" width="600px" />
<p class="caption">
Figura 5.2: Gráfico de Residuales vs. Valores Predichos
</p>
</div>
</div>
<div id="detección-de-puntos-atípicos-e-influyentes" class="section level4 hasAnchor" number="5.2.4.2">
<h4><span class="header-section-number">5.2.4.2</span> Detección de Puntos Atípicos e Influyentes<a href="rlm.html#detección-de-puntos-atípicos-e-influyentes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Aunque un análisis residual es útil para evaluar el ajuste de un modelo, las desviaciones del Modelo de Regresión quedan a menudo ocultas por el proceso de ajuste llevado a cabo. Por ejemplo, pueden existir “valores atípicos”, bien sea en la variable respuesta <span class="math inline">\(Y\)</span> o en las variables regresoras <span class="math inline">\(X´s\)</span>, los cuales pueden tener un efecto considerable sobre el análisis del modelo, pero estas observaciones no se detectan fácilmente a partir de una evaluación de los residuales. De hecho, estos valores atípicos pueden determinar el ajuste del modelo.</p>
<p>Los elementos <span class="math inline">\(h_{jj}\)</span>-de la Diagonal de la matriz <span class="math inline">\(\mathbf{H}=\mathbf{Z}(\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t\)</span>, se pueden interpretar de dos maneras relacionadas.</p>
<ol style="list-style-type: decimal">
<li><strong>Primero</strong>, el valor <span class="math inline">\(h_{jj}\)</span>-está asociado con el valor que mide la distancia que existe en el espacio de las variables regresoras, desde el punto <span class="math inline">\(j\)</span>-ésimo de dicha observación al resto de las otras <span class="math inline">\(n-1\)</span>-observaciones. Para un modelo de regresión lineal con una sola variables regresora, <span class="math inline">\(z_1\)</span>, es decir para un MRLS, se tiene que:
<span class="math display">\[
h_{jj}= \frac{1}{n} + \frac{(z_j-\overline{z})^2}{\sum_{j=1}^n\ (z_j-\overline{z})^2}=\frac{1}{n} + \frac{(z_j-\overline{z})^2}{S_{zz}^2}
\]</span></li>
</ol>
<p>con
<span class="math display">\[
S_{zz}^2=\sum_{j=1}^n\ (z_j-\overline{z})^2
\]</span></p>
<p>El promedio de los <span class="math inline">\(h_{jj}\)</span> es:
<span class="math display">\[
\overline{h}=\frac{\sum_{j=1}^n \ h_{jj}}{n}=\frac{tr(\mathbf{H})}{n}=\frac{r+1}{n}
\]</span></p>
<p>donde <span class="math inline">\(r\)</span>-es el número de variables regresoras, ie. <span class="math inline">\((r+1)\)</span>-número de parámetros del modelo. Además, <span class="math inline">\(1/n \leq h_{jj} &lt;1\)</span>.</p>
<p><strong>Puntos de Balanceo (u Observaciones Atípicas con respecto a los Valores de las variables Regresoras):</strong></p>
<p>Una observación se dice que es un punto de <em>Balanceo</em> si dicha observación está o se encuentra <em>Alejada</em> con respecto a los valores de las variables regresoras, es decir, se encuentra lejos del centro de gravedad del espacio definido por la variables regresoras. Para identificar las observaciones de Balanceo se utiliza el siguiente criterio.</p>
<p>Una observación <span class="math inline">\(\underline{\mathbf{z}}_{\ (j)}\)</span> es un punto de <em>Balanceo</em> (atípico) se se cumple que:
<span class="math display">\[
h_{jj}&gt;2\overline{h}=\frac{2(r+1)}{n}
\]</span></p>
<p>en el caso de que <span class="math inline">\(2\overline{h}&gt;1\)</span>, se dice simplemente que una observación <span class="math inline">\(\underline{\mathbf{z}}_{\ (j)}\)</span> es un punto de balanceo si:
<span class="math display">\[
h_{jj}&gt;1.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Segundo</strong>, el valor <span class="math inline">\(h_{jj}\)</span>-es una medida de la fuerza que ejerce una sola observación sobre el ajuste del modelo.</li>
</ol>
<p>El vector de valores predichos está dado por:
<span class="math display">\[
\widehat{\underline{\mathbf{y}}}=\mathbf{Z}\ \widehat{\underline{\boldsymbol \beta}}=\mathbf{Z}(\mathbf{Z}^t\mathbf{Z})^{-1}\mathbf{Z}^t \ \underline{\mathbf{y}}= \mathbf{H}\ \underline{\mathbf{y}},
\]</span></p>
<p>donde, la <span class="math inline">\(j\)</span>-ésima fila expresa el valor ajustado <span class="math inline">\(\widehat{y_j}\)</span>-en término de las observaciones como sigue:
<span class="math display">\[
\widehat{y_j}=h_{jj}\ y_j + \sum_{k\neq j}\ h_{jk}\ y_k.
\]</span></p>
<p>Originado que, siempre que los demás valores de <span class="math inline">\(y\)</span>-se mantengan fijos, se tiene que:
<span class="math display">\[
(\text{el cambio en}\ \ \widehat{y_j}) = h_{jj}\ (\text{el cambio en}\ \ y_j)
\]</span></p>
<p>Si el valor <span class="math inline">\(h_{jj}\)</span>-es relativamente grande, comparado con los demás <span class="math inline">\(h_{jk}\)</span> entonces, <span class="math inline">\(y_j\)</span>-tendrá una mayor contribución sobre el valor predicho <span class="math inline">\(\widehat{y_j}\)</span>.</p>
<p><strong>Observaciones Atípicas (u Observaciones Atípicas con respecto a los Valores de las variables Respuestas):</strong></p>
<p>Una observación se dice que es un punto <em>Atípico</em> si dicha observación está o se encuentra <em>Alejada</em> con respecto a los valores de la variable respuesta <span class="math inline">\(Y\)</span>, es decir, se encuentra lejos del centro de gravedad del espacio definido por la variables respuestas.</p>
<p>Aquellas observaciones con valores de <span class="math inline">\(h_{jj}\)</span>-grandes y residuales estudentizados grandes son candidatas a ser observaciones influyentes, como se ve a continuación.</p>
<p><strong>Observaciones Influyentes:</strong></p>
<p>Aquellas observaciones que afectan significativamente las inferencias extraídas de los datos se dice que son influyentes. Una observación es influyente <em>si tiene un impacto notable sobre los coeficientes de regresión ajustados</em>, es decir, una observación influyente se dice que <em>hala al modelo en su dirección</em>. <em>Una observación es influyente si su exclusión del modelo causa cambios importantes en la ecuación de regresión ajustada</em>. Estas observaciones se caracterizan por tener un valor moderadamente inusual tanto en el espacio de las predictoras como en el espacio de la(s) respuestas.</p>
<p>Los métodos para evaluar la influencia suelen basarse en el cambio del vector de parámetros estimados <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}\)</span>, cuando se eliminan las observaciones evaluadas. Gráficos basados en los valores <span class="math inline">\(h_{jj}\)</span> y estadísticas influenciales y sus usos en la verificación de los diagnósticos de modelos de regresión se describen en Atkinson <span class="citation">(<a href="#ref-atkinson1986">1986</a>)</span>, en Belsley at. all. <span class="citation">(<a href="#ref-belsley2005">2005</a>)</span> y Cook at. all. <span class="citation">(<a href="#ref-cook2009">2009</a>)</span>. Se recomiendan dichas referencias para cualquier persona involucrada en el análisis de modelos de regresión.</p>
<p>Después de identificar las observaciones que están alejadas con respecto a los valores de la(s) respuestas <span class="math inline">\(Y\)</span> (atípicas) y/o con respecto a sus valores en <span class="math inline">\(X\)</span> (puntos de balanceo) evaluamos si éstas son influyentes o no.</p>
<div id="criterios-para-detectar-observaciones-influyntes" class="section level5 hasAnchor" number="5.2.4.2.1">
<h5><span class="header-section-number">5.2.4.2.1</span> Criterios para detectar Observaciones Influyntes<a href="rlm.html#criterios-para-detectar-observaciones-influyntes" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Para detectar las observaciones influyentes se utilizan criterios basados en:</p>
<p><strong>La Distancia de Cook</strong></p>
<p>La Distancia de Cook es una medida de la distancia cuadrática entre,
el estimador de <span class="math inline">\(\underline{\boldsymbol\beta}\)</span> por mínimos cuadrados basado en las <span class="math inline">\(n\)</span> observaciones, y el estimador de <span class="math inline">\(\underline{\boldsymbol\beta}\)</span> obtenido eliminando la <span class="math inline">\(i\)</span>-ésima observación, así:
<span class="math display" id="eq:distancia-cook">\[
\begin{equation}
D_i = \frac{\left( \underline{\boldsymbol{\widehat{\beta}}}_{\left(i\right)} - \underline{\boldsymbol{\widehat{\beta}}}\right)&#39;\boldsymbol{X&#39;X}\left(\underline{\boldsymbol{\widehat{\beta}}}_{\left(i\right)} - \underline{\boldsymbol{\widehat{\beta}}}\right)}{p\,\text{MSE}} = \frac{r_i^2}{p}\left(\frac{h_{ii}}{1 - h_{ii}}\right),\ i = 1, \ldots, n
\end{equation}
\tag{5.30}
\]</span></p>
<p>donde, <span class="math inline">\(\underline{\boldsymbol{\widehat{\beta}}}_{\ \left(i\right)}\)</span> es el vector de parámetros estimados obtenido cuando no se considera en el ajuste del modelo a la observación <span class="math inline">\(i\)</span>.</p>
<p>Note que si <span class="math inline">\(D_i\)</span> es alto entonces la observación <span class="math inline">\(i\)</span> tiene influencia sobre el vector de parámetros estimados <span class="math inline">\(\boldsymbol{\widehat{\beta}}\)</span>.</p>
<p><strong>NOTAS:</strong></p>
<ul>
<li><p>Si <span class="math inline">\(D_i = f_{0.5;\  p, n - p}\)</span> entonces, al eliminar el punto <span class="math inline">\(i\)</span> se movería <span class="math inline">\(\underline{\boldsymbol{\widehat{\beta}}}_{\ \left(i\right)}\)</span> hacia la frontera de una región de confianza aproximada del 50% para <span class="math inline">\(\underline{\boldsymbol{\beta}}\)</span>, basándose en el conjunto completo de datos, lo cual es un desplazamiento grande e indica que el estimador por mínimos cuadrados es sensible al <span class="math inline">\(i\)</span>-ésimo punto de datos.</p></li>
<li><p>Como <span class="math inline">\(f_{0.5,\ p, n - p}\approx 1\)</span> se dice que la observación <span class="math inline">\(i\)</span> será <em>influyente</em> si <span class="math inline">\(D_i &gt; 1\)</span>.</p></li>
</ul>
<p><strong>Los DFFITS</strong></p>
<p>El <span class="math inline">\(DFFITS_i\)</span> es el número de desviaciones estándar que el valor ajustado <span class="math inline">\(\widehat{y_i}\)</span> se mueve si la observación <span class="math inline">\(i\)</span> es omitida, se define como sigue:
<span class="math display" id="eq:dffits">\[
\begin{equation}
\text{DFFITS}_i = \frac{\widehat{Y}_i - \widehat{Y}_{\left(i\right)}}{\sqrt{\text{MSE}_{\left(i\right)}\,h_{ii}}} = \frac{e_i}{\sqrt{MSE_{\left(i\right)}\left(1 - h_{ii}\right)}}\left(\frac{h_{ii}}{1 - h_{ii}}\right)^{1/2}
\end{equation}
\tag{5.31}
\]</span></p>
<p>donde, <span class="math inline">\(\widehat{Y}_{\ \left(i\right)}\)</span> es el <span class="math inline">\(i\)</span>-ésimo valor ajustado obtenido cuando no se considera en el ajuste del modelo a la observación <span class="math inline">\(i\)</span> y <span class="math inline">\(\text{MSE}_{\left(i\right)}\)</span> es el cuadrado medio del error obtenido cuando no se considera en el ajuste del modelo a la observación <span class="math inline">\(i\)</span>.</p>
<p>Una observación será <em>influyente</em> si <span class="math inline">\(\vert\text{DFFITS}_i\vert &gt; 2\sqrt{\frac{p}{n}}\)</span>.</p>
<p><strong>Los DFBETAS</strong></p>
<p>Los <span class="math inline">\(DFBETA_{\ j(i)}\)</span> indican cuánto cambia el <span class="math inline">\(j\)</span>-ésimo coeficiente de regresión estimado <span class="math inline">\(\widehat{\beta}_j\)</span> en unidades de desviación estándar, si se omite la <span class="math inline">\(i\)</span>-ésima observación, se definen como sigue:
<span class="math display" id="eq:df-betas">\[
\begin{equation}
\text{DFBETAS}_{j\left(i\right)} = \frac{\widehat{\beta}_j - \widehat{\beta}_{j\left(i\right)}}{\sqrt{\text{MSE}_{\left(i\right)}\,c_{jj}}}
\end{equation}
\tag{5.32}
\]</span></p>
<p>donde <span class="math inline">\(c_{jj}\)</span> es el <span class="math inline">\(j\)</span>-ésimo elemento en la diagonal principal de la matriz: <span class="math inline">\((\boldsymbol{X&#39;X})^{-1}\)</span> y <span class="math inline">\(\text{MSE}_{\left(i\right)}\)</span> es el MSE de la regresión sin la observación <span class="math inline">\(i\)</span>.</p>
<p>Una observación será <em>influyente</em> si <span class="math inline">\(\vert \text{DFBETAS}_{\ j\left(i\right)}\vert &gt; 2/\sqrt{n}\)</span>.</p>
<p><strong>NOTA:</strong> Tanto las Distancias de COOK, <span class="math inline">\(D_i\)</span>, como los <span class="math inline">\(DFFITS_{\ i}\)</span> y los <span class="math inline">\(DFBETAS_{\ j(i)}\)</span> se pueden afectar tanto por un error de ajuste grande como, por un gran balanceo, por eso, los puntos que sean detectados por estos criterios deben ser investigados.</p>
<p>Si, después de las comprobaciones de los diagnósticos, no se detectan violaciones graves a los supuestos, podemos hacer inferencias sobre el vector de parámetros del modelo <span class="math inline">\(\underline{\boldsymbol \beta}\)</span> y sobre los valores futuros de la respuesta <span class="math inline">\(Y\)</span> con algo de seguridad de que no seremos engañados.</p>
<div class="example">
<p><span id="exm:ejemplo4-mrlm" class="example"><strong>Ejemplo 5.6  (Análisis de Puntos Influyentes) </strong></span>Continaundo con el ejemplo <a href="rlm.html#exm:ejemplo3-mrlm">5.5</a>, se tiene el siguiente análisis sobre detección d epuntos influyentes.</p>
</div>
<p><strong>Análisis de Diagnósticos para Observaciones Influyentes:</strong></p>
<table>
<caption>
<span id="tab:unnamed-chunk-134">Tabla 5.5: </span>Tabla de Diagnósticos (Obs. Influyentes)
</caption>
<thead>
<tr>
<th style="text-align:center;">
Y
</th>
<th style="text-align:center;">
yhat
</th>
<th style="text-align:center;">
se.yhat
</th>
<th style="text-align:center;">
residuals
</th>
<th style="text-align:center;">
res.stud
</th>
<th style="text-align:center;">
Cooks.D
</th>
<th style="text-align:center;">
hii.value
</th>
<th style="text-align:center;">
Dffits
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
141.5
</td>
<td style="text-align:center;">
142.6
</td>
<td style="text-align:center;">
0.5045
</td>
<td style="text-align:center;">
-1.0632
</td>
<td style="text-align:center;">
-0.9726
</td>
<td style="text-align:center;">
0.0672
</td>
<td style="text-align:center;">
0.1756
</td>
<td style="text-align:center;">
-0.4449
</td>
</tr>
<tr>
<td style="text-align:center;">
168.9
</td>
<td style="text-align:center;">
169.9
</td>
<td style="text-align:center;">
0.8879
</td>
<td style="text-align:center;">
-1.0315
</td>
<td style="text-align:center;">
-1.2686
</td>
<td style="text-align:center;">
0.6398
</td>
<td style="text-align:center;">
0.5439
</td>
<td style="text-align:center;">
-1.5520
</td>
</tr>
<tr>
<td style="text-align:center;">
154.8
</td>
<td style="text-align:center;">
153.7
</td>
<td style="text-align:center;">
0.5405
</td>
<td style="text-align:center;">
1.1007
</td>
<td style="text-align:center;">
1.0231
</td>
<td style="text-align:center;">
0.0881
</td>
<td style="text-align:center;">
0.2015
</td>
<td style="text-align:center;">
0.5180
</td>
</tr>
<tr>
<td style="text-align:center;">
146.5
</td>
<td style="text-align:center;">
147.4
</td>
<td style="text-align:center;">
0.5919
</td>
<td style="text-align:center;">
-0.9151
</td>
<td style="text-align:center;">
-0.8729
</td>
<td style="text-align:center;">
0.0810
</td>
<td style="text-align:center;">
0.2417
</td>
<td style="text-align:center;">
-0.4744
</td>
</tr>
<tr>
<td style="text-align:center;">
172.8
</td>
<td style="text-align:center;">
172.3
</td>
<td style="text-align:center;">
0.9174
</td>
<td style="text-align:center;">
0.4650
</td>
<td style="text-align:center;">
0.5963
</td>
<td style="text-align:center;">
0.1641
</td>
<td style="text-align:center;">
0.5806
</td>
<td style="text-align:center;">
0.6366
</td>
</tr>
<tr>
<td style="text-align:center;">
160.1
</td>
<td style="text-align:center;">
159.0
</td>
<td style="text-align:center;">
0.8210
</td>
<td style="text-align:center;">
1.1066
</td>
<td style="text-align:center;">
1.2566
</td>
<td style="text-align:center;">
0.4575
</td>
<td style="text-align:center;">
0.4650
</td>
<td style="text-align:center;">
1.3041
</td>
</tr>
<tr>
<td style="text-align:center;">
108.5
</td>
<td style="text-align:center;">
108.2
</td>
<td style="text-align:center;">
1.0712
</td>
<td style="text-align:center;">
0.3375
</td>
<td style="text-align:center;">
0.6142
</td>
<td style="text-align:center;">
0.4777
</td>
<td style="text-align:center;">
0.7916
</td>
<td style="text-align:center;">
1.0893
</td>
</tr>
</tbody>
</table>
<p><strong>Identificación de observaciones atípicas</strong></p>
<p>Se considera que una observación es <em>atípica</em> cuando su residual estudentizado <span class="math inline">\(r_i\)</span>, es tal que: <span class="math inline">\(\vert r_i\vert &gt; 3\)</span>.</p>
<p>De acuerdo a la columna <em>res.stud</em> de residuales estudentizados se tiene que no hay observaciones atípicas.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico2-residuales-estudent"></span>
<img src="bookdown-iam_files/figure-html/grafico2-residuales-estudent-1.png" alt="Gráfico de Residuales-Studentizados vs. Valores Predichos" width="600px" />
<p class="caption">
Figura 5.3: Gráfico de Residuales-Studentizados vs. Valores Predichos
</p>
</div>
<pre><code>## integer(0)</code></pre>
<p><strong>Identificación de puntos de balanceo</strong></p>
<p>Se asume que la observación <span class="math inline">\(i\)</span> es un <em>punto de balanceo</em> si <span class="math inline">\(h_{ii} &gt; 2p/n\)</span>.</p>
<p>En la práctica tenemos que: <span class="math inline">\(h_{ii} &gt; 2p/n = 2(3/7) = 0.8571\)</span>.</p>
<p>De acuerdo a la columna <em>hii.value</em> de valores de la diagonal de la matriz <span class="math inline">\(\boldsymbol{H}\)</span> se tiene que no hay observaciones o puntos de balanceo.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico3-hii"></span>
<img src="bookdown-iam_files/figure-html/grafico3-hii-1.png" alt="Gráfico de hii vs. Valores Predichos" width="600px" />
<p class="caption">
Figura 5.4: Gráfico de hii vs. Valores Predichos
</p>
</div>
<pre><code>## integer(0)</code></pre>
<p><strong>Identificación de observaciones influyentes</strong></p>
<p>Recuerde que para identificar esos valores tenemos dos criterios, que son:</p>
<ul>
<li><p>Se dice que la observación <span class="math inline">\(i\)</span> será <em>influencial</em> si <span class="math inline">\(D_i &gt; 1\)</span>.</p></li>
<li><p>Una observación será <em>influencial</em> si <span class="math inline">\(\vert\text{DFFITS}_i\vert &gt; 2\sqrt{\frac{p}{n}}\)</span>.</p></li>
</ul>
<p>En este caso, se tiene que el criterio basado en DFFITS debe superar en valor absoluto a <span class="math inline">\(2\sqrt{\frac{3}{7}} = 1.3093\)</span>, ie. <span class="math inline">\(\vert\text{DFFITS}_i\vert&gt;1.3093\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico4-cooks"></span>
<img src="bookdown-iam_files/figure-html/grafico4-cooks-1.png" alt="Gráfico de Distancias de Cook vs. Valores Predichos" width="600px" />
<p class="caption">
Figura 5.5: Gráfico de Distancias de Cook vs. Valores Predichos
</p>
</div>
<pre><code>## integer(0)</code></pre>
<ul>
<li>De acuerdo a la columna <em>Cooks.D</em> de distancias de Cook no se tienen observaciones influyentes.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico5-dffits"></span>
<img src="bookdown-iam_files/figure-html/grafico5-dffits-1.png" alt="Gráfico de DFFITS vs. Valores Predichos" width="600px" />
<p class="caption">
Figura 5.6: Gráfico de DFFITS vs. Valores Predichos
</p>
</div>
<pre><code>## [1] 2</code></pre>
<ul>
<li>De acuerdo a la columna <em>Dffits</em> de valores DFFITS tenemos que la observación número dos, ie. <span class="math inline">\(\underline{\mathbf{z}}_{\ 2}\)</span> es influyente.</li>
</ul>
<p><strong>En resumen</strong>, para el análisis de observaciones extremas se tiene que:</p>
<ul>
<li><p>No hay observaciones atípicas.</p></li>
<li><p>No hay observaciones o puntos de balanceo.</p></li>
<li><p>Hay una observación influyente, que es la número dos, ie. <span class="math inline">\(\underline{\mathbf{z}}_{\ 2}\)</span>.</p></li>
</ul>
<p><strong>Resumen de Valores u Observaciones Atípicas</strong></p>
<pre><code>## Potentially influential observations of
##   lm(formula = y ~ z1 + z2, data = datos) :
## 
##   dfb.1_  dfb.z1 dfb.z2  dffit cov.r   cook.d hat  
## 2  0.27   -0.20  -1.13_* -1.55  1.11    0.64   0.54
## 5 -0.42    0.50  -0.41    0.64  4.27_*  0.16   0.58
## 6  0.14   -0.19   1.06_*  1.30  0.98    0.46   0.46
## 7  1.02_* -0.93   0.06    1.09  8.45_*  0.48   0.79</code></pre>
<pre><code>##   StudRes    Hat  CookD
## 2 -1.4212 0.5439 0.6398
## 5  0.5411 0.5806 0.1641
## 6  1.3989 0.4650 0.4575
## 7  0.5589 0.7916 0.4777</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico6-resuemne-influyentes"></span>
<img src="bookdown-iam_files/figure-html/grafico6-resuemne-influyentes-1.png" alt="Gráfico de Observaciones Influyentes" width="600px" />
<p class="caption">
Figura 5.7: Gráfico de Observaciones Influyentes
</p>
</div>
</div>
</div>
<div id="problemas-adicionales-en-modelos-de-regresión-lineal" class="section level4 hasAnchor" number="5.2.4.3">
<h4><span class="header-section-number">5.2.4.3</span> Problemas Adicionales en Modelos de Regresión Lineal<a href="rlm.html#problemas-adicionales-en-modelos-de-regresión-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Se discutirán brevemente varios aspectos importantes de modelos de regresión lineal, que merecen y reciben extensos tratamientos en los siguientes libros dedicados al análisis de regresión: <span class="citation">(<a href="#ref-cook2009">Cook and Weisberg 2009</a>)</span>, <span class="citation">(<a href="#ref-cook1982">Cook and Weisberg 1982</a>)</span>, <span class="citation">(<a href="#ref-draper1998">Draper and Smith 1998</a>)</span> y <span class="citation">(<a href="#ref-seber2003">Seber and Lee 2003</a>)</span>.</p>
<div id="seleccionar-variables-predictoras-a-partir-de-un-conjunto-grande" class="section level5 hasAnchor" number="5.2.4.3.1">
<h5><span class="header-section-number">5.2.4.3.1</span> Seleccionar Variables Predictoras a Partir de un Conjunto Grande<a href="rlm.html#seleccionar-variables-predictoras-a-partir-de-un-conjunto-grande" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>En la práctica frecuentemente es difícil formular la función de regresión adecuada inmediatamente. Preguntas como: ¿Cuáles variables predictoras deberían ser incluidas? ¿De qué forma debería ser la función de regresión? deben de ser respondidas a la hora de tratar con modelos de regresión lineal.</p>
<p>Cuando la lista de posibles variables predictoras es muy grande, no todas las variables pueden ser incluidas en la función de regresión. Hoy en día existen técnicas diseñadas y programas de cómputos disponibles para seleccionar el “mejor” subconjunto de variables predictoras. Una técnica buena, es tratar todos los subconjuntos de una variable a la vez, <span class="math inline">\(z_1-sola\)</span>, <span class="math inline">\(z_2-sola\)</span>, etc., subconjuntos de dos variables a la vez, <span class="math inline">\(z_1,z_2-solas\)</span>, <span class="math inline">\(z_1,z_3-solas\)</span>, etc. La elección del “mejor” modelo o “mejor subconjunto” de variables, es tomada mediante la evaluación de algunos criterios cuantitativos tales como el <span class="math inline">\(R^2\)</span>- entre muchos otros, ver. <span class="citation">(<a href="#ref-box1949">Box 1949</a>)</span>, <span class="citation">(<a href="#ref-box2015">Box et al. 2015</a>)</span> y <span class="citation">(<a href="#ref-chatterjee2006">Chatterjee and Hadi 2006</a>)</span>. Sin embargo el <span class="math inline">\(R^2\)</span>-siempre crece con la inclusión de variables adicionales al modelo. Aunque, este problema se puede evitar, usando el <span class="math inline">\(R^2-Ajustado\)</span>, dado por:
<span class="math display" id="eq:r2-ajustado">\[
\begin{equation}
R^2_{Ajust}=1-\frac{(n-1)}{n-(r+1)}(1-R^2)
\end{equation}
\tag{5.33}
\]</span></p>
<p>un mejor Estadístico para Seleccionar variables, parece ser la Estadística <span class="math inline">\(Cp\)</span>-de Mallows, ver. <span class="citation">(<a href="#ref-wood1973">Wood 1973</a>)</span>, la cual se define como sigue:
<span class="math display" id="eq:cp-mallows">\[
\begin{equation}
C_p = \frac{ \biggl( \underset{\Large{\text{con}\ p-\text{parámetros incluyendo un intercepto}}}{\text{Suma de Cuadrados para el Modelo sub-conjunto}} \biggr)  }{\text{Varianza Residual del Modelo Full}} - (n-2p)
\end{equation}
\tag{5.34}
\]</span></p>
<p>donde <span class="math inline">\(p\)</span>-es igual al número de parámetros en el modelo a evaluar.</p>
<p>Una gráfica de las parejas <span class="math inline">\((p \ , \ C_p)\)</span>-para cada subconjunto de predictoras, nos indicará cuales modelos pronostican bien la respuesta observada. Los modelos buenos, tienen las coordenadas <span class="math inline">\((p \ , \ C_p)\)</span>-típicamente cercanas a una línea recta de <span class="math inline">\(45^{°}\)</span>. En la figura xxxx, se observa encerrado en un círculo el “mejor” sub-conjunto de variables predictoras.</p>
<p>Si la lista de variables predictoras en muy grande, las consideraciones de costos nos limitarán al número de modelos que pueden ser examinados. Otro acercamiento, llamado <em>Regresión Paso a Paso</em> ver. <span class="citation">(<a href="#ref-draper1998">Draper and Smith 1998</a>)</span>, intenta seleccionar predictoras importantes sin considerar todas las posibilidades.</p>
<div class="example">
<p><span id="exm:ejemplo5-mrlm" class="example"><strong>Ejemplo 5.7  (Todas las Regresiones Posibles) </strong></span>Continaundo con el ejemplo <a href="rlm.html#exm:ejemplo-mrlm">5.2</a>, se tiene el siguiente análisis sobre selección de subconjuntos de variables.</p>
</div>
<p><strong>Tabla de todas las regresiones posibles</strong></p>
<pre><code>##   k  R_sq adj_R_sq      SSE       Cp
## 1 1 0.994    0.992   18.041    9.447
## 2 1 0.203    0.043 2238.655 1541.471
## 3 2 0.998    0.997    5.798    3.000
##   Variables_in_model
## 1                 z1
## 2                 z2
## 3              z1 z2</code></pre>
<p><strong>Resumen de mejores submodelos de acuerdo al criterio <span class="math inline">\(\boldsymbol{R^2}\)</span></strong></p>
<pre><code>## Models are Indexed in rows 
##  k p     R2 Variables.in.model
##  1 2 0.9936                 z1
##  2 3 0.9979              z1 z2</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico-criterio-R2"></span>
<img src="bookdown-iam_files/figure-html/grafico-criterio-R2-1.png" alt="Selección de Subconjuntos según el R2" width="600px" />
<p class="caption">
Figura 5.8: Selección de Subconjuntos según el R2
</p>
</div>
<p><strong>Resumen de mejores submodelos de acuerdo al criterio <span class="math inline">\(\boldsymbol{R^2_{\text{adj},p}}\)</span></strong></p>
<pre><code>## Models are Indexed in rows 
##  k p  adjR2 Variables.in.model
##  1 2 0.9923                 z1
##  2 3 0.9969              z1 z2</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico-criterio-R2-adj"></span>
<img src="bookdown-iam_files/figure-html/grafico-criterio-R2-adj-1.png" alt="Selección de Subconjuntos según el R2-Ajustado" width="600px" />
<p class="caption">
Figura 5.9: Selección de Subconjuntos según el R2-Ajustado
</p>
</div>
<p><strong>Resumen de mejores submodelos de acuerdo al criterio <span class="math inline">\(\boldsymbol{C_p}\)</span></strong></p>
<pre><code>## Models are Indexed in rows 
##  k p    Cp Variables.in.model
##  1 2 9.447                 z1
##  2 3 3.000              z1 z2</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grafico-criterio-cp"></span>
<img src="bookdown-iam_files/figure-html/grafico-criterio-cp-1.png" alt="Selección de Subconjuntos según el CP-Mallows" width="600px" />
<p class="caption">
Figura 5.10: Selección de Subconjuntos según el CP-Mallows
</p>
</div>
<p><img src="bookdown-iam_files/figure-html/unnamed-chunk-141-1.png" width="672" /></p>
</div>
<div id="regresión-paso-a-paso" class="section level5 hasAnchor" number="5.2.4.3.2">
<h5><span class="header-section-number">5.2.4.3.2</span> Regresión Paso a Paso<a href="rlm.html#regresión-paso-a-paso" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Este procedimiento puede describirse enumerando los pasos básicos (o algoritmo) involucrados en los cálculos:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Paso-1:</strong> Se consideran todos los modelos de regresión lineal simple posibles. La variable predictora que explica la mayor proporción significante de la variación en la respuesta <span class="math inline">\(Y\)</span> (ie. la variable <span class="math inline">\(X\)</span> que tiene la correlación más grande con la variable respuesta <span class="math inline">\(Y\)</span>) será la primera variable a entrar en la función de regresión.</p></li>
<li><p><strong>Paso.2:</strong> La siguiente variable regresora a entrar al modelo (y que aún no haya sido incluida) es aquella con la mayor contribución significante a la Suma de Cuadrados de la Regresión. La significancia de dicha contribución se determina mediante una prueba <span class="math inline">\(F\)</span>. El valor de la Estadística <span class="math inline">\(F\)</span> que debe superarse antes de que la contribución de una variable sea considerada significativa se le llama frecuentemente el <span class="math inline">\(F\)</span>-de entrada.</p></li>
<li><p><strong>Paso-3:</strong> Una vez que una variable adicional ha sido incluida a la ecuación de la función de regresión, se evalúan las contribuciones individuales a las suma de cuadrados de regresión de las variables que ya habían ingresado anteriormente para chequear la significancia de dichas variables, mediante una prueba <span class="math inline">\(F\)</span>. Si el valor del Estadístico <span class="math inline">\(F\)</span> (también conocido como <span class="math inline">\(F\)</span>-de salida) es menor que el correspondiente <span class="math inline">\(F\)</span> con el nivel de significancia pre-establecido, entonces esa variable es eliminada del modelo o de la función de regresión.</p></li>
<li><p><strong>Paso-4:</strong> Se repiten los pasos 2 y 3, hasta que todas las posibles variables a adicionar son no-significativas y todas las posibles eliminaciones son significantes. En este punto, el proceso de selección se detiene.</p></li>
</ol>
</div>
<div id="criterio-aic-para-seleccionar-variables" class="section level5 hasAnchor" number="5.2.4.3.3">
<h5><span class="header-section-number">5.2.4.3.3</span> Criterio AIC Para Seleccionar Variables<a href="rlm.html#criterio-aic-para-seleccionar-variables" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Debido a que en el proceso paso a paso, no hay garantía de que se seleccione, por ejemplo, las tres mejores variables para la predicción de la respuesta en un modelo de regresión, un segundo inconveniente es que los métodos de selección (automáticos, como lo es el Paso a Paso) no son capaces de indicar cuándo ciertas transformaciones de variables son útiles, otro criterio popular, para seleccionar un modelo adecuado, es el llamado <em>Criterio de Información de Akaikes</em>, el cual balancea el tamaño de la suma de cuadrados de residuales con el número de parámetros presentes en el modelo.</p>
<p><em>El Criterio de Información de Akaikes (AIC) es:</em>
<span class="math display" id="eq:criterio-aic">\[
\begin{equation}
AIC =n\ \text{Ln}\  \left( \frac{  \underset{\Large{\text{con}\ p-\text{parámetros, incluyendo un intercepto}}}{\text{Suma de Cuadrados para el Modelo sub-conjunto}}   }{n} \right) + 2p
\end{equation}
\tag{5.35}
\]</span></p>
<p>Se desea que la suma de cuadrados de residuales sea pequeña, pero el segundo término penaliza la presencia de muchos parámetros. En general, se desean seleccionar aquellos modelos que tienen <span class="math inline">\(AIC\)</span> lo más pequeño posible.</p>
</div>
<div id="colinealidad-en-las-variables-regresoras" class="section level5 hasAnchor" number="5.2.4.3.4">
<h5><span class="header-section-number">5.2.4.3.4</span> Colinealidad en las Variables Regresoras<a href="rlm.html#colinealidad-en-las-variables-regresoras" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Si la matriz <span class="math inline">\(\mathbf{Z}\)</span>-no es de rango completo, algunas combinaciones lineales, tales como <span class="math inline">\(\mathbf{Z}\ \underline{\mathbf{a}}\)</span>, deben ser igual a <span class="math inline">\(\underline{\mathbf{0}}\)</span>. En estas situaciones se dice que las columnas de <span class="math inline">\(\mathbf{Z}\)</span> son colineales. Esto implica que la matriz <span class="math inline">\(\mathbf{Z}^t\mathbf{Z}\)</span>-no tiene inversa. Para la mayoría de los Análisis de Regresión, es poco probable que
<span class="math inline">\(\mathbf{Z}\ \underline{\mathbf{a}}=\underline{\mathbf{0}}\)</span>-exactamente. Aún, si existen combinaciones de las columnas de <span class="math inline">\(\mathbf{Z}\)</span> que son cercanamente iguales a <span class="math inline">\(\underline{\mathbf{0}}\)</span>, el cálculo de la inversa <span class="math inline">\((\mathbf{Z}^t\mathbf{Z})^{-1}\)</span>-es numéricamente inestable. Típicamente, las entradas de la diagonal de <span class="math inline">\((\mathbf{Z}^t\mathbf{Z})^{-1}\)</span>-serán muy grandes. Esto produce varianzas estimadas muy grandes para los <span class="math inline">\(\widehat{\beta_i}\)</span> y esto nos lleva a la dificultad de identificar o detectar los coeficientes de regresión <span class="math inline">\(\widehat{\beta_i}\)</span> del modelo que son significantes.</p>
<p>Los problemas causados por la multicolinealidad se puede superar de alguna de las siguientes formas:</p>
<ul>
<li><p>Eliminando cada uno de los pares de variables predictoras o regresoras
que están fuertemente correlacionados. o bien,</p></li>
<li><p>Relacionando la variable respuesta <span class="math inline">\(Y\)</span> con las componentes principales de las variables predictoras o regresoras, es decir, las filas <span class="math inline">\(\underline{\mathbf{z}}_j\)</span> de la matriz <span class="math inline">\(\mathbf{Z}\)</span> se tratan como una muestra, y unas pocas primeras componentes principales se calculan como se describe posteriormente en este texto. A continuación, se realiza una regresión lineal de la variable respuesta <span class="math inline">\(Y\)</span> sobre estas nuevas variables predictoras llamadas componentes principales.</p></li>
</ul>
</div>
<div id="sesgo-causado-por-modelos-mal-especificados" class="section level5 hasAnchor" number="5.2.4.3.5">
<h5><span class="header-section-number">5.2.4.3.5</span> Sesgo Causado por Modelos Mal Especificados<a href="rlm.html#sesgo-causado-por-modelos-mal-especificados" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Suponga que algunas variables predictoras importantes son omitidas del modelo de regresión propuesto, es decir, suponga que el verdadero modelo de regresión tiene la matriz <span class="math inline">\(\mathbf{Z}\)</span> de rango <span class="math inline">\(r+1\)</span> dada por:
<span class="math display">\[
\mathbf{Z}=[\ \mathbf{Z}_1 \ \ | \ \ \mathbf{Z}_2\ ]
\]</span></p>
<p>y que
<span class="math display">\[
\underline{\mathbf{y}}= \mathbf{Z}\ \underline{\boldsymbol \beta} + \underline{\boldsymbol \varepsilon}
\]</span></p>
<p>se reescribir como sigue:
<span class="math display" id="eq:mrlm-particionado2">\[
\begin{equation}
\underset{n\times 1}{\underline{\mathbf{y}}}= \begin{bmatrix} \underset{n\times (q+1)}{ \mathbf{Z}_1 } &amp; | &amp; \underset{n\times (r-q)}{  \mathbf{Z}_2} \end{bmatrix}_{n\times (r+1)} \  \begin{bmatrix} \underset{(q+1)\times 1}{  \underline{\boldsymbol \beta}_{\ (1)} }  \\ --- \\  \underset{(r-q)\times 1}{ \underline{\boldsymbol \beta}_{\ (2)} } \end{bmatrix}_{(r+1)\times 1} + \ \ \ \ \  \underline{\boldsymbol \varepsilon}\ \ \   \\ \\
\underline{\mathbf{y}} = \mathbf{Z}_1\ \underline{\boldsymbol \beta}_{\ (1)} + \mathbf{Z}_2\ \underline{\boldsymbol \beta}_{\ (2)} + \underline{\boldsymbol \varepsilon}
\end{equation}
\tag{5.36}
\]</span></p>
<p>donde <span class="math inline">\(E[\underline{\boldsymbol \varepsilon}]=\underline{\mathbf{0}}\)</span> y <span class="math inline">\(Var[\underline{\boldsymbol \varepsilon}]=\sigma^2\ \mathbf{I}\)</span>. Sin embargo, el investigador sin saberlo ajusta un modelo usando solamente las primeras <span class="math inline">\(q\)</span>-variables regresoras minimizando la suma de cuadrados de errores dada por:
<span class="math display">\[
SS_{res}=\bigl(\ \underline{\mathbf{y} } -  \mathbf{Z}_1\ \underline{\boldsymbol \beta}_{\ (1)} \ \bigr)^{t}\ \bigl(\ \underline{\mathbf{y} } -  \mathbf{Z}_1\ \underline{\boldsymbol \beta}_{\ (1)} \ \bigr).
\]</span></p>
<p>El <em>Estimador de Mínimos Cuadrados</em> de <span class="math inline">\(\underline{\boldsymbol \beta}_{\ (1)}\)</span> es:
<span class="math display">\[
\widehat{\underline{\boldsymbol \beta}}_{\ (1)}=( \mathbf{Z}_1^t \mathbf{Z}_1)^{-1} \mathbf{Z}_1^t \ \underline{\mathbf{y}}.
\]</span></p>
<p>Entonces en este caso, a diferencia de la situación cuando el modelo es correcto se tiene que:
<span class="math display" id="eq:mrlm-valor-esperado-beta-1">\[
\begin{equation}
E\bigl[\   \widehat{\underline{\boldsymbol \beta}}_{\ (1)}\ \bigr]=( \mathbf{Z}_1^t \mathbf{Z}_1)^{-1} \mathbf{Z}_1^t \ E\bigl[\ \underline{\mathbf{y}} \ \bigr] = ( \mathbf{Z}_1^t \mathbf{Z}_1)^{-1} \mathbf{Z}_1^t \ \biggl( \mathbf{Z}_1\ \underline{\boldsymbol \beta}_{\ (1)} + \mathbf{Z}_2\ \underline{\boldsymbol \beta}_{\ (2)} + E[\ \underline{\boldsymbol \varepsilon}\ ]\biggr) \\ \\
= \underline{\boldsymbol \beta}_{\ (1)} + \underbrace{ ( \mathbf{Z}_1^t \mathbf{Z}_1)^{-1}\ \mathbf{Z}_1^t\mathbf{Z}_2\ \underline{\boldsymbol \beta}_{\ (2)} } \\
E\bigl[\   \widehat{\underline{\boldsymbol \beta}}_{\ (1)}\ \bigr] \neq \underline{\boldsymbol \beta}_{\ (1)}
\end{equation}
\tag{5.37}
\]</span></p>
<p>es decir, <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (1)}\)</span> es un <em>Estimador Sesgado</em> de <span class="math inline">\(\underline{\boldsymbol \beta}_{\ (1)}\)</span>, a menos que las columans de <span class="math inline">\(\mathbf{Z}_1\)</span> sean perpendiculares a las columnas de <span class="math inline">\(\mathbf{Z}_2\)</span>, es decir que, <span class="math inline">\(\mathbf{Z}_1^t \mathbf{Z}_2=\underline{\mathbf{0}}\)</span>. Si hay variables importantes ausentes del modelo, el Estimador de Mínimos Cuadrados <span class="math inline">\(\widehat{\underline{\boldsymbol \beta}}_{\ (1)}\)</span> puede ser engañoso.</p>
</div>
</div>
</div>
</div>
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-atkinson1986" class="csl-entry">
Atkinson, AC. 1986. <span>“[Influential Observations, High Leverage Points, and Outliers in Linear Regression]: Comment: Aspects of Diagnostic Regression Analysis.”</span> <em>Statistical Science</em> 1 (3): 397–402.
</div>
<div id="ref-belsley2005" class="csl-entry">
Belsley, David A, Edwin Kuh, and Roy E Welsch. 2005. <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. John Wiley &amp; Sons.
</div>
<div id="ref-box1949" class="csl-entry">
Box, George EP. 1949. <span>“A General Distribution Theory for a Class of Likelihood Criteria.”</span> <em>Biometrika</em> 36 (3/4): 317–46.
</div>
<div id="ref-box2015" class="csl-entry">
Box, George EP, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. 2015. <em>Time Series Analysis: Forecasting and Control</em>. John Wiley &amp; Sons.
</div>
<div id="ref-chatterjee2006" class="csl-entry">
Chatterjee, Samprit, and Ali S Hadi. 2006. <em>Regression Analysis by Example</em>. John Wiley &amp; Sons.
</div>
<div id="ref-cook1982" class="csl-entry">
Cook, R Dennis, and Sanford Weisberg. 1982. <em>Residuals and Influence in Regression</em>. New York: Chapman; Hall.
</div>
<div id="ref-cook2009" class="csl-entry">
———. 2009. <em>Applied Regression Including Computing and Graphics</em>. John Wiley &amp; Sons.
</div>
<div id="ref-draper1998" class="csl-entry">
Draper, Norman R, and Harry Smith. 1998. <em>Applied Regression Analysis</em>. Vol. 326. John Wiley &amp; Sons.
</div>
<div id="ref-durbin1971" class="csl-entry">
Durbin, James, and Geoffrey S Watson. 1971. <span>“Testing for Serial Correlation in Least Squares Regression. III.”</span> <em>Biometrika</em> 58 (1): 1–19.
</div>
<div id="ref-seber2003" class="csl-entry">
Seber, George AF, and Alan J Lee. 2003. <em>Linear Regression Analysis</em>. Vol. 330. John Wiley &amp; Sons.
</div>
<div id="ref-wood1973" class="csl-entry">
Wood, Fred S. 1973. <span>“The Use of Individual Effects and Residuals in Fitting Equations to Data.”</span> <em>Technometrics</em> 15 (4): 677–95.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introducción-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mrl-multiv.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-iam.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
